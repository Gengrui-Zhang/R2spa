---
title: "Reliability of Factor Scores"
author: "Yixiao Li, Winnie Tse, Mark Lai"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{reliability}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
  - \DeclareMathOperator{\Var}{\mathrm{Var}}
  - \DeclareMathOperator{\E}{\mathrm{E}}
  - \DeclareMathOperator{\Tr}{\mathrm{Tr}}
  - \newcommand{\bv}[1]{\boldsymbol{\mathbf{#1}}}  %APA-consistent bold
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

TODO:

- [ ] Implement the three reliability estimation (quadratic root method as experimental)
- [ ] Perform a small-scale simulation
- [x] Write down math equations for all methods
- [ ] Theoretical exploration of added uncertainty due to (a) small sample and (b) number of items. 

In this vignette, we explore analytically the reliability of factor scores, and evaluate them using simulated data.

```{r}
library(lavaan)
library(R2spa)
library(tidyr)
library(ggplot2)
```

## Analytic Formula for Factor Scores

### Theoretical Values

> aka, using true parameter values to compute weights

From a unidimensional factor model $\bv y= \bv \lambda \eta + \bv e$, for factor scores of the form

$$
\begin{aligned}
  \tilde \eta & = \bv a' (\bv y - \hat {\bv \mu}) = \bv a' (\bv \nu + \bv \lambda \eta + \bv e - \bv \nu) \\
  & = \bv a' \bv \lambda \eta + \bv a' \bv e,
\end{aligned}
$$

where we set $\E(\eta)$ = 0 so that $\hat {\bv \mu}$ = $\bv \nu$, the theoretical reliability formula of factor scores, assuming no sampling errors in the weights is given by 

$$
\rho = \frac{(\bv a' \bv \lambda)^2}{(\bv a'\bv \lambda)^2 + \bv a' \bv \Theta \bv a},
$$

assuming that we set $\Var(\eta) = 1$.

#### Regression factor scores

With $\bv a$ = $(\bv \Sigma^{-1} \bv \lambda)$,

$$
\begin{align*}
\text{Rel} 
&= \frac{(\bv \lambda' \bv \Sigma^{-1} \bv \lambda)^2}{\bv \lambda'\bv \Sigma^{-1}\bv \lambda \bv \lambda \bv \Sigma^{-1}\bv \lambda + \bv \lambda'\bv \Sigma^{-1}\bv \Theta \bv \Sigma^{-1}\bv \lambda} \\
&= \frac{(\bv \lambda'\bv \Sigma^{-1}\bv \lambda)^2}{\bv \lambda'\bv \Sigma^{-1}(\bv \lambda\bv \lambda'+\bv \Theta)\bv \Sigma^{-1}\bv \lambda} \\
&= \frac{(\bv \lambda'\bv \Sigma^{-1}\bv \lambda)^2}{\bv \lambda'\bv \Sigma^{-1}\bv \lambda} \\
&= \bv a' \bv \lambda
\end{align*}
$$

#### Bartlett factor scores

With $\bv a$ = $\bv \Theta^{-1} \bv \lambda (\bv \lambda' \bv \Theta^{-1} \bv \lambda)^{-1}$, and thus $\bv a' \bv \lambda = 1$,

$$
\begin{align*}
\text{Rel} 
&= \frac{1}{1 + (\bv \lambda' \bv \Theta^{-1} \bv \lambda)^{-1} \bv \lambda' \bv \Theta^{-1} \bv \Theta \bv \Theta^{-1} \bv \lambda (\bv \lambda' \bv \Theta^{-1} \bv \lambda)^{-1}} \\
&= \frac{1}{1 + (\bv \lambda' \bv \Theta^{-1} \bv \lambda)^{-1}}
\end{align*}
$$

### Sample Estimators

When factor scores are computed using sample estimates of $\bv \lambda$ and $\bv \Theta$, we have $\hat{\bv \mu}$ = $\hat{\bv \nu}$, and

$$
\begin{aligned}
  \tilde \eta & = \hat{\bv a}' (\bv y - \hat {\bv \mu}) = \hat{\bv a}' (\bv \nu + \bv \lambda \eta + \bv e - \hat{\bv \nu}) \\
  & = \hat{\bv a}' \bv \lambda \eta + \hat{\bv a}'(\bv \nu - \hat{\bv \nu}) + \hat{\bv a}' \bv e,
\end{aligned}
$$

where

- $\bv {\hat a}$ = vector of estimated weights of length $p$
- $\bv a$ = vector of true weights of length $p$,
- $\bv y$ = vector of observed scores of length $p$,
- $\lambda$ = vector of factor loadings of length $p$, 
- $\eta$ = latent variable,
- $\bv e$ = vector of measurement errors of length $p$,
- $\Theta$ = $\Var(\bv e)$ = $p \times p$ variance-covariance matrix of errors,
- $p$ = number of items.

If we assume that $\hat{\bv a}$ and $\hat{\bv \nu}$ are independent, and $E(\hat{\bv a}) = \bv a$, $\Var(\hat{\bv a}) = V_{\hat a}$, $E(\hat{\bv \nu}) = \bv \nu$, $\Var(\hat{\bv \nu}) = V_{\hat \nu}$, then using law of total variance,

$$
\begin{aligned}
  \Var(\tilde \eta) & = \E[\Var(\tilde \eta | \hat{\bv a})] + \Var[\E(\tilde \eta | \hat{\bv a})] \\
  & = \E[\hat {\bv a}' \bv \lambda \bv \lambda' \hat{\bv a} + \bv a' V_{\hat \nu} \bv a + \hat {\bv a}' \bv \Theta \hat{\bv a}] + V[\hat{\bv a}' 0] \\
  & = \bv a' \bv \lambda \bv \lambda' \bv a + \Tr(V_{\hat a} \bv \lambda \bv \lambda') + \bv a' V_{\hat \nu} \bv a + \Tr(V_{\hat a} V_{\hat \nu}) + \bv a' \bv \Theta \bv a + \Tr(V_{\hat a} \bv \Theta)
\end{aligned}
$$

So, accounting for sampling error,

$$
\rho = \frac{(\bv a' \bv \lambda)^2 + \bv \lambda'V_{\hat a} \bv \lambda}{(\bv a' \bv \lambda)^2 + \bv a'\bv \Theta \bv a + \bv \lambda'V_{\hat a}\bv \lambda + \bv a' V_{\hat \nu} \bv a + \Tr(V_{\hat a} V_{\hat \nu}) + \Tr(\bv \Theta \bv V_{\hat a})}
$$

The sample estimates can be obtained by substituting in the sample estimates of $\bv \lambda$ and $\bv \Theta$ and using the corresponding form of $\bv a$.

#### Quadratic formula for regression factor scores

The reliability formula for the regression factor scores derived using a quadratic formula is

$$\text{Rel} = \frac{1\pm\sqrt{1 - 4\theta_{corrected}/\psi}}{2}$$

where $\theta_{corrected}$ is the corrected error term for the factor scores.

```{r}
set.seed(1247)
# Simulate small data with 6 items
lambda <- seq(.3, .9, length.out = 6)
theta <- 1 - lambda^2
num_obs <- 100

eta <- rnorm(num_obs)
y <- t(
  tcrossprod(lambda, eta) + rnorm(num_obs * length(lambda), sd = sqrt(theta))
)
# Run cfa
fit <- cfa("f =~ y1 + y2 + y3 + y4 + y5 + y6",
  data = data.frame(y) |> setNames(paste0("y", seq_along(lambda))),
  std.lv = TRUE,
  meanstructure = TRUE
)
fs <- lavPredict(fit, fsm = TRUE)
head(fs)
```

# Simulation of each component

```{r}
# Population matrix
true_sigma <- tcrossprod(lambda) + diag(theta)
(true_rel <- crossprod(lambda, solve(true_sigma, lambda)))
# Squared correlation
cor(eta, fs)^2
# Sample estimate of reliability, ignoring error
(est_rel <- with(lavInspect(fit, what = "est"), {
  crossprod(lambda, solve(fit@implied$cov[[1]], lambda))
}))
# Corrected reliability estimator
# Factor score function using parameter estimates
get_a <- function(lambda, theta) {
  solve(tcrossprod(lambda) + diag(theta), lambda)
}
lam <- lavInspect(fit, what = "est")$lambda
th <- lavInspect(fit, what = "est")$theta
jac_a <- lavaan::lav_func_jacobian_complex(
  \(x) get_a(x[1:6], x[7:12]),
  x = c(lam, diag(th))
)
va <- jac_a %*% vcov(fit)[1:12, 1:12] %*% t(jac_a)
aa <- tcrossprod(get_a(lam, diag(th))) + va
vnu <- vcov(fit)[13:18, 13:18]
sum(diag(tcrossprod(lam) %*% aa)) /
  sum(diag((fit@implied$cov[[1]] + vnu) %*% aa))
```

```{r}
#| eval: false
# num_sims <- 100000
# out <- matrix(nrow = 6 * 6, ncol = num_sims)

# for (i in seq_len(num_sims)) {
#   # eta <- rnorm(num_obs)
#   # err <- matrix(
#   #   rnorm(num_obs * length(lambda), sd = sqrt(theta)),
#   #   ncol = num_obs
#   # )
#   # y <- t(
#   #   tcrossprod(lambda, eta) + err
#   # )
#   y <- MASS::mvrnorm(num_obs, mu = lambda * 0,
#                      Sigma = true_sigma)
#   inv_covy <- solve(cov(y) * (num_obs - 1) / num_obs)
#   out[, i] <- inv_covy
#   # covy <- cov(y)
#   # out[, i] <- covy[lower.tri(covy, diag = TRUE)]
# }
# # rowMeans(out) / (true_sigma[lower.tri(true_sigma, diag = TRUE)])
# mean(rowMeans(out) / solve(true_sigma))
# # mean(rowMeans(out) / solve(true_sigma)[lower.tri(true_sigma, diag = TRUE)])
# num_obs / (num_obs - length(lambda) - 2)
```

```{r}
#| eval: false
out <- matrix(ncol = 7, nrow = num_sims)

# get_rel <- function(lam, dth, vc = NULL, method = c("adjust", "quad")) {
#   sigma <- tcrossprod(lam) + diag(dth)
#   if (is.null(vc)) {
#     # Formula 1 (no adjustment)
#     crossprod(lam, solve(sigma, lam))
#   } else {
#     method <- match.arg(method)
#     jac_a <- lavaan::lav_func_jacobian_complex(
#       function(x) {
#         R2spa:::compute_a_from_mat(
#           lambda = x[1:6],
#           theta = diag(x[7:12]),
#           psi = matrix(1)
#         )
#       },
#       c(lam, dth)
#     )
#     va <- jac_a %*% vc %*% t(jac_a)
#     ahat <- crossprod(lam, solve(sigma))
#     aa <- crossprod(ahat) + va
#     if (method == "adjust") {
#       # Formula 2: adjust for both error and true variances
#       sum(diag(tcrossprod(lam) %*% aa)) / sum(diag(sigma %*% aa))
#     } else if (method == "quad") {
#       # Formula 3: solve quadratic equation with adjusted error
#       ev_fs <- sum(diag(diag(dth) %*% aa))
#       (1 + sqrt(1 - 4 * ev_fs)) / 2
#     }
#   }
# }

# bc_rel <- function(fit, nsim = 500, adjust = FALSE, ...) {
#   mc_sim <- MASS::mvrnorm(nsim, mu = coef(fit), Sigma = vcov(fit))
#   if (adjust) {
#     vc <- vcov(fit)
#   } else {
#     vc <- NULL
#   }
#   mc_rel <- apply(mc_sim, MARGIN = 1,
#                   FUN = \(x) get_rel(x[1:6], x[7:12], vc = vc, ...))
#   2 * with(
#     lavInspect(fit, what = "est"),
#     get_rel(lambda, dth = diag(theta), vc = vc, ...)
#   ) - mean(mc_rel) # bias-corrected
# } # Seems reasonable when n = 100

# Function for getting all versions
all_rel <- function(lam, th, vc) {
  sigma <- tcrossprod(lam) + th
  ahat <- crossprod(lam, solve(sigma))
  # Formula 1: no adjustment
  rel1 <- ahat %*% lam

  jac_a <- lavaan::lav_func_jacobian_complex(
    function(x) {
      R2spa:::compute_a_from_mat(
        lambda = x[seq_along(lam)],
        theta = diag(x[-(seq_along(lam))]),
        psi = matrix(1)
      )
    },
    c(lam, diag(th))
  )
  va <- jac_a %*% vcov(fit) %*% t(jac_a)
  aa <- crossprod(ahat) + va
  # Formula 2: adjust for both error in weights and true variances
  rel2 <- sum(diag(tcrossprod(lam) %*% aa)) / sum(diag(sigma %*% aa))

  ev_fs <- sum(diag(th %*% aa))
  # Formula 3: solve quadratic equation with adjusted error
  rel3 <- (1 + sqrt(1 - 4 * ev_fs)) / 2

  c(rel1, rel2, rel3)
}

# Function for getting all bias-corrected versions
all_bc_rel <- function(fit, nsim = 500) {
  vc <- vcov(fit)
  mc_sim <- MASS::mvrnorm(nsim, mu = coef(fit), Sigma = vc)
  mc_rel <- apply(mc_sim, MARGIN = 1,
                  FUN = function(x) {
                    all_rel(x[1:6], th = diag(x[7:12]), vc = vc)
                  })
  2 * with(
    lavInspect(fit, what = "est"),
    all_rel(lambda, th = theta, vc = vc)
  ) - rowMeans(mc_rel) # bias-corrected
}

for (i in seq_len(num_sims)) {
  eta <- rnorm(num_obs)
  err <- matrix(
    rnorm(num_obs * length(lambda), sd = sqrt(theta)),
    ncol = num_obs
  )
  y <- t(
    tcrossprod(lambda, eta) + err
  )
  # Run cfa
  fit <- cfa("f =~ y1 + y2 + y3 + y4 + y5 + y6",
    data = data.frame(y) |> setNames(paste0("y", seq_along(lambda))),
    std.lv = TRUE
  )
  pars_fit <- lavInspect(fit, what = "est")
  tilde_eta <- lavPredict(fit, fsm = TRUE)
  true_rel <- cor(tilde_eta, eta)^2
  est_a <- attr(tilde_eta, "fsm")[[1]]
  out[i, ] <- c(
    true_rel, # est_a %*% lambda, est_a %*% diag(theta) %*% t(est_a),
    # est_a %*% pars_fit$lambda, # need clarification
    # est_a %*% pars_fit$theta %*% t(est_a),
    # all_bc_rel(fit)
    all_rel(pars_fit$lambda, pars_fit$theta, vcov(fit)), 
    all_bc_rel(fit)
    # crossprod(pars_fit$lambda, solve(true_sigma, pars_fit$lambda)),
    # crossprod(lambda, solve(lavInspect(fit, what = "sigma"), lambda)),
    # sum(diag(vcov(pop_fit)[1:6, 1:6] %*% lavInspect(fit, what = "sigma")))
    )
  setTxtProgressBar(
    txtProgressBar(min = 0, max = num_sims, style = 3, width = 50, char = "="), 
    i
  )
}
colnames(out) <- c("true_rel", "no_adj_rel", "adj_rel", "quadratic", 
                   "no_adj_rel_bc", "adj_rel_bc", "quadratic_bc")

# save the file
saveRDS(out, "vignettes/sim_results_reliability.RDS")
```


```{r}
num_sims <- 2000
# Create a table for raw biases: mean(estimated - true)
# example: mean(out[, "no_adj_rel"] - out[, "true_rel"])
out <- read.csv("sim_output.csv")
mean(out[, "no_adj_rel"] - out[, "true_rel"])

# Creating a data frame
data <- data.frame(
  Type = c("no_adj_rel", "adj_rel", "quadratic",
           "no_adj_rel_bc", "adj_rel_bc", "quadratic_bc"),
  Raw_Biases = c(mean(out[, "no_adj_rel"] - out[, "true_rel"]), 
                 mean(out[, "adj_rel"] - out[, "true_rel"]), 
                 mean(out[, "quadratic"] - out[, "true_rel"]), 
                 mean(out[, "no_adj_rel_bc"] - out[, "true_rel"]), 
                 mean(out[, "adj_rel_bc"] - out[, "true_rel"]), 
                 mean(out[, "quadratic_bc"] - out[, "true_rel"], na.rm = TRUE))
)

# Displaying the data frame
print(data)

# Create a table for RMSE: sqrt(1 / num_sims * sum(estimated - true)^2)
# example: sqrt(1 / num_sims * sum(out[, "no_adj_rel"] - out[, "true_rel"])^2)
RMSE_data <- data.frame(
  Type = c("no_adj_rel", "adj_rel", "quadratic", "no_adj_rel_bc", "adj_rel_bc", "quadratic_bc"),
  RMSE = c(sqrt(1 / num_sims * sum(out[, "no_adj_rel"] - out[, "true_rel"])^2),
           sqrt(1 / num_sims * sum(out[, "adj_rel"] - out[, "true_rel"])^2),
           sqrt(1 / num_sims * sum(out[, "quadratic"] - out[, "true_rel"])^2),
           sqrt(1 / num_sims * sum(out[, "no_adj_rel_bc"] - out[, "true_rel"])^2),
           sqrt(1 / num_sims * sum(out[, "adj_rel_bc"] - out[, "true_rel"])^2),
           sqrt(1 / num_sims * sum(out[, "quadratic_bc"] - out[, "true_rel"], na.rm = TRUE)^2))
)
print(RMSE_data)

```

```{r}
# Boxplot to compare the bias across formula
bias_tab <- data.frame(no_adj_rel = out[, "no_adj_rel"] - out[, "true_rel"], 
                       adj_rel = out[, "adj_rel"] - out[, "true_rel"], 
                       quadratic = out[, "quadratic"] - out[, "true_rel"]) |>
  pivot_longer(everything(), names_to = "formula", values_to = "bias")
bias_tab |>
  ggplot(aes(x = formula, y = bias, col = formula)) +
  geom_boxplot() +
  geom_hline(yintercept = 0)
```

