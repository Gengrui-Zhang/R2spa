---
title: "Correction to Measurement Error"
author: "Mark Lai"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{correction-error}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

When computing the reliability or the standard errors of factor scores, we typically condition on the estimates of the model parameters, aka assuming that they are known. However, when sample size is small, ignoring those can be problematic as the model parameters are imprecise. This vignette provides details on how to obtain more realistic estimates of standard errors of factor scores.

# Unidimensional Case

Consider the conditional variance of the factor scores, $Var(\tilde \eta \mid \eta)$, with $\tilde \eta = \boldsymbol{a}^\top \boldsymbol{y}$. From the factor model, we have

$$Var(\tilde \eta \mid \eta) = Var(\boldsymbol{a}^\top \boldsymbol{\lambda} \eta + \boldsymbol{a}^\top \boldsymbol{\varepsilon} \mid \eta) = \eta^2 Var(\boldsymbol{a}^\top \boldsymbol{\lambda}) + Var(\boldsymbol{a}^\top \boldsymbol{\varepsilon}).$$

The second term is the error component. Here is a small simulation:

```{r}
#' Load package and set seed
library(lavaan)
library(R2spa)
set.seed(191254)

#' Fixed conditions
num_obs <- 100
lambda <- c(.3, .5, .7, .9)
theta <- c(.5, .4, .5, .7)

#' Simulate true score, and standardized
eta <- rnorm(num_obs)
eta <- (eta - mean(eta))
eta <- eta / sqrt(mean(eta^2))

#' Function for simulating y
simy <- function(eta, lambda) {
    t(tcrossprod(lambda, eta) +
          rnorm(length(lambda) * length(eta), sd = sqrt(theta)))
}
```

```{r, eval = FALSE}
#' Simulation
nsim <- 2500
tfsy_sim <- fsy_sim <- matrix(NA, nrow = num_obs, ncol = nsim)
# Also save the scoring matrix
a_sim <- matrix(NA, nrow = length(lambda), ncol = nsim)
for (i in seq_len(nsim)) {
    y <- simy(eta = eta, lambda = lambda)
    tfsy_sim[, i] <- R2spa::compute_fscore(
      y, lambda = lambda, theta = diag(theta), psi = matrix(1)
    )
    fsy <- R2spa::get_fs(y, std.lv = TRUE)
    fsy_sim[, i] <- fsy[, 1]
    a_sim[, i] <- attr(fsy, which = "scoring_matrix")
}
```

```{r echo = FALSE, eval = FALSE}
saveRDS(list(tfsy_sim, fsy_sim, a_sim), file = "sim_correction-error.RDS")
```

```{r echo = FALSE, include = FALSE}
tfsy_sim <- readRDS("sim_correction-error.RDS")[[1]]
fsy_sim <- readRDS("sim_correction-error.RDS")[[2]]
a_sim <- readRDS("sim_correction-error.RDS")[[3]]
```

```{r}
#' Average conditional variance
# a known
apply(tfsy_sim, 1, var) |> mean()
# compare to theoretical value
true_a <- R2spa:::compute_a_reg(lambda, psi = matrix(1), theta = diag(theta))
true_a %*% diag(theta) %*% t(true_a)
# a estimated
apply(fsy_sim, 1, var) |> mean() 
```

The standard errors are larger when using sample estimates of weights.

When $\boldsymbol{a}$ is known, the error variance is simply $\boldsymbol{a}^\top \boldsymbol{\Theta} \boldsymbol{a}$. However, when $\boldsymbol{a}$ is unknown and estimated as $\tilde{\boldsymbol{a}}$ from the data, 

$$Var(\tilde{\boldsymbol{a}}^\top \boldsymbol{\varepsilon}) = E[Var(\tilde{\boldsymbol{a}}^\top \boldsymbol{\varepsilon} \mid \tilde{\boldsymbol{a}})] + Var[E(\tilde{\boldsymbol{a}}^\top \boldsymbol{\varepsilon} \mid \tilde{\boldsymbol{a}})].$$

Under the assumption of $E(\boldsymbol{\varepsilon})$ = $\boldsymbol{0}$, the second term is 0. The first term is an expectation of a quadratic form, 

$$E(\tilde{\boldsymbol{a}}^\top \boldsymbol{\Theta} \tilde{\boldsymbol{a}}) = E(\tilde{\boldsymbol{a}}^\top) \boldsymbol{\Theta} E(\tilde{\boldsymbol{a}}) + \mathrm{tr}(\boldsymbol{\Theta} \boldsymbol{V}_{\tilde{\boldsymbol{a}}})$$

where $\boldsymbol{V}_{\tilde{\boldsymbol{a}}}$ is the covariance matrix of $\tilde{\boldsymbol{a}}$. We can see this using the simulated data

```{r}
correct_fac <- sum(diag(diag(theta) %*% cov(t(a_sim))))
true_a %*% diag(theta) %*% t(true_a) + correct_fac
```

## Correction Factor

We can approximate $\boldsymbol{V}_{\tilde{\boldsymbol{a}}}$ using the [delta method](https://en.wikipedia.org/wiki/Delta_method), and the other terms in the above using sample estimates

$$\tilde{\boldsymbol{a}}^\top \hat{\boldsymbol{\Theta}} \tilde{\boldsymbol{a}} + \mathrm{tr}(\hat{\boldsymbol{\Theta}} \hat{\boldsymbol{V}}_{\tilde{\boldsymbol{a}}}).$$

The delta method estimate, $\hat{\boldsymbol{V}}_{\tilde{\boldsymbol{a}}}$, is

$$\boldsymbol{J}_{\tilde{\boldsymbol{a}}}(\boldsymbol{\theta})^\top \hat{\boldsymbol{V}}_{\tilde{\boldsymbol{\theta}}} \boldsymbol{J}_{\tilde{\boldsymbol{a}}}(\boldsymbol{\theta}),$$

where $\boldsymbol{J}_{\tilde{\boldsymbol{a}}}(\boldsymbol{\theta})$ is the Jacobian matrix, and $\hat{\boldsymbol{V}}_{\tilde{\boldsymbol{\theta}}}$ is the sampling variance of the sample estimator of $\boldsymbol{\theta}$. The Jacobian can be approximated using finite difference with `lavaan::lav_func_jacobian_complex()`.

```{r}
y <- simy(eta = eta, lambda = lambda)
cfa_fit <- cfa("f =~ y1 + y2 + y3 + y4",
               data = setNames(data.frame(y), paste0("y", 1:4)),
               std.lv = TRUE)
# Jacobian
R2spa:::compute_a(coef(cfa_fit), lavobj = cfa_fit)
jac_a <- lavaan::lav_func_jacobian_complex(
  \(x) R2spa:::compute_a(x, lavobj = cfa_fit),
  coef(cfa_fit)
)
sum(diag(lavInspect(cfa_fit, what = "est")$theta %*%
           jac_a %*% vcov(cfa_fit) %*% t(jac_a)))
fsy <- R2spa::get_fs(y, std.lv = TRUE)
attr(fsy, which = "av_efs")
```

# Multidimensional Case
