[{"path":"https://gengrui-zhang.github.io/R2spa/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 R2spa authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/R2spa.html","id":"single-group-single-predictor","dir":"Articles","previous_headings":"","what":"Single group, single predictor1","title":"Two-Stage Path Analysis (2S-PA) Model Examples","text":"call tspa(), data frame factor scores needed latent variables. get data frame, apply get_fs() latent variables specify model parameter respective definitions. Combine factor scores latent variable using cbind() can used tspa() model building. build Two-Stage Path Analysis model, simply call tspa() function model = regressions, data = combined factor score data frame using get_fs(), specify standard error either list data frame. Values standard error can found column named fs_[variable name]_se. example, standard error latent variable ind60 can found column fs_ind60_se fs_dat data frame. view Two-Stage Path Analysis model, use attributes([model name])$tspaModel. Function cat() can help tidy model output. output, values error constraints computed squaring standard errors previous section.","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4    # regressions     dem60 ~ ind60 ' fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60) tspa_fit <- tspa(model = \"dem60 ~ ind60\",                   data = fs_dat,                   se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472)) cat(attributes(tspa_fit)$tspaModel) ## # latent variables (indicated by factor scores) ## ind60=~ c(1) * fs_ind60 ## dem60=~ c(1) * fs_dem60 ##  ## # constrain the errors ## fs_ind60~~ c(0.01472861368225) * fs_ind60 ## fs_dem60~~ c(0.45649913886784) * fs_dem60 ##  ## # structural model ## dem60 ~ ind60"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/R2spa.html","id":"single-group-multiple-predictors","dir":"Articles","previous_headings":"","what":"Single group, multiple predictors","title":"Two-Stage Path Analysis (2S-PA) Model Examples","text":"call tspa(), data frame factor scores needed latent variables. get data frame, apply get_fs() latent variables specify model parameters respective definitions. Combine factor scores latent variables using cbind() call tspa() model building. build Two-Stage Path Analysis model, simply call tspa() function model = regressions (predictors), data = factor score data frame created combining results get_fs(), specify standard errors either list data frame. Values standard errors can found column named fs_[variable name]_se.2 example, standard error latent variable ind60 can found column fs_ind60_se fs_dat data frame. output model, values error constraints computed squaring standard errors.","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # # residual correlations   #   y1 ~~ y5   #   y2 ~~ y4 + y6   #   y3 ~~ y7   #   y4 ~~ y8   #   y6 ~~ y8 ' fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat_dem65 <- get_fs(data = PoliticalDemocracy,                         model = \"dem65 =~ y5 + y6 + y7 + y8\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60, fs_dat_dem65) tspa_3var_fit <- tspa(model = \"dem60 ~ ind60                           dem65 ~ ind60 + dem60\",                        data = fs_dat,                        se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472,                                     dem65 = 0.5724405)) cat(attributes(tspa_3var_fit)$tspaModel) ## # latent variables (indicated by factor scores) ## ind60=~ c(1) * fs_ind60 ## dem60=~ c(1) * fs_dem60 ## dem65=~ c(1) * fs_dem65 ##  ## # constrain the errors ## fs_ind60~~ c(0.01472861368225) * fs_ind60 ## fs_dem60~~ c(0.45649913886784) * fs_dem60 ## fs_dem65~~ c(0.32768812604025) * fs_dem65 ##  ## # structural model ## dem60 ~ ind60 ##                           dem65 ~ ind60 + dem60"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/R2spa.html","id":"multigroup-single-predictor","dir":"Articles","previous_headings":"","what":"Multigroup, single predictor","title":"Two-Stage Path Analysis (2S-PA) Model Examples","text":"call tspa(), data frame factor scores needed multigroup variables. get data frame, apply get_fs() multigroup variables specify model parameter respective definitions. Combine factor scores multigroup variables using cbind() can fed tspa() model building. build Two-Stage Path Analysis model, simply call tspa() function model = regression relation. Specify standard error either list data frame. Values standard error can found column named fs_[variable name]_se. example, standard error multigroup variable visual can found column fs_visual_se fs_hs data frame. get standard errors group faster, unique() can called upon standard error column. example, case, unique(fs_hs$fs_visual_se) can called get standard errors multigroup variable visual. Function standardizedsolution() enables user view table standard error, z score, p-value, lower bound confidence interval multigroup regression relation. view Two-Stage Path Analysis model multigroup, use attributes([model name])$tspaModel. Function cat() can help tidy model output. output, values error constraints computed squaring standard errors previous section.","code":"model <- '    # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9    # regressions     visual ~ speed ' hs_mod <- ' visual =~ x1 + x2 + x3 speed =~ x7 + x8 + x9 '  # get factor scores fs_dat_visual <- get_fs(data = HolzingerSwineford1939,                          model = \"visual =~ x1 + x2 + x3\",                          group = \"school\") fs_dat_speed <- get_fs(data = HolzingerSwineford1939,                         model = \"speed =~ x7 + x8 + x9\",                         group = \"school\") fs_hs <- cbind(do.call(rbind, fs_dat_visual),                do.call(rbind, fs_dat_speed)) # tspa model tspa_fit <- tspa(model = \"visual ~ speed\",                  data = fs_hs,                  se_fs = data.frame(visual = c(0.3391326, 0.311828),                                     speed = c(0.2786875, 0.2740507)),                  group = \"school\"                  # group.equal = \"regressions\"                  ) stdsol <- standardizedsolution(tspa_fit) subset(stdsol, subset = op == \"~\") ##       lhs op   rhs group est.std    se     z pvalue ci.lower ci.upper ## 5  visual  ~ speed     1   0.277 0.114 2.423  0.015    0.053    0.501 ## 16 visual  ~ speed     2   0.439 0.095 4.627  0.000    0.253    0.625 cat(attributes(tspa_fit)$tspaModel) ## # latent variables (indicated by factor scores) ## visual=~ c(1, 1) * fs_visual ## speed=~ c(1, 1) * fs_speed ##  ## # constrain the errors ## fs_visual~~ c(c(0.11501092038276, 0.097236701584)) * fs_visual ## fs_speed~~ c(c(0.07766672265625, 0.07510378617049)) * fs_speed ##  ## # structural model ## visual ~ speed"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"simulate-data","dir":"Articles","previous_headings":"","what":"Simulate Data","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"","code":"#' Population parameters num_obs <- 5000  # relatively large sample cov_xm <- .45 betas <- c(.2, .3, -.4) # Results seem more accurate with more items lambdax <- 1.7 * c(.6, .7, .8, .5, .5, .3, .9, .7, .6)  # on normal ogive metric lambdam <- 1.7 * c(.8, .7, .7, .6, .5)  # on normal ogive metric lambday <- c(.5, .7, .9) thresx <- matrix(c(1, 1.5, 1.3, 0.5, 2, -0.5, -1, 0.5, 0.8), nrow = 1)  # binary thresm <- matrix(c(-2, -2, -0.4, -0.5, 0,                    -1.5, -0.5, 0, 0, 0.5,                    -1, 1, 1.5, 0.8, 1), nrow = 3,                  byrow = TRUE)  # ordinal with 4 categories inty <- 1.5  # intercept of y # error variance of y r2y <- crossprod(     betas,     matrix(c(1, cov_xm, 0, cov_xm, 1, 0, 0, 0, 1 + cov_xm^2), nrow = 3)     ) %*% betas evar <- as.numeric(1 - r2y)  #' Simulate latent variables X and M with variances of 1, and the disturbance of #' Y cov_xm_ey <- matrix(c(1, cov_xm, 0,                       cov_xm, 1, 0,                       0, 0, evar), nrow = 3) eta <- MASS::mvrnorm(num_obs, mu = rep(0, 3), Sigma = cov_xm_ey,                      empirical = TRUE) # Add product term eta <- cbind(eta, eta[, 1] * eta[, 2])  #' Compute latent y (standardized) etay <- inty + eta[, -3] %*% betas + eta[, 3] # Verify the structural coefficients with eta known lm(etay ~ eta[, -3]) #>  #> Call: #> lm(formula = etay ~ eta[, -3]) #>  #> Coefficients: #> (Intercept)   eta[, -3]1   eta[, -3]2   eta[, -3]3   #>      1.4938       0.2002       0.2999      -0.3862  #' Simulate y (continuous) y <- t(     lambday %*% t(etay) +         rnorm(num_obs * length(lambday), sd = sqrt(1 - lambday^2)) )  #' Simulate latent continuous responses for X and M xstar <- eta[, 1] %*% t(lambdax) + rnorm(num_obs * length(lambdax)) mstar <- eta[, 2] %*% t(lambdam) + rnorm(num_obs * length(lambdam)) #' Obtain categorical items x <- vapply(     seq_along(lambdax),     FUN = \\(i) {         findInterval(xstar[, i], thresx[, i])     },     FUN.VALUE = numeric(num_obs)) m <- vapply(     seq_along(lambdam),     FUN = \\(i) {         findInterval(mstar[, i], thresm[, i])     },     FUN.VALUE = numeric(num_obs))"},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"y","dir":"Articles","previous_headings":"Compute Factor Scores","what":"Y","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"","code":"fsy <- get_fs(data = y, std.lv = TRUE)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"x","dir":"Articles","previous_headings":"Compute Factor Scores","what":"X","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"example, use EAP scores, conceptually similar regression scores continuous data.","code":"irtx <- mirt(data.frame(x), itemtype = \"2PL\",              verbose = FALSE)  # IRT (2-PL) for x marginal_rxx(irtx)  # marginal reliability #> [1] 0.7606752 fsx <- fscores(irtx, full.scores.SE = TRUE) empirical_rxx(fsx)  # empirical reliability #>        F1  #> 0.7732511"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"m","dir":"Articles","previous_headings":"Compute Factor Scores","what":"M","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"","code":"irtm <- mirt(data.frame(m), itemtype = \"graded\",              verbose = FALSE)  # IRT (GRM) for m marginal_rxx(irtm)  # marginal reliability #> [1] 0.7931549 fsm <- fscores(irtm, full.scores.SE = TRUE) empirical_rxx(fsm)  # empirical reliability #>        F1  #> 0.7942367"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"loading-and-error-variances-for-product-of-factor-scores","dir":"Articles","previous_headings":"","what":"Loading and Error Variances for Product of Factor scores","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"Let \\(\\tilde \\eta_X\\) \\(\\tilde \\eta_M\\) factor scores predictor moderator. \\[\\tilde \\eta_X \\tilde \\eta_M = (\\lambda_X \\eta_X + e_{\\tilde X}) (\\lambda_M \\eta_M + e_{\\tilde M}) = (\\lambda_X \\lambda_M \\eta_X \\eta_M) + \\lambda_X \\eta_X e_M + \\lambda_M \\eta_M e_X + e_X e_M\\] loading \\(\\lambda_X \\lambda_M\\), error variance product indicator \\[\\lambda_X^2 V(\\eta_X) V(e_M) + \\lambda_M^2 V(\\eta_M) V(e_X) + V(e_X) V(e_M)\\]","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"directly-using-factor-scores","dir":"Articles","previous_headings":"","what":"Directly Using Factor Scores","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"","code":"# Assemble data fs_dat <- data.frame(fsy[c(1, 3, 4)], fsx, fsm) |>     setNames(c(         \"fsy\", \"rel_fsy\", \"ev_fsy\",         \"fsx\", \"se_fsx\", \"fsm\", \"se_fsm\"     )) |>     # Compute reliability; only needed for 2S-PA     within(expr = {         rel_fsx <- 1 - se_fsx^2         rel_fsm <- 1 - se_fsm^2         ev_fsx <- se_fsx^2 * (1 - se_fsx^2)         ev_fsm <- se_fsm^2 * (1 - se_fsm^2)     }) |>     # Add interaction     within(expr = {         fsxm <- fsx * fsm         ld_fsxm <- rel_fsx * rel_fsm         ev_fsxm <- rel_fsx^2 * ev_fsm + rel_fsm^2 * ev_fsx + ev_fsm * ev_fsx     }) lm(fsy ~ fsx * fsm, data = fs_dat)  # some bias #>  #> Call: #> lm(formula = fsy ~ fsx * fsm, data = fs_dat) #>  #> Coefficients: #> (Intercept)          fsx          fsm      fsx:fsm   #>     0.09867      0.19539      0.25989     -0.35788"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"two-stage-path-analysis","dir":"Articles","previous_headings":"","what":"Two-Stage Path Analysis","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"Create OpenMx model without latent variables Create loading error covariance matrices Combine (1) (2), run 2S-PA","code":"fsreg_mx <- mxModel(\"str\",     type = \"RAM\",     manifestVars = c(\"fsy\", \"fsx\", \"fsm\", \"fsxm\"),     # Path coefficients     mxPath(         from = c(\"fsx\", \"fsm\", \"fsxm\"), to = \"fsy\",         values = 0     ),     # Variances     mxPath(         from = c(\"fsy\", \"fsx\", \"fsm\", \"fsxm\"),         to = c(\"fsy\", \"fsx\", \"fsm\", \"fsxm\"),         arrows = 2,         values = c(0.6, 1, 1, 1)     ),     # Covariances     mxPath(         from = c(\"fsx\", \"fsm\", \"fsxm\"),         to = c(\"fsx\", \"fsm\", \"fsxm\"),         arrows = 2, connect = \"unique.pairs\",         values = 0     ),     mxPath(         from = \"one\", to = c(\"fsy\", \"fsx\", \"fsm\", \"fsxm\"),         values = 0     ) ) fsreg_umx <- umxLav2RAM(     \"       fsy ~ b1 * fsx + b2 * fsm + b3 * fsxm       fsy + fsx + fsm + fsxm ~ 1       fsx ~~ vx * fsx + fsm + fsxm       fsm ~~ vm * fsm + fsxm     \",     printTab = FALSE) #>  #> ?plot.MxModel options: std, means, digits, strip_zero, file, splines=T/F/ortho,..., min=, max =, same = , fixed, resid= 'circle|line|none' DiagrammeR::grViz(omxGraphviz(fsreg_mx)) #> digraph \"str\" { #>   node [style=filled, fontname=\"Arial\", fontsize=16]; #>       /* Manifest Variables */ #>       { rank = max; fsy; fsx; fsm; fsxm } #>   fsy [shape=square, fillcolor=\"#a9fab1\", height=0.5, width=0.5]; #>   fsx [shape=square, fillcolor=\"#a9fab1\", height=0.5, width=0.5]; #>   fsm [shape=square, fillcolor=\"#a9fab1\", height=0.5, width=0.5]; #>   fsxm [shape=square, fillcolor=\"#a9fab1\", height=0.5, width=0.5]; #> /* Means */ #>   one [shape=triangle]; #> /* Paths */ #>   fsx -> fsy[dir=forward]; #>   fsm -> fsy[dir=forward]; #>   fsxm -> fsy[dir=forward]; #>   fsy -> fsy[dir=both, headport=s, tailport=s]; #>   fsx -> fsx[dir=both, headport=s, tailport=s]; #>   fsx -> fsm[dir=both]; #>   fsx -> fsxm[dir=both]; #>   fsm -> fsm[dir=both, headport=s, tailport=s]; #>   fsm -> fsxm[dir=both]; #>   fsxm -> fsxm[dir=both, headport=s, tailport=s]; #>   one -> fsy[dir=forward]; #>   one -> fsx[dir=forward]; #>   one -> fsm[dir=forward]; #>   one -> fsxm[dir=forward]; #> } # Loading matL <- mxMatrix(     type = \"Diag\", nrow = 4, ncol = 4,     free = FALSE,     labels = c(\"data.rel_fsy\", \"data.rel_fsx\", \"data.rel_fsm\", \"data.ld_fsxm\"),     name = \"L\" ) # Error matE <- mxMatrix(     type = \"Diag\", nrow = 4, ncol = 4,     free = FALSE,     labels = c(\"data.ev_fsy\", \"data.ev_fsx\", \"data.ev_fsm\", \"data.ev_fsxm\"),     name = \"E\" ) tspa_mx <-     tspa_mx_model(         mxModel(fsreg_umx,                 mxAlgebra(b1 * sqrt(vx), name = \"stdx_b1\"),                 mxAlgebra(b2 * sqrt(vm), name = \"stdx_b2\"),                 mxAlgebra(b3 * sqrt(vx) * sqrt(vm), name = \"stdx_b3\"),                 mxCI(c(\"stdx_b1\", \"stdx_b2\", \"stdx_b3\"))),         data = fs_dat,         mat_ld = matL, mat_vc = matE) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx, intervals = TRUE) #> Running 2SPAD with 14 parameters # Summarize the results (takes a minute or so) # Also include the X-Standardized coefficients summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>              name matrix  row  col      Estimate  Std.Error A #> 1              b1   m1.A  fsy  fsx  2.064624e-01 0.01898040   #> 2              b2   m1.A  fsy  fsm  2.989998e-01 0.01871068   #> 3              b3   m1.A  fsy fsxm -4.152434e-01 0.01870233   #> 4    fsy_with_fsy   m1.S  fsy  fsy  6.398847e-01 0.01957377   #> 5              vx   m1.S  fsx  fsx  9.991969e-01 0.02776722   #> 6    fsm_with_fsx   m1.S  fsx  fsm  4.455208e-01 0.01927466   #> 7              vm   m1.S  fsm  fsm  9.996933e-01 0.02685963   #> 8   fsx_with_fsxm   m1.S  fsx fsxm  2.425816e-02 0.02230940   #> 9   fsm_with_fsxm   m1.S  fsm fsxm  4.883994e-03 0.02200249   #> 10 fsxm_with_fsxm   m1.S fsxm fsxm  1.035316e+00 0.03836077   #> 11     one_to_fsy   m1.M    1  fsy  1.871730e-01 0.01613095   #> 12     one_to_fsx   m1.M    1  fsx -1.368763e-04 0.01615981   #> 13     one_to_fsm   m1.M    1  fsm -6.265502e-05 0.01587408   #> 14    one_to_fsxm   m1.M    1 fsxm  4.506424e-01 0.01832830   #>  #> confidence intervals: #>                     lbound   estimate     ubound note #> m1.stdx_b1[1,1]  0.1693079  0.2063795  0.2436209      #> m1.stdx_b2[1,1]  0.2622028  0.2989540  0.3356675      #> m1.stdx_b3[1,1] -0.4563615 -0.4150129 -0.3757700      #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:             14                  39986              50641.35 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 5000/40000 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -29330.65               50669.35                 50669.43 #> BIC:     -289927.14               50760.59                 50716.10 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-01-16 07:13:24  #> Wall clock time: 144.9299 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.11  #> Need help?  See help(mxSummary) # Standard errors for X-Standardized coefficients mxSE(m1.stdx_b1, tspa_mx_fit) #> Treating first argument as an expression #>            [,1] #> [1,] 0.01891785 mxSE(m1.stdx_b2, tspa_mx_fit) #> Treating first argument as an expression #>            [,1] #> [1,] 0.01868464 mxSE(m1.stdx_b3, tspa_mx_fit) #> Treating first argument as an expression #>            [,1] #> [1,] 0.02048485"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"first-order-correction-for-se","dir":"Articles","previous_headings":"","what":"First-order correction for SE","title":"Corrected Standard Errors","text":"\\[\\hat V_{\\gamma, c} = \\hat V_{\\gamma} + \\boldsymbol{J}_\\boldsymbol{\\gamma}(\\hat{\\boldsymbol{\\theta}}) \\hat V_{\\theta} \\boldsymbol{J}_\\boldsymbol{\\gamma}(\\hat{\\boldsymbol{\\theta}})^\\top,\\] \\(\\boldsymbol{J}_\\boldsymbol{\\gamma}\\) Jacobian matrix \\(\\hat{\\boldsymbol{\\gamma}}\\) respect \\(\\boldsymbol{\\theta}\\), \\[\\hat V_{\\gamma, c} = \\hat V_{\\gamma} + (\\boldsymbol{H}_\\gamma)^{-1} \\left(\\frac{\\partial^2 \\ell}{\\partial \\theta \\partial \\gamma^\\top}\\right) \\hat V_{\\theta} \\left(\\frac{\\partial^2 \\ell}{\\partial \\theta \\partial \\gamma^\\top}\\right)^\\top (\\boldsymbol{H}_\\gamma)^{-1},\\] \\(V_{\\gamma}\\) naive covariance matrix structural parameter estimates \\(\\hat{\\boldsymbol{\\gamma}}\\) assuming measurement error variance parameter, \\(\\boldsymbol{\\theta}\\), known, \\(\\boldsymbol{H}_\\gamma\\) Hessian matrix log-likelihood \\(\\ell\\) respect \\(\\hat{\\boldsymbol{\\gamma}}\\), \\(V_{\\theta}\\) can obtained first-stage measurement model analysis. example based Political Democracy data used https://lavaan.ugent./tutorial/sem.html example Bollen (1989)’s book.","code":"library(lavaan) library(R2spa) library(numDeriv) library(boot)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"separate-measurement-models","dir":"Articles","previous_headings":"","what":"Separate Measurement Models","title":"Corrected Standard Errors","text":"Standardized solution Compare joint structural measurement model Bootstrap Standard Errors standard errors seem diverge slightly among methods. SE(dem60~ind60) bootstrap lowest. SE(ind60~~ind60) joint model particularly higher two. SE(dem60~~dem60) lowest corrected 2S-PA. pointed joint model 2S-PA different estimators.","code":"model <- '   # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4    # regressions     dem60 ~ ind60 ' cfa_ind60 <- cfa(\"ind60 =~ x1 + x2 + x3\", data = PoliticalDemocracy) # Regression factor scores fs1 <- get_fs_lavaan(cfa_ind60, vfsLT = TRUE) # Delta method variance of (loading, intercept) vldev1 <- attr(fs1, which = \"vfsLT\") cfa_dem60 <- cfa(\"dem60 =~ y1 + y2 + y3 + y4\",                  data = PoliticalDemocracy) # Regression factor scores fs2 <- get_fs_lavaan(cfa_dem60, vfsLT = TRUE) # Delta method variance of (loading, intercept) vldev2 <- attr(fs2, which = \"vfsLT\") fs_dat <- data.frame(   fs_ind60 = fs1$fs_ind60,   fs_dem60 = fs2$fs_dem60 ) # Combine sampling variance of loading and error variance # Note: loadings first, then error variance vldev <- block_diag(vldev1, vldev2)[c(1, 3, 2, 4), c(1, 3, 2, 4)] # 2S-PA # Assemble loadings ld <- block_diag(attr(fs1, which = \"fsL\"),                  attr(fs2, which = \"fsL\")) ev <- block_diag(attr(fs1, which = \"fsT\"),                  attr(fs2, which = \"fsT\")) tspa_fit <- tspa(model = \"dem60 ~ ind60\",                  data = fs_dat,                  fsL = ld,                  fsT = ev) # Unadjusted covariance vcov(tspa_fit) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.131               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.006  0.000  0.477 # Adjusted covariance matrix (vc_cor <- vcov_corrected(tspa_fit, vfsLT = vldev, which_free = c(1, 4, 5, 7))) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.133               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60  0.001  0.000  0.518 # Corrected standard errors sqrt(diag(vc_cor)) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.36415174   0.07585532   0.71996886 tspa_est_std <- function(ld_ev) {   ld1 <- ld   ev1 <- ev   ld1[c(1, 4)] <- ld_ev[1:2]   ev1[c(1, 4)] <- ld_ev[3:4]   tfit <- tspa(model = \"dem60 ~ ind60\",                data = fs_dat,                fsL = ld1,                fsT = ev1)   standardizedSolution(tfit)$est.std[8] } tspa_est_std(c(ld[c(1, 4)], ev[c(1, 4)])) #> [1] 0.4531693 # Jacobian jac_std <- numDeriv::jacobian(tspa_est_std,                               x = c(ld[c(1, 4)], ev[c(1, 4)])) # Corrected standard error sqrt(   standardizedSolution(tspa_fit)[8, \"se\"]^2 +     jac_std %*% vldev %*% t(jac_std) ) #>           [,1] #> [1,] 0.1013887 sem_fit <- sem(model, data = PoliticalDemocracy) # Larger standard error (vc_j <-    vcov(sem_fit)[c(\"dem60~ind60\", \"ind60~~ind60\", \"dem60~~dem60\"),                 c(\"dem60~ind60\", \"ind60~~ind60\", \"dem60~~dem60\")]) #>               dem60~ind60  ind60~~ind60  dem60~~dem60 #> dem60~ind60   0.143355743 -0.0033833569  0.0648673299 #> ind60~~ind60 -0.003383357  0.0075268147 -0.0001098306 #> dem60~~dem60  0.064867330 -0.0001098306  0.7655699561 sqrt(diag(vc_j)) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.37862348   0.08675722   0.87496855 run_tspa <- function(df, inds) {   fs_ind60 <- get_fs(data = df[inds, ], model = \"ind60 =~ x1 + x2 + x3\",                      se = \"none\", test = \"none\")   fs_dem60 <- get_fs(data = df[inds, ], model = \"dem60 =~ y1 + y2 + y3 + y4\",                      se = \"none\", test = \"none\")   fs_dat <- cbind(fs_ind60, fs_dem60)   # Assemble loadings   ld <- block_diag(attr(fs_ind60, which = \"fsL\"),                    attr(fs_dem60, which = \"fsL\"))   ev <- block_diag(attr(fs_ind60, which = \"fsT\"),                    attr(fs_dem60, which = \"fsT\"))   tspa_fit <- tspa(model = \"dem60 ~ ind60\",                    data = fs_dat,                    fsL = ld,                    fsT = ev,                    test = \"none\")   coef(tspa_fit) } boo <- boot(PoliticalDemocracy, statistic = run_tspa, R = 1999) # Use MAD to downweigh outlying replications boo$t |>     apply(MARGIN = 2, FUN = mad) |>     setNames(c(\"dem60~ind60\", \"ind60~~ind60\", \"dem60~~dem60\")) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.32521386   0.07271009   0.87676911 run_sem <- function(df, inds) {   sem_fit <- sem(model, data = df[inds, ], se = \"none\", test = \"none\")   coef(sem_fit) } boo <- boot(PoliticalDemocracy, statistic = run_sem, R = 999)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"joint-measurement-model","dir":"Articles","previous_headings":"","what":"Joint Measurement Model","title":"Corrected Standard Errors","text":"Bootstrap Standard Errors","code":"cfa_joint <- cfa(\"ind60 =~ x1 + x2 + x3                   dem60 =~ y1 + y2 + y3 + y4\",                  data = PoliticalDemocracy) # Factor score fs_joint <- get_fs_lavaan(cfa_joint, vfsLT = TRUE) # Delta method variance vldev_joint <- attr(fs_joint, which = \"vfsLT\") tspa_fit2 <- tspa(model = \"dem60 ~ ind60\",                   data = fs_joint,                   fsT = attr(fs_joint, \"fsT\"),                   fsL = attr(fs_joint, \"fsL\")) # Unadjusted covariance vcov(tspa_fit2) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.124               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.006  0.000  0.430 # Adjusted covariance (vc2_cor <- vcov_corrected(tspa_fit2, vfsLT = vldev_joint)) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.130               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.009  0.000  0.493 # Corrected standard errors sqrt(diag(vc2_cor)) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.36049260   0.07654824   0.70229676 run_tspa2 <- function(df, inds) {   fs_joint <- get_fs(df[inds, ],                      model = \"ind60 =~ x1 + x2 + x3                               dem60 =~ y1 + y2 + y3 + y4\",                      se = \"none\", test = \"none\")   tspa_fit2 <- tspa(model = \"dem60 ~ ind60\",                     data = fs_joint,                     fsT = attr(fs_joint, \"fsT\"),                     fsL = attr(fs_joint, \"fsL\"),                     test = \"none\")   coef(tspa_fit2) } boo2 <- boot(PoliticalDemocracy, statistic = run_tspa2, R = 1999) # run_tspa2b <- function(df, inds) { #   fsb_joint <- get_fs(df[inds, ], #                       model = \"ind60 =~ x1 + x2 + x3 #                               dem60 =~ y1 + y2 + y3 + y4\", #                       se = \"none\", test = \"none\", method = \"Bartlett\") #   tspa_fit2b <- tspa(model = \"dem60 ~ ind60\", #                      data = fsb_joint, #                      fsT = attr(fsb_joint, \"fsT\"), #                      fsL = diag(2), #                      test = \"none\") #   coef(tspa_fit2b) # } # boo2b <- boot(PoliticalDemocracy, statistic = run_tspa2b, R = 4999) # run_sem2 <- function(df, inds) { #   sem_fit <- sem(model, data = df[inds, ]) #   coef(sem_fit)[c(6, 14, 15)] # } # boo2j <- boot(PoliticalDemocracy, statistic = run_sem2, R = 4999) # Use MAD to downweigh outlying replications boo2$t |>     apply(MARGIN = 2, FUN = mad) |>     setNames(c(\"dem60~ind60\", \"ind60~~ind60\", \"dem60~~dem60\")) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.34364943   0.07851415   0.88168143"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"with-bartletts-method","dir":"Articles","previous_headings":"Joint Measurement Model","what":"With Bartlett’s Method","title":"Corrected Standard Errors","text":"","code":"# Factor score fsb_joint <- get_fs(PoliticalDemocracy,                     model = \"ind60 =~ x1 + x2 + x3                              dem60 =~ y1 + y2 + y3 + y4\",                     method = \"Bartlett\",                     vfsLT = TRUE) vldevb_joint <- attr(fsb_joint, which = \"vfsLT\") tspa_fit2b <- tspa(model = \"dem60 ~ ind60\",                    data = fsb_joint,                    fsT = attr(fsb_joint, \"fsT\"),                    fsL = diag(2) |>                        `dimnames<-`(list(c(\"fs_ind60\", \"fs_dem60\"),                                          c(\"ind60\", \"dem60\")))) # Unadjusted covariance matrix vcov(tspa_fit2b) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.124               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.006  0.000  0.430 # Adjusted covariance matrix (vc2b_cor <- vcov_corrected(     tspa_fit2b,     # Exclude fixed loadings and error variance     vfsLT = vldevb_joint[c(5, 7), c(5, 7)],     # Specify which elements are free (error variances only)     which_free = c(5, 7))) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.124               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.006  0.000  0.448 # Corrected standard errors sqrt(diag(vc2b_cor)) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.35271752   0.07634634   0.66959806"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"multiple-groups","dir":"Articles","previous_headings":"","what":"Multiple Groups","title":"Corrected Standard Errors","text":"","code":"# Multigroup, three-factor example mod <- \"   # latent variables     visual =~ x1 + x2 + x3     textual =~ x4 + x5 + x6     speed =~ x7 + x8 + x9 \" # Factor scores based on partial invariance fs_dat <- get_fs(HolzingerSwineford1939, model = mod, std.lv = TRUE,                  group = \"school\",                  group.equal = c(\"loadings\", \"intercepts\"),                  group.partial = c(\"visual=~x2\", \"x7~1\")) fit <- cfa(mod,            data = HolzingerSwineford1939,            std.lv = TRUE,            group = \"school\",            group.equal = c(\"loadings\", \"intercepts\"),            group.partial = c(\"visual=~x2\", \"x7~1\")) fs_dat <- get_fs_lavaan(fit) vldev <- R2spa:::vcov_ld_evfs(fit) tspa_fit <- tspa(model = \"visual ~~ textual + speed                           textual ~~ speed\",                  data = fs_dat,                  group = \"school\",                  fsL = attr(fs_dat, which = \"fsL\"),                  fsT = attr(fs_dat, which = \"fsT\")) # Unadjusted covariance vcov(tspa_fit)[c(1:6, 10:15), c(1:6, 10:15)] #>                     visual~~textual visual~~speed textual~~speed visual~~visual #> visual~~textual         0.012032756   0.004143016    0.003562702    0.009059744 #> visual~~speed           0.004143016   0.015289623    0.005749677    0.006286186 #> textual~~speed          0.003562702   0.005749677    0.012308431    0.002169626 #> visual~~visual          0.009059744   0.006286186    0.002169626    0.026249333 #> textual~~textual        0.007226793   0.002111028    0.004878946    0.003126897 #> speed~~speed            0.001464756   0.006962597    0.006774549    0.001505415 #> visual~~textual.g2      0.000000000   0.000000000    0.000000000    0.000000000 #> visual~~speed.g2        0.000000000   0.000000000    0.000000000    0.000000000 #> textual~~speed.g2       0.000000000   0.000000000    0.000000000    0.000000000 #> visual~~visual.g2       0.000000000   0.000000000    0.000000000    0.000000000 #> textual~~textual.g2     0.000000000   0.000000000    0.000000000    0.000000000 #> speed~~speed.g2         0.000000000   0.000000000    0.000000000    0.000000000 #>                     textual~~textual speed~~speed visual~~textual.g2 #> visual~~textual          0.007226793  0.001464756        0.000000000 #> visual~~speed            0.002111028  0.006962597        0.000000000 #> textual~~speed           0.004878946  0.006774549        0.000000000 #> visual~~visual           0.003126897  0.001505415        0.000000000 #> textual~~textual         0.016702352  0.001425195        0.000000000 #> speed~~speed             0.001425195  0.032202255        0.000000000 #> visual~~textual.g2       0.000000000  0.000000000        0.011424159 #> visual~~speed.g2         0.000000000  0.000000000        0.005834597 #> textual~~speed.g2        0.000000000  0.000000000        0.006283615 #> visual~~visual.g2        0.000000000  0.000000000        0.008537899 #> textual~~textual.g2      0.000000000  0.000000000        0.007689097 #> speed~~speed.g2          0.000000000  0.000000000        0.003681750 #>                     visual~~speed.g2 textual~~speed.g2 visual~~visual.g2 #> visual~~textual          0.000000000       0.000000000       0.000000000 #> visual~~speed            0.000000000       0.000000000       0.000000000 #> textual~~speed           0.000000000       0.000000000       0.000000000 #> visual~~visual           0.000000000       0.000000000       0.000000000 #> textual~~textual         0.000000000       0.000000000       0.000000000 #> speed~~speed             0.000000000       0.000000000       0.000000000 #> visual~~textual.g2       0.005834597       0.006283615       0.008537899 #> visual~~speed.g2         0.020016361       0.008699815       0.010689107 #> textual~~speed.g2        0.008699815       0.016930572       0.004219633 #> visual~~visual.g2        0.010689107       0.004219633       0.021628068 #> textual~~textual.g2      0.002940790       0.006708957       0.003370422 #> speed~~speed.g2          0.017174234       0.011969242       0.005282811 #>                     textual~~textual.g2 speed~~speed.g2 #> visual~~textual             0.000000000     0.000000000 #> visual~~speed               0.000000000     0.000000000 #> textual~~speed              0.000000000     0.000000000 #> visual~~visual              0.000000000     0.000000000 #> textual~~textual            0.000000000     0.000000000 #> speed~~speed                0.000000000     0.000000000 #> visual~~textual.g2          0.007689097     0.003681750 #> visual~~speed.g2            0.002940790     0.017174234 #> textual~~speed.g2           0.006708957     0.011969242 #> visual~~visual.g2           0.003370422     0.005282811 #> textual~~textual.g2         0.017541486     0.002565923 #> speed~~speed.g2             0.002565923     0.055832832 # Adjusted covariance vcov_corrected(tspa_fit, vfsLT = vldev)[c(1:6, 10:15), c(1:6, 10:15)] #>                     visual~~textual visual~~speed textual~~speed visual~~visual #> visual~~textual        1.766770e-02  3.948995e-03   1.680095e-03   0.0032658891 #> visual~~speed          3.948995e-03  2.625930e-02   6.180231e-03  -0.0002941259 #> textual~~speed         1.680095e-03  6.180231e-03   2.088330e-02   0.0034032016 #> visual~~visual         3.265889e-03 -2.941259e-04   3.403202e-03   0.0552337485 #> textual~~textual       6.343722e-03  3.471197e-03   4.310592e-03   0.0022325897 #> speed~~speed           2.319057e-03 -2.462920e-04   2.977648e-03   0.0039800365 #> visual~~textual.g2     2.894521e-04 -1.727969e-05  -1.158344e-04  -0.0036750998 #> visual~~speed.g2      -7.758231e-05  4.463544e-04  -3.159286e-04  -0.0017771299 #> textual~~speed.g2     -1.395251e-04 -2.939691e-05   1.074132e-04   0.0021179697 #> visual~~visual.g2      6.773687e-04  9.416669e-05  -5.286419e-04  -0.0139631267 #> textual~~textual.g2   -8.773851e-05  1.870183e-04  -5.236929e-05   0.0001038840 #> speed~~speed.g2       -3.614222e-04  2.154682e-04   1.739917e-04   0.0030238026 #>                     textual~~textual  speed~~speed visual~~textual.g2 #> visual~~textual         6.343722e-03  2.319057e-03       2.894521e-04 #> visual~~speed           3.471197e-03 -2.462920e-04      -1.727969e-05 #> textual~~speed          4.310592e-03  2.977648e-03      -1.158344e-04 #> visual~~visual          2.232590e-03  3.980036e-03      -3.675100e-03 #> textual~~textual        1.943788e-02 -2.260608e-05       3.559274e-04 #> speed~~speed           -2.260608e-05  5.904050e-02       1.853678e-03 #> visual~~textual.g2      3.559274e-04  1.853678e-03       2.079770e-02 #> visual~~speed.g2       -8.758319e-04  3.799116e-03       7.128677e-03 #> textual~~speed.g2      -2.295514e-05 -2.136909e-03       2.631124e-03 #> visual~~visual.g2       1.246137e-03  2.827516e-03       1.425193e-02 #> textual~~textual.g2    -2.028541e-04 -1.070262e-03       5.734502e-03 #> speed~~speed.g2         7.282334e-04 -1.578688e-02       3.266464e-04 #>                     visual~~speed.g2 textual~~speed.g2 visual~~visual.g2 #> visual~~textual        -7.758231e-05     -1.395251e-04      6.773687e-04 #> visual~~speed           4.463544e-04     -2.939691e-05      9.416669e-05 #> textual~~speed         -3.159286e-04      1.074132e-04     -5.286419e-04 #> visual~~visual         -1.777130e-03      2.117970e-03     -1.396313e-02 #> textual~~textual       -8.758319e-04     -2.295514e-05      1.246137e-03 #> speed~~speed            3.799116e-03     -2.136909e-03      2.827516e-03 #> visual~~textual.g2      7.128677e-03      2.631124e-03      1.425193e-02 #> visual~~speed.g2        3.246016e-02      6.793932e-03      8.478633e-03 #> textual~~speed.g2       6.793932e-03      2.230005e-02     -7.599378e-04 #> visual~~visual.g2       8.478633e-03     -7.599378e-04      1.079909e-01 #> textual~~textual.g2     4.641147e-03      6.987265e-03      4.758042e-04 #> speed~~speed.g2         6.636008e-03      1.596428e-02     -3.656654e-03 #>                     textual~~textual.g2 speed~~speed.g2 #> visual~~textual           -8.773851e-05   -0.0003614222 #> visual~~speed              1.870183e-04    0.0002154682 #> textual~~speed            -5.236929e-05    0.0001739917 #> visual~~visual             1.038840e-04    0.0030238026 #> textual~~textual          -2.028541e-04    0.0007282334 #> speed~~speed              -1.070262e-03   -0.0157868803 #> visual~~textual.g2         5.734502e-03    0.0003266464 #> visual~~speed.g2           4.641147e-03    0.0066360081 #> textual~~speed.g2          6.987265e-03    0.0159642788 #> visual~~visual.g2          4.758042e-04   -0.0036566537 #> textual~~textual.g2        2.062776e-02    0.0018416930 #> speed~~speed.g2            1.841693e-03    0.1076812729"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/correction-error.html","id":"unidimensional-case","dir":"Articles","previous_headings":"","what":"Unidimensional Case","title":"Correction to Measurement Error","text":"Consider conditional variance factor scores, \\(Var(\\tilde \\eta \\mid \\eta)\\), \\(\\tilde \\eta = \\boldsymbol{}^\\top \\boldsymbol{y}\\). factor model, \\[Var(\\tilde \\eta \\mid \\eta) = Var(\\boldsymbol{}^\\top \\boldsymbol{\\lambda} \\eta + \\boldsymbol{}^\\top \\boldsymbol{\\varepsilon} \\mid \\eta) = \\eta^2 Var(\\boldsymbol{}^\\top \\boldsymbol{\\lambda}) + Var(\\boldsymbol{}^\\top \\boldsymbol{\\varepsilon}).\\] second term error component. small simulation: standard errors larger using sample estimates weights. \\(\\boldsymbol{}\\) known, error variance simply \\(\\boldsymbol{}^\\top \\boldsymbol{\\Theta} \\boldsymbol{}\\). However, \\(\\boldsymbol{}\\) unknown estimated \\(\\tilde{\\boldsymbol{}}\\) data, \\[Var(\\tilde{\\boldsymbol{}}^\\top \\boldsymbol{\\varepsilon}) = E[Var(\\tilde{\\boldsymbol{}}^\\top \\boldsymbol{\\varepsilon} \\mid \\tilde{\\boldsymbol{}})] + Var[E(\\tilde{\\boldsymbol{}}^\\top \\boldsymbol{\\varepsilon} \\mid \\tilde{\\boldsymbol{}})].\\] assumption \\(E(\\boldsymbol{\\varepsilon})\\) = \\(\\boldsymbol{0}\\), second term 0. first term expectation quadratic form, \\[E(\\tilde{\\boldsymbol{}}^\\top \\boldsymbol{\\Theta} \\tilde{\\boldsymbol{}}) = E(\\tilde{\\boldsymbol{}}^\\top) \\boldsymbol{\\Theta} E(\\tilde{\\boldsymbol{}}) + \\mathrm{tr}(\\boldsymbol{\\Theta} \\boldsymbol{V}_{\\tilde{\\boldsymbol{}}})\\] \\(\\boldsymbol{V}_{\\tilde{\\boldsymbol{}}}\\) covariance matrix \\(\\tilde{\\boldsymbol{}}\\). can see using simulated data","code":"#' Load package and set seed library(lavaan) #> This is lavaan 0.6-17 #> lavaan is FREE software! Please report any bugs. library(R2spa) set.seed(191254)  #' Fixed conditions num_obs <- 100 lambda <- c(.3, .5, .7, .9) theta <- c(.5, .4, .5, .7)  #' Simulate true score, and standardized eta <- rnorm(num_obs) eta <- (eta - mean(eta)) eta <- eta / sqrt(mean(eta^2))  #' Function for simulating y simy <- function(eta, lambda) {     t(tcrossprod(lambda, eta) +           rnorm(length(lambda) * length(eta), sd = sqrt(theta))) } #' Simulation nsim <- 2500 tfsy_sim <- fsy_sim <- matrix(NA, nrow = num_obs, ncol = nsim) # Also save the scoring matrix a_sim <- matrix(NA, nrow = length(lambda), ncol = nsim) for (i in seq_len(nsim)) {     y <- simy(eta = eta, lambda = lambda)     tfsy_sim[, i] <- R2spa::compute_fscore(       y, lambda = lambda, theta = diag(theta), psi = matrix(1)     )     fsy <- R2spa::get_fs(y, std.lv = TRUE)     fsy_sim[, i] <- fsy[, 1]     a_sim[, i] <- attr(fsy, which = \"scoring_matrix\") } #' Average conditional variance # a known apply(tfsy_sim, 1, var) |> mean() #> [1] 0.1871828 # compare to theoretical value true_a <- R2spa:::compute_a_reg(lambda, psi = matrix(1), theta = diag(theta)) true_a %*% diag(theta) %*% t(true_a) #>           [,1] #> [1,] 0.1893211 # a estimated apply(fsy_sim, 1, var) |> mean()  #> [1] 0.2078126 correct_fac <- sum(diag(diag(theta) %*% cov(t(a_sim)))) true_a %*% diag(theta) %*% t(true_a) + correct_fac #>           [,1] #> [1,] 0.2100938"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/correction-error.html","id":"correction-factor","dir":"Articles","previous_headings":"Unidimensional Case","what":"Correction Factor","title":"Correction to Measurement Error","text":"can approximate \\(\\boldsymbol{V}_{\\tilde{\\boldsymbol{}}}\\) using delta method, terms using sample estimates \\[\\tilde{\\boldsymbol{}}^\\top \\hat{\\boldsymbol{\\Theta}} \\tilde{\\boldsymbol{}} + \\mathrm{tr}(\\hat{\\boldsymbol{\\Theta}} \\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{}}}).\\] delta method estimate, \\(\\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{}}}\\), \\[\\boldsymbol{J}_{\\tilde{\\boldsymbol{}}}(\\boldsymbol{\\theta})^\\top \\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{\\theta}}} \\boldsymbol{J}_{\\tilde{\\boldsymbol{}}}(\\boldsymbol{\\theta}),\\] \\(\\boldsymbol{J}_{\\tilde{\\boldsymbol{}}}(\\boldsymbol{\\theta})\\) Jacobian matrix, \\(\\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{\\theta}}}\\) sampling variance sample estimator \\(\\boldsymbol{\\theta}\\). Jacobian can approximated using finite difference lavaan::lav_func_jacobian_complex().","code":"y <- simy(eta = eta, lambda = lambda) cfa_fit <- cfa(\"f =~ y1 + y2 + y3 + y4\",                data = setNames(data.frame(y), paste0(\"y\", 1:4)),                std.lv = TRUE) # Jacobian R2spa:::compute_a(coef(cfa_fit), lavobj = cfa_fit) #> [[1]] #>         [,1]      [,2]     [,3]      [,4] #> f 0.09890619 0.3246767 0.450034 0.3134454 jac_a <- lavaan::lav_func_jacobian_complex(     \\(x) R2spa:::compute_a(x, lavobj = cfa_fit)[[1]],     coef(cfa_fit) ) sum(diag(lavInspect(cfa_fit, what = \"est\")$theta %*%              jac_a %*% vcov(cfa_fit) %*% t(jac_a))) #> [1] 0.02152978 fsy <- R2spa::get_fs(y, std.lv = TRUE) attr(fsy, which = \"fsT\") #>           fs_f1 #> fs_f1 0.2014365 # Using `get_fs(..., corrected_fsT = TRUE) fsy2 <- R2spa::get_fs(y, std.lv = TRUE, corrected_fsT = TRUE) attr(fsy2, which = \"fsT\") #>           fs_f1 #> fs_f1 0.2229663"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/correction-error.html","id":"multidimensional-case","dir":"Articles","previous_headings":"","what":"Multidimensional Case","title":"Correction to Measurement Error","text":"\\(\\), \\(j\\) element correction matrix can approximated \\(\\mathrm{tr}(\\hat{\\boldsymbol{\\Theta}} \\hat{\\boldsymbol{V}}_{{\\tilde{\\boldsymbol{}}_i}{\\tilde{\\boldsymbol{}}_j}})\\), \\(\\hat{\\boldsymbol{V}}_{{\\tilde{\\boldsymbol{}}_i}{\\tilde{\\boldsymbol{}}_j}})\\) obtained \\[\\boldsymbol{J}_{\\tilde{\\boldsymbol{}}_i}(\\boldsymbol{\\theta})^\\top \\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{\\theta}}} \\boldsymbol{J}_{\\tilde{\\boldsymbol{}}_j}(\\boldsymbol{\\theta}),\\]","code":"#' Set seed set.seed(251329)  #' Fixed conditions num_obs <- 100 lambda <- Matrix::bdiag(list(c(.3, .5, .7, .9), c(.7, .6, .7))) |>     as.matrix() theta <- c(.5, .4, .5, .7, .7, .8, .5) psi <- matrix(c(1, -.4, -.4, 1), nrow = 2)  #' Simulate true score (exact means and covariances) eta <- MASS::mvrnorm(num_obs, mu = rep(0, 2), Sigma = psi, empirical = TRUE)  #' Function for simulating y simy <- function(eta, lambda) {     tcrossprod(eta, lambda) +         MASS::mvrnorm(num_obs, mu = rep(0, length(theta)), Sigma = diag(theta)) } #' Simulation nsim <- 2500 tfsy_sim <- fsy_sim <- array(NA, dim = c(num_obs, ncol(lambda), nsim)) # Also save the scoring matrix a_sim <- array(NA, dim = c(ncol(lambda), nrow(lambda), nsim)) for (i in seq_len(nsim)) {     y <- simy(eta = eta, lambda = lambda)     tfsy_sim[, , i] <- R2spa::compute_fscore(       y, lambda = lambda, theta = diag(theta), psi = psi     )     fsy <- R2spa::get_fs(         data.frame(y) |> setNames(paste0(\"y\", 1:7)),         model = \"f1 =~ y1 + y2 + y3 + y4\\nf2 =~ y5 + y6 + y7\",         std.lv = TRUE)     fsy_sim[, , i] <- as.matrix(fsy[, 1:2])     a_sim[, , i] <- attr(fsy, which = \"scoring_matrix\") } #' Average conditional variance # a known apply(tfsy_sim, MARGIN = 1, FUN = \\(x) cov(t(x))) |>      rowMeans() |> matrix(ncol = 2, nrow = 2) #>            [,1]       [,2] #> [1,]  0.1789934 -0.0483855 #> [2,] -0.0483855  0.2022312 # compare to theoretical value true_a <- R2spa:::compute_a_reg(lambda, psi = psi, theta = diag(theta)) true_a %*% diag(theta) %*% t(true_a) #>             [,1]        [,2] #> [1,]  0.18076106 -0.04855749 #> [2,] -0.04855749  0.20339709 # a estimated apply(fsy_sim, MARGIN = 1, FUN = \\(x) cov(t(x))) |>      rowMeans() |> matrix(ncol = 2, nrow = 2) #>             [,1]        [,2] #> [1,]  0.20175239 -0.05254471 #> [2,] -0.05254471  0.23427145 correct_fac11 <- sum(diag(diag(theta) %*% cov(t(a_sim[1, , ])))) correct_fac22 <- sum(diag(diag(theta) %*% cov(t(a_sim[2, , ])))) correct_fac21 <- sum(diag(diag(theta) %*%                               cov(t(a_sim[2, , ]), t(a_sim[1, , ])))) correct_fac <- matrix(c(correct_fac11, correct_fac21,                         correct_fac21, correct_fac22), nrow = 2) true_a %*% diag(theta) %*% t(true_a) + correct_fac #>             [,1]        [,2] #> [1,]  0.20114155 -0.05375924 #> [2,] -0.05375924  0.23429418 y <- simy(eta = eta, lambda = lambda) cfa_fit <- cfa(\"f1 =~ y1 + y2 + y3 + y4\\nf2 =~ y5 + y6 + y7\",                data = setNames(data.frame(y), paste0(\"y\", 1:7)),                std.lv = TRUE) # Jacobian R2spa:::compute_a(coef(cfa_fit), lavobj = cfa_fit) #> [[1]] #>           [,1]        [,2]        [,3]        [,4]        [,5]        [,6] #> f1  0.06546889  0.22696068  0.38977223  0.33101521 -0.05012587 -0.02226577 #> f2 -0.01227313 -0.04254719 -0.07306865 -0.06205377  0.37397828  0.16612010 #>           [,7] #> f1 -0.05314801 #> f2  0.39652577 jac_a1 <- lavaan::lav_func_jacobian_complex(     \\(x) R2spa:::compute_a(x, lavobj = cfa_fit)[[1]][1, ],     coef(cfa_fit) ) jac_a2 <- lavaan::lav_func_jacobian_complex(     \\(x) R2spa:::compute_a(x, lavobj = cfa_fit)[[1]][2, ],     coef(cfa_fit) ) # Correction[1, 1] sum(diag(lavInspect(cfa_fit, what = \"est\")$theta %*%              jac_a1 %*% vcov(cfa_fit) %*% t(jac_a1))) #> [1] 0.01551285 # Correction[2, 2] sum(diag(lavInspect(cfa_fit, what = \"est\")$theta %*%              jac_a2 %*% vcov(cfa_fit) %*% t(jac_a2))) #> [1] 0.02207038 # Correction[2, 1] sum(diag(lavInspect(cfa_fit, what = \"est\")$theta %*%              jac_a2 %*% vcov(cfa_fit) %*% t(jac_a1))) #> [1] -0.004873356 fsy <- R2spa::get_fs(     data.frame(y) |> setNames(paste0(\"y\", 1:7)),     model = \"f1 =~ y1 + y2 + y3 + y4\\nf2 =~ y5 + y6 + y7\",     std.lv = TRUE) attr(fsy, which = \"fsT\") #>             fs_f1       fs_f2 #> fs_f1  0.16691097 -0.05653148 #> fs_f2 -0.05653148  0.19891917 # Using `get_fs(..., corrected_fsT = TRUE) fsy2 <- R2spa::get_fs(     data.frame(y) |> setNames(paste0(\"y\", 1:7)),     model = \"f1 =~ y1 + y2 + y3 + y4\\nf2 =~ y5 + y6 + y7\",     std.lv = TRUE,     corrected_fsT = TRUE) attr(fsy2, which = \"fsT\") #>             fs_f1       fs_f2 #> fs_f1  0.18242382 -0.06140484 #> fs_f2 -0.06140484  0.22098955"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/efa-score.html","id":"example-big-five-inventory","dir":"Articles","previous_headings":"","what":"Example: Big Five Inventory","title":"EFA Scores","text":"EFA Factor Correlation Sum scores Hand calculate Bartlett scores using weights Bartlett scores","code":"# Covariance (with FIML) corr_bfi <- lavCor(bfi[1:25], missing = \"fiml\") # EFA (Target rotation) target_mat_bfi <- matrix(0, nrow = 25, ncol = 5) target_mat_bfi[1:5, 1] <- NA target_mat_bfi[6:10, 2] <- NA target_mat_bfi[11:15, 3] <- NA target_mat_bfi[16:20, 4] <- NA target_mat_bfi[21:25, 5] <- NA fa_target_bfi <- psych::fa(     corr_bfi, n.obs = 2436,     nfactors = 5,     rotate = \"targetQ\", Target = target_mat_bfi,     scores = \"Bartlett\") #> Loading required namespace: GPArotation # Factor correlations fa_target_bfi$Phi |>     (`[`)(paste0(\"MR\", 1:5), paste0(\"MR\", 1:5)) |>     knitr::kable(digits = 2, caption = \"EFA Factor Correlation\") # Correlation with sum scores bfi |>     transform(A = (7 - A1) + A2 + A3 + A4 + A5,            C = C1 + C2 + C3 + (7 - C4) + (7 - C5),            E = (7 - E1) + (7 - E2) + E3 + E4 + E5,            N = N1 + N2 + N3 + N4 + N5,            O = O1 + (7 - O2) + O3 + O4 + (7 - O5)) |>     subset(select = A:O) |>     cor(use = \"complete\") |>     knitr::kable(digits = 2, caption = \"Sum scores\") # Bartlett score for first person bscores <-     psych::factor.scores(bfi[, 1:25], f = fa_target_bfi,                          method = \"Bartlett\") fa_target_bfi$weights #>             MR4          MR3          MR2          MR1           MR5 #> A1  0.019379345  0.088570322  0.037984352 -0.192609241 -0.0446095141 #> A2  0.039471717 -0.058827607  0.016480405  0.387253874  0.0174635927 #> A3  0.038138482 -0.002491405 -0.018448555  0.443385489 -0.0008791950 #> A4  0.008949806 -0.011419400  0.074609038  0.196183077 -0.1058123616 #> A5 -0.007115326  0.064563350 -0.028894457  0.297710866 -0.0033648030 #> C1  0.025793549 -0.013945609  0.252251588 -0.025663593  0.0878936560 #> C2  0.067216606 -0.054276500  0.383692956  0.040736916  0.0208035616 #> C3  0.023175743 -0.039728147  0.266210191  0.042308787 -0.0532943534 #> C4  0.045093233  0.017707633 -0.349869186  0.042230966 -0.0202260355 #> C5  0.055608096 -0.047779140 -0.303202360  0.040826161  0.1037402597 #> E1  0.002492943 -0.234998207  0.069439907  0.033510757 -0.0074443287 #> E2  0.069816249 -0.398147198  0.017193600  0.096706957  0.0534356364 #> E3  0.025889788  0.192977816 -0.031642142  0.069479646  0.1645920884 #> E4 -0.004903761  0.315619052 -0.010031608  0.099826497 -0.1699081214 #> E5  0.034244648  0.197071361  0.128843720 -0.049763160  0.0903700702 #> N1  0.376489849  0.174004405  0.036365710 -0.153681565 -0.0900410369 #> N2  0.315648329  0.098517700  0.033517347 -0.103972896  0.0008553925 #> N3  0.274596160 -0.019282873 -0.009840675  0.065937424  0.0331731810 #> N4  0.182196509 -0.186091836 -0.069870228  0.123634306  0.1272479199 #> N5  0.144850775 -0.078929736  0.008603568  0.133251027 -0.0834902117 #> O1  0.007965717  0.045782965  0.014054766 -0.025087007  0.3135869739 #> O2  0.045459925  0.014231220 -0.021761255  0.082163004 -0.2754285178 #> O3  0.016818868  0.080919045 -0.024446602 -0.001049249  0.4783313815 #> O4  0.052204814 -0.121040967 -0.019643988  0.103037548  0.2512799168 #> O5  0.024318925  0.034957969  0.005514044  0.026713299 -0.3498465254 # Calculation by hand y1 <- scale(bfi[, 1:25])[1, ]  # z-score crossprod(fa_target_bfi$weights, as.matrix(y1)) #>             [,1] #> MR4 -0.379596163 #> MR3  0.008672754 #> MR2 -1.662210036 #> MR1 -1.056626928 #> MR5 -2.263921509 # Compare to results from psych::fa() bscores$scores[1, ] #>          MR4          MR3          MR2          MR1          MR5  #> -0.379596163  0.008672754 -1.662210036 -1.056626928 -2.263921509 # Covariance of Bartlett scores cov(bscores$scores, use = \"complete\") |>     (`[`)(paste0(\"MR\", 1:5), paste0(\"MR\", 1:5)) |>     knitr::kable(digits = 2, caption = \"With Bartlett scores\")"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/efa-score.html","id":"using-compute_fscore-and-perform-a-two-stage-analysis","dir":"Articles","previous_headings":"Example: Big Five Inventory","what":"Using compute_fscore() and perform a two-stage analysis","title":"EFA Scores","text":"Use R2spa::compute_fscore() Recover factor covariances 2S-PA Bartlett scores 2S-PA","code":"# Obtain error covariances yc <- scale(bfi[, 1:25]) yc <- yc[complete.cases(yc), ] lam <- fa_target_bfi$loadings colnames(lam) <- c(\"N\", \"E\", \"C\", \"A\", \"O\") phi <- fa_target_bfi$Phi th <- diag(fa_target_bfi$uniquenesses) # # scoring weights # a <- solve(crossprod(lam, solve(th, lam)), t(solve(th, lam))) # ecov_fs <- a %*% th %*% t(a) # dimnames(ecov_fs) <- rep(list(c(\"N\", \"E\", \"C\", \"A\", \"O\")), 2) # Two-stage analysis library(R2spa) bfi_fs <- compute_fscore(yc, lambda = lam, theta = th,                          method = \"Bartlett\", center_y = FALSE,                          fs_matrices = TRUE) head(bfi_fs) #>                 N            E           C           A          O #> 61617 -0.37959616  0.008672754 -1.66221004 -1.05662693 -2.2639215 #> 61618  0.05502992  0.657251998 -0.81281047 -0.21048629 -0.3557784 #> 61620  0.66912918  0.351719435 -0.00993787 -0.96855007  0.3107586 #> 61621 -0.14122632 -0.048191450 -1.35947549  0.01849824 -1.5743464 #> 61622 -0.36044079  0.557318357 -0.07396380 -1.02840458 -1.0629543 #> 61623  0.18540827  1.461174698  1.82621364  0.11707095  0.5289163 # Scoring matrix attr(bfi_fs, which = \"scoring_matrix\") #>          [,1]        [,2]         [,3]         [,4]         [,5]        [,6] #> N  0.01937935  0.03947172  0.038138482  0.008949806 -0.007115326  0.02579355 #> E  0.08857032 -0.05882761 -0.002491405 -0.011419400  0.064563350 -0.01394561 #> C  0.03798435  0.01648041 -0.018448555  0.074609038 -0.028894457  0.25225159 #> A -0.19260924  0.38725387  0.443385489  0.196183077  0.297710866 -0.02566359 #> O -0.04460951  0.01746359 -0.000879195 -0.105812362 -0.003364803  0.08789366 #>          [,7]        [,8]        [,9]       [,10]        [,11]       [,12] #> N  0.06721661  0.02317574  0.04509323  0.05560810  0.002492943  0.06981625 #> E -0.05427650 -0.03972815  0.01770763 -0.04777914 -0.234998207 -0.39814720 #> C  0.38369296  0.26621019 -0.34986919 -0.30320236  0.069439907  0.01719360 #> A  0.04073692  0.04230879  0.04223097  0.04082616  0.033510757  0.09670696 #> O  0.02080356 -0.05329435 -0.02022604  0.10374026 -0.007444329  0.05343564 #>         [,13]        [,14]       [,15]       [,16]         [,17]        [,18] #> N  0.02588979 -0.004903761  0.03424465  0.37648985  0.3156483287  0.274596160 #> E  0.19297782  0.315619052  0.19707136  0.17400440  0.0985176996 -0.019282873 #> C -0.03164214 -0.010031608  0.12884372  0.03636571  0.0335173471 -0.009840675 #> A  0.06947965  0.099826497 -0.04976316 -0.15368157 -0.1039728964  0.065937424 #> O  0.16459209 -0.169908121  0.09037007 -0.09004104  0.0008553925  0.033173181 #>         [,19]        [,20]        [,21]       [,22]        [,23]       [,24] #> N  0.18219651  0.144850775  0.007965717  0.04545993  0.016818868  0.05220481 #> E -0.18609184 -0.078929736  0.045782965  0.01423122  0.080919045 -0.12104097 #> C -0.06987023  0.008603568  0.014054766 -0.02176125 -0.024446602 -0.01964399 #> A  0.12363431  0.133251027 -0.025087007  0.08216300 -0.001049249  0.10303755 #> O  0.12724792 -0.083490212  0.313586974 -0.27542852  0.478331381  0.25127992 #>          [,25] #> N  0.024318925 #> E  0.034957969 #> C  0.005514044 #> A  0.026713299 #> O -0.349846525 # Error covariance attr(bfi_fs, which = \"fsT\") #>              fs_N         fs_E         fs_C        fs_A         fs_O #> fs_N  0.169330114 -0.005940885  0.008966234  0.02689064  0.006632488 #> fs_E -0.005940885  0.267237548 -0.008256099 -0.06840869 -0.028478148 #> fs_C  0.008966234 -0.008256099  0.316471763 -0.01719329 -0.016556329 #> fs_A  0.026890643 -0.068408687 -0.017193295  0.34849177 -0.010843643 #> fs_O  0.006632488 -0.028478148 -0.016556329 -0.01084364  0.454791519 ts_fit <- tspa(\"\",                data = data.frame(bscores$scores) |>                    setNames(c(\"fs_N\", \"fs_E\", \"fs_C\", \"fs_A\", \"fs_O\")),                fsT = attr(bfi_fs, which = \"fsT\"),                fsL = diag(5) |>                    `dimnames<-`(list(c(\"fs_N\", \"fs_E\", \"fs_C\", \"fs_A\", \"fs_O\"),                                      c(\"N\", \"E\", \"C\", \"A\", \"O\")))) lavInspect(ts_fit, what = \"cor.lv\") |>     (`[`)(c(\"A\", \"C\", \"E\", \"N\", \"O\"), c(\"A\", \"C\", \"E\", \"N\", \"O\")) |>     knitr::kable(digits = 2, caption = \"With Bartlett scores and 2S-PA\")"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"single-group","dir":"Articles","previous_headings":"","what":"Single Group","title":"Grand Standardized Coefficients","text":"single groups, standardized solution can obtained first obtaining latent variable covariance matrix: \\[   \\begin{aligned}     \\boldsymbol{\\mathbf{\\eta }}& = \\boldsymbol{\\mathbf{\\alpha }}+ \\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}} + \\boldsymbol{\\mathbf{B}} \\boldsymbol{\\mathbf{\\eta }}+ \\boldsymbol{\\mathbf{\\zeta }}\\\\     (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}}) \\boldsymbol{\\mathbf{\\eta }}& = \\boldsymbol{\\mathbf{\\alpha }}+ \\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}} + \\boldsymbol{\\mathbf{\\zeta }}\\\\     (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}}) \\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}) (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^\\top & = \\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}}) + \\boldsymbol{\\mathbf{\\Psi }}\\\\     \\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}) & = (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^{-1} [\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}}) + \\boldsymbol{\\mathbf{\\Psi}}] {(\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^\\top}^{-1}   \\end{aligned} \\] standardized estimates \\(\\boldsymbol{\\mathbf{B}}\\) matrix obtained \\[ \\boldsymbol{\\mathbf{B}}_s = \\boldsymbol{\\mathbf{S}}_\\eta^{-1} \\boldsymbol{\\mathbf{B}} \\boldsymbol{\\mathbf{S}}_\\eta^{1} \\] \\(\\boldsymbol{\\mathbf{S}}_\\eta\\) diagonal matrix containing square root diagonal elements \\(\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}})\\)","code":"myModel <- '    # latent variables      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4    # regressions      dem60 ~ ind60 ' fit <- sem(model = myModel,            data  = PoliticalDemocracy) # Latent variable covariances lavInspect(fit, \"cov.lv\") #>       ind60 dem60 #> ind60 0.449       #> dem60 0.646 4.382 # Using R2spa::veta()` fit_est <- lavInspect(fit, \"est\") R2spa:::veta(fit_est$beta, psi = fit_est$psi) #>           ind60     dem60 #> ind60 0.4485415 0.6455929 #> dem60 0.6455929 4.3824834 S_eta <- diag(   sqrt(diag(     R2spa:::veta(fit_est$beta, psi = fit_est$psi)   )) ) solve(S_eta) %*% fit_est$beta %*% S_eta #>           [,1] [,2] #> [1,] 0.0000000    0 #> [2,] 0.4604657    0 # Compare to lavaan standardizedSolution(fit)[8, ] #>     lhs op   rhs est.std  se     z pvalue ci.lower ci.upper #> 8 dem60  ~ ind60    0.46 0.1 4.593      0    0.264    0.657"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"multiple-groups","dir":"Articles","previous_headings":"","what":"Multiple Groups","title":"Grand Standardized Coefficients","text":"","code":"reg <- '   # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9    # regressions     visual ~ c(b1, b1) * speed ' reg_fit <- sem(reg, data = HolzingerSwineford1939,                group = \"school\",                group.equal = c(\"loadings\", \"intercepts\"))"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"separate-standardization-by-group","dir":"Articles","previous_headings":"Multiple Groups","what":"Separate standardization by group","title":"Grand Standardized Coefficients","text":"","code":"standardizedSolution(reg_fit, type = \"std.lv\") |>   subset(subset = label == \"b1\") #>       lhs op   rhs group label est.std    se     z pvalue ci.lower ci.upper #> 7  visual  ~ speed     1    b1   0.369 0.073 5.032      0    0.226    0.513 #> 30 visual  ~ speed     2    b1   0.496 0.086 5.768      0    0.327    0.664 # Compare to doing it by hand reg_fit_est <- lavInspect(reg_fit, what = \"est\") # Group 1: S_eta1 <- diag(   sqrt(diag(     R2spa:::veta(reg_fit_est[[1]]$beta, psi = reg_fit_est[[1]]$psi)   )) ) solve(S_eta1) %*% reg_fit_est[[1]]$beta %*% S_eta1 #>      [,1]      [,2] #> [1,]    0 0.3694845 #> [2,]    0 0.0000000 # Group 2: S_eta2 <- diag(   sqrt(diag(     R2spa:::veta(reg_fit_est[[2]]$beta, psi = reg_fit_est[[2]]$psi)   )) ) solve(S_eta2) %*% reg_fit_est[[2]]$beta %*% S_eta2 #>      [,1]      [,2] #> [1,]    0 0.4958876 #> [2,]    0 0.0000000"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"grand-standardization","dir":"Articles","previous_headings":"Multiple Groups","what":"Grand standardization","title":"Grand Standardized Coefficients","text":"group group-specific covariance matrix \\(\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}_g)\\). group-specific latent means \\[\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) = (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^{-1}[\\boldsymbol{\\mathbf{\\alpha }}+ \\Gamma \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{X}})]\\] grand mean \\(\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}) = \\sum_{g = 1}^G n_g \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) / N\\) grand covariance matrix : \\[\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}) = \\frac{1}{N} \\sum_{g = 1}^G n_g \\left\\{\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}_g) + [\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) - \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}})][\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) - \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}})]^\\top \\right\\}\\] need involve mean well function grandStandardizdSolution() automates computation grand standardized coefficients, asymptotic standard error obtained using delta method:","code":"# Latent means lavInspect(reg_fit, what = \"mean.lv\")  # lavaan #> $Pasteur #> visual  speed  #>      0      0  #>  #> $`Grant-White` #> visual  speed  #> -0.204 -0.167 R2spa:::eeta(reg_fit_est[[1]]$beta, alpha = reg_fit_est[[1]]$alpha) #>        intercept #> visual         0 #> speed          0 R2spa:::eeta(reg_fit_est[[2]]$beta, alpha = reg_fit_est[[2]]$alpha) #>         intercept #> visual -0.2036667 #> speed  -0.1666424 # Grand covariance ns <- lavInspect(reg_fit, what = \"nobs\") R2spa:::veta_grand(ns, beta_list = lapply(reg_fit_est, `[[`, \"beta\"),                    psi_list = lapply(reg_fit_est, `[[`, \"psi\"),                    alpha_list = lapply(reg_fit_est, `[[`, \"alpha\")) #>           visual     speed #> visual 0.5497595 0.2085522 #> speed  0.2085522 0.4059395 grandStandardizedSolution(reg_fit) #>       lhs op   rhs exo group block label est.std    se     z pvalue ci.lower #> 7  visual  ~ speed   0     1     1    b1   0.431 0.073 5.867      0    0.287 #> 30 visual  ~ speed   0     2     2    b1   0.431 0.073 5.867      0    0.287 #>    ci.upper #> 7     0.575 #> 30    0.575"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/multiple-factors.html","id":"factor-score","dir":"Articles","previous_headings":"","what":"Factor score","title":"Multi-Factor Measurement Model","text":"CFA model multiple latent factors, even indicator loads one factor, resulting factor scores generally weighted composites indicators. Consider regression score, form \\[\\tilde{\\boldsymbol \\eta} = \\mathbf{}(\\mathbf{y} - \\hat{\\boldsymbol \\mu}) + \\boldsymbol{\\alpha}\\] \\(\\mathbf{} = \\boldsymbol{\\Psi}\\boldsymbol{\\Lambda}^\\top \\hat{\\boldsymbol{\\Sigma}}^{-1}\\) \\(q\\) \\(\\times\\) \\(p\\) matrix, \\(\\hat{\\boldsymbol \\mu} = \\boldsymbol{\\nu} + \\boldsymbol{\\Lambda} \\boldsymbol{\\alpha}\\) \\(\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Psi}\\boldsymbol{\\Lambda}^\\top + \\boldsymbol{\\Theta}\\) model-implied means covariances indicators \\(\\mathbf{y}\\), \\(\\boldsymbol{\\alpha}\\) \\(\\boldsymbol{\\Psi}\\) latent means latent covariances. Therefore, assuming model correctly specified \\(\\mathbf{y} = \\boldsymbol{\\nu} + \\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\boldsymbol{\\varepsilon}\\), \\[   \\begin{aligned}   \\tilde{\\boldsymbol \\eta} & = \\mathbf{}(\\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\boldsymbol{\\varepsilon} - \\boldsymbol{\\Lambda} \\boldsymbol{\\alpha}) + \\boldsymbol{\\alpha} \\\\   & = (\\mathbf{} - \\mathbf{}\\boldsymbol{\\Lambda}) \\boldsymbol{\\alpha} +   \\mathbf{}\\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\mathbf{}\\boldsymbol{\\varepsilon}.   \\end{aligned} \\] consider \\(\\tilde{\\boldsymbol \\eta}\\) indicators \\(\\boldsymbol \\eta\\), can see \\(\\boldsymbol{\\nu}_\\tilde{\\boldsymbol{\\eta}} = (\\mathbf{} - \\mathbf{}\\boldsymbol{\\Lambda}) \\boldsymbol{\\alpha}\\) intercept, \\(\\boldsymbol{\\Lambda}_\\tilde{\\boldsymbol{\\eta}} = \\mathbf{}\\boldsymbol{\\Lambda}\\) \\(q\\) \\(\\times\\) \\(q\\) loading matrix, \\(\\boldsymbol{\\Theta}_\\tilde{\\boldsymbol{\\eta}} = \\mathbf{}\\boldsymbol{\\varepsilon}\\) error covariance matrix. can see \\(\\boldsymbol{\\Lambda}_\\tilde{\\boldsymbol{\\eta}}\\) generally diagonal, following shows: can also use R2spa::get_fs(): Therefore, need specify cross-loadings using 2S-PA. consistent SEM results.","code":"# CFA my_cfa <- \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4 \" cfa_fit <- cfa(model = my_cfa,                data  = PoliticalDemocracy,                std.lv = TRUE) # A matrix pars <- lavInspect(cfa_fit, what = \"est\") lambda_mat <- pars$lambda psi_mat <- pars$psi sigma_mat <- cfa_fit@implied$cov[[1]] ginvsigma <- MASS::ginv(sigma_mat) alambda <- psi_mat %*% crossprod(lambda_mat, ginvsigma %*% lambda_mat) alambda #>            ind60      dem60 #> ind60 0.95538579 0.01834111 #> dem60 0.05816994 0.86888887 (fs_dat <- get_fs(PoliticalDemocracy, model = my_cfa, std.lv = TRUE)) |> head() #>     fs_ind60   fs_dem60 fs_ind60_se fs_dem60_se ind60_by_fs_ind60 #> 1 -0.8101568 -1.3119114   0.1859987   0.3012901         0.9553858 #> 2  0.1888466 -1.3644831   0.1859987   0.3012901         0.9553858 #> 3  1.0960931  1.3107705   0.1859987   0.3012901         0.9553858 #> 4  1.8702043  1.4849083   0.1859987   0.3012901         0.9553858 #> 5  1.2446060  0.9193277   0.1859987   0.3012901         0.9553858 #> 6  0.3348621  0.4886331   0.1859987   0.3012901         0.9553858 #>   ind60_by_fs_dem60 dem60_by_fs_ind60 dem60_by_fs_dem60 ev_fs_ind60 #> 1        0.05816994        0.01834111         0.8688889  0.03459552 #> 2        0.05816994        0.01834111         0.8688889  0.03459552 #> 3        0.05816994        0.01834111         0.8688889  0.03459552 #> 4        0.05816994        0.01834111         0.8688889  0.03459552 #> 5        0.05816994        0.01834111         0.8688889  0.03459552 #> 6        0.05816994        0.01834111         0.8688889  0.03459552 #>   ecov_fs_dem60_fs_ind60 ev_fs_dem60 #> 1            0.004017388  0.09077571 #> 2            0.004017388  0.09077571 #> 3            0.004017388  0.09077571 #> 4            0.004017388  0.09077571 #> 5            0.004017388  0.09077571 #> 6            0.004017388  0.09077571 tspa_fit <- tspa(model = \"dem60 ~ ind60\", data = fs_dat,                  fsT = attr(fs_dat, \"fsT\"),                   fsL = attr(fs_dat, \"fsL\")) cat(attr(tspa_fit, \"tspaModel\")) #> # latent variables (indicated by factor scores) #>  ind60 =~ c(0.955385785456617) * fs_ind60 + c(0.0581699407736295) * fs_dem60 #>  # latent variables (indicated by factor scores) #>  dem60 =~ c(0.0183411131911887) * fs_ind60 + c(0.868888868390616) * fs_dem60 #>  # constrain the errors #> fs_ind60 ~~ c(0.0345955179271981) * fs_ind60 #>  # constrain the errors #> fs_dem60 ~~ c(0.00401738814566851) * fs_ind60 #>  # constrain the errors #> fs_dem60 ~~ c(0.0907757082269278) * fs_dem60 #>   #>  # structural model #>  dem60 ~ ind60 parameterestimates(tspa_fit) #>         lhs op      rhs   est    se     z pvalue ci.lower ci.upper #> 1     ind60 =~ fs_ind60 0.955 0.000    NA     NA    0.955    0.955 #> 2     ind60 =~ fs_dem60 0.058 0.000    NA     NA    0.058    0.058 #> 3     dem60 =~ fs_ind60 0.018 0.000    NA     NA    0.018    0.018 #> 4     dem60 =~ fs_dem60 0.869 0.000    NA     NA    0.869    0.869 #> 5  fs_ind60 ~~ fs_ind60 0.035 0.000    NA     NA    0.035    0.035 #> 6  fs_ind60 ~~ fs_dem60 0.004 0.000    NA     NA    0.004    0.004 #> 7  fs_dem60 ~~ fs_dem60 0.091 0.000    NA     NA    0.091    0.091 #> 8     dem60  ~    ind60 0.460 0.113 4.089      0    0.240    0.681 #> 9     ind60 ~~    ind60 1.000 0.169 5.900      0    0.668    1.332 #> 10    dem60 ~~    dem60 0.788 0.150 5.267      0    0.495    1.081"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/multiple-factors.html","id":"three-factor-model-example","dir":"Articles","previous_headings":"","what":"Three-factor model example","title":"Multi-Factor Measurement Model","text":"can also use R2spa::get_fs(): Therefore, need specify cross-loadings using 2S-PA. Compare SEM:","code":"# CFA cfa_3fac <-  \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     dem65 =~ y5 + y6 + y7 + y8 \" cfa_3fac_fit <- cfa(model = cfa_3fac,                     data  = PoliticalDemocracy,                     std.lv = TRUE) # A matrix pars <- lavInspect(cfa_3fac_fit, what = \"est\") lambda_mat <- pars$lambda psi_mat <- pars$psi sigma_mat <- cfa_3fac_fit@implied$cov[[1]] ginvsigma <- MASS::ginv(sigma_mat) alambda <- psi_mat %*% crossprod(lambda_mat, ginvsigma %*% lambda_mat) alambda #>             ind60        dem60      dem65 #> ind60  0.95064774 -0.005967124 0.02951139 #> dem60 -0.02069724  0.533020047 0.41603787 #> dem65  0.09868528  0.401095944 0.49133377 (fs_dat_3fac <- get_fs(PoliticalDemocracy, model = cfa_3fac, std.lv = TRUE)) |>   head() #>     fs_ind60   fs_dem60    fs_dem65 fs_ind60_se fs_dem60_se fs_dem65_se #> 1 -0.7990475 -1.1571745 -1.13720655   0.1844193   0.2422541   0.2270976 #> 2  0.2152486 -1.0238236 -0.80871922   0.1844193   0.2422541   0.2270976 #> 3  1.1028297  1.3890842  1.46389950   0.1844193   0.2422541   0.2270976 #> 4  1.8585004  1.3163888  1.43045560   0.1844193   0.2422541   0.2270976 #> 5  1.2432985  0.9522026  1.03348589   0.1844193   0.2422541   0.2270976 #> 6  0.3067994  0.1109206  0.05023217   0.1844193   0.2422541   0.2270976 #>   ind60_by_fs_ind60 ind60_by_fs_dem60 ind60_by_fs_dem65 dem60_by_fs_ind60 #> 1         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 2         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 3         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 4         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 5         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 6         0.9506477       -0.02069724        0.09868528      -0.005967124 #>   dem60_by_fs_dem60 dem60_by_fs_dem65 dem65_by_fs_ind60 dem65_by_fs_dem60 #> 1           0.53302         0.4010959        0.02951139         0.4160379 #> 2           0.53302         0.4010959        0.02951139         0.4160379 #> 3           0.53302         0.4010959        0.02951139         0.4160379 #> 4           0.53302         0.4010959        0.02951139         0.4160379 #> 5           0.53302         0.4010959        0.02951139         0.4160379 #> 6           0.53302         0.4010959        0.02951139         0.4160379 #>   dem65_by_fs_dem65 ev_fs_ind60 ecov_fs_dem60_fs_ind60 ev_fs_dem60 #> 1         0.4913338   0.0340105           0.0003881641  0.05868703 #> 2         0.4913338   0.0340105           0.0003881641  0.05868703 #> 3         0.4913338   0.0340105           0.0003881641  0.05868703 #> 4         0.4913338   0.0340105           0.0003881641  0.05868703 #> 5         0.4913338   0.0340105           0.0003881641  0.05868703 #> 6         0.4913338   0.0340105           0.0003881641  0.05868703 #>   ecov_fs_dem65_fs_ind60 ecov_fs_dem65_fs_dem60 ev_fs_dem65 #> 1            0.005026024             0.05337525   0.0515733 #> 2            0.005026024             0.05337525   0.0515733 #> 3            0.005026024             0.05337525   0.0515733 #> 4            0.005026024             0.05337525   0.0515733 #> 5            0.005026024             0.05337525   0.0515733 #> 6            0.005026024             0.05337525   0.0515733 tspa_fit_3fac <- tspa(model = \"dem60 ~ ind60               dem65 ~ ind60 + dem60\",               data = fs_dat_3fac,               fsT = attr(fs_dat_3fac, \"fsT\"),               fsL = attr(fs_dat_3fac, \"fsL\")) cat(attr(tspa_fit_3fac, \"tspaModel\")) #> # latent variables (indicated by factor scores) #>  ind60 =~ c(0.950647742844847) * fs_ind60 + c(-0.0206972362902851) * fs_dem60 + c(0.0986852834195767) * fs_dem65 #>  # latent variables (indicated by factor scores) #>  dem60 =~ c(-0.00596712444175395) * fs_ind60 + c(0.533020047181513) * fs_dem60 + c(0.401095943690546) * fs_dem65 #>  # latent variables (indicated by factor scores) #>  dem65 =~ c(0.0295113941487134) * fs_ind60 + c(0.416037872255944) * fs_dem60 + c(0.491333767947424) * fs_dem65 #>  # constrain the errors #> fs_ind60 ~~ c(0.0340104951546674) * fs_ind60 #>  # constrain the errors #> fs_dem60 ~~ c(0.00038816407039879) * fs_ind60 #>  # constrain the errors #> fs_dem65 ~~ c(0.00502602420598866) * fs_ind60 #>  # constrain the errors #> fs_dem60 ~~ c(0.0586870313996517) * fs_dem60 #>  # constrain the errors #> fs_dem65 ~~ c(0.0533752457536215) * fs_dem60 #>  # constrain the errors #> fs_dem65 ~~ c(0.0515732993693881) * fs_dem65 #>   #>  # structural model #>  dem60 ~ ind60 #>               dem65 ~ ind60 + dem60 parameterestimates(tspa_fit_3fac) #>         lhs op      rhs    est    se      z pvalue ci.lower ci.upper #> 1     ind60 =~ fs_ind60  0.951 0.000     NA     NA    0.951    0.951 #> 2     ind60 =~ fs_dem60 -0.021 0.000     NA     NA   -0.021   -0.021 #> 3     ind60 =~ fs_dem65  0.099 0.000     NA     NA    0.099    0.099 #> 4     dem60 =~ fs_ind60 -0.006 0.000     NA     NA   -0.006   -0.006 #> 5     dem60 =~ fs_dem60  0.533 0.000     NA     NA    0.533    0.533 #> 6     dem60 =~ fs_dem65  0.401 0.000     NA     NA    0.401    0.401 #> 7     dem65 =~ fs_ind60  0.030 0.000     NA     NA    0.030    0.030 #> 8     dem65 =~ fs_dem60  0.416 0.000     NA     NA    0.416    0.416 #> 9     dem65 =~ fs_dem65  0.491 0.000     NA     NA    0.491    0.491 #> 10 fs_ind60 ~~ fs_ind60  0.034 0.000     NA     NA    0.034    0.034 #> 11 fs_ind60 ~~ fs_dem60  0.000 0.000     NA     NA    0.000    0.000 #> 12 fs_ind60 ~~ fs_dem65  0.005 0.000     NA     NA    0.005    0.005 #> 13 fs_dem60 ~~ fs_dem60  0.059 0.000     NA     NA    0.059    0.059 #> 14 fs_dem60 ~~ fs_dem65  0.053 0.000     NA     NA    0.053    0.053 #> 15 fs_dem65 ~~ fs_dem65  0.052 0.000     NA     NA    0.052    0.052 #> 16    dem60  ~    ind60  0.448 0.114  3.937  0.000    0.225    0.671 #> 17    dem65  ~    ind60  0.146 0.069  2.112  0.035    0.010    0.281 #> 18    dem65  ~    dem60  0.913 0.073 12.435  0.000    0.769    1.057 #> 19    ind60 ~~    ind60  1.000 0.169  5.902  0.000    0.668    1.332 #> 20    dem60 ~~    dem60  0.799 0.153  5.224  0.000    0.499    1.099 #> 21    dem65 ~~    dem65  0.026 0.043  0.620  0.535   -0.057    0.110 sem_3fac <- sem(\"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     dem65 =~ y5 + y6 + y7 + y8   # structural model     dem60 ~ ind60     dem65 ~ ind60 + dem60   \",   data = PoliticalDemocracy ) standardizedSolution(sem_3fac) #>      lhs op   rhs est.std    se      z pvalue ci.lower ci.upper #> 1  ind60 =~    x1   0.920 0.023 39.823  0.000    0.874    0.965 #> 2  ind60 =~    x2   0.973 0.017 58.858  0.000    0.941    1.006 #> 3  ind60 =~    x3   0.872 0.031 28.034  0.000    0.811    0.933 #> 4  dem60 =~    y1   0.845 0.039 21.698  0.000    0.769    0.921 #> 5  dem60 =~    y2   0.760 0.054 14.142  0.000    0.655    0.866 #> 6  dem60 =~    y3   0.705 0.063 11.225  0.000    0.582    0.828 #> 7  dem60 =~    y4   0.860 0.036 23.650  0.000    0.789    0.931 #> 8  dem65 =~    y5   0.803 0.046 17.602  0.000    0.714    0.893 #> 9  dem65 =~    y6   0.783 0.049 15.918  0.000    0.687    0.879 #> 10 dem65 =~    y7   0.819 0.043 19.122  0.000    0.735    0.903 #> 11 dem65 =~    y8   0.847 0.038 22.389  0.000    0.773    0.921 #> 12 dem60  ~ ind60   0.448 0.102  4.393  0.000    0.248    0.648 #> 13 dem65  ~ ind60   0.146 0.070  2.071  0.038    0.008    0.283 #> 14 dem65  ~ dem60   0.913 0.048 19.120  0.000    0.819    1.006 #> 15    x1 ~~    x1   0.154 0.042  3.636  0.000    0.071    0.238 #> 16    x2 ~~    x2   0.053 0.032  1.634  0.102   -0.010    0.116 #> 17    x3 ~~    x3   0.240 0.054  4.417  0.000    0.133    0.346 #> 18    y1 ~~    y1   0.286 0.066  4.348  0.000    0.157    0.415 #> 19    y2 ~~    y2   0.422 0.082  5.166  0.000    0.262    0.582 #> 20    y3 ~~    y3   0.503 0.089  5.676  0.000    0.329    0.676 #> 21    y4 ~~    y4   0.261 0.063  4.173  0.000    0.138    0.383 #> 22    y5 ~~    y5   0.355 0.073  4.842  0.000    0.211    0.499 #> 23    y6 ~~    y6   0.387 0.077  5.024  0.000    0.236    0.538 #> 24    y7 ~~    y7   0.329 0.070  4.696  0.000    0.192    0.467 #> 25    y8 ~~    y8   0.283 0.064  4.416  0.000    0.157    0.408 #> 26 ind60 ~~ ind60   1.000 0.000     NA     NA    1.000    1.000 #> 27 dem60 ~~ dem60   0.799 0.091  8.737  0.000    0.620    0.978 #> 28 dem65 ~~ dem65   0.026 0.046  0.579  0.562   -0.063    0.116"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-growth-vignette.html","id":"joint-structural-equation-model-latent-growth-with-strict-invariance-model","dir":"Articles","previous_headings":"","what":"Joint structural equation model: Latent growth with strict invariance model","title":"Linear Growth Modeling with Two-Stage Path Analysis","text":"tutorial, authors first performed longitudinal invariance testing found support strict invariance. moved fit latent growth model based strict invariance model, shown .","code":"jsem_growth_mod <- \" # factor loadings (with constraints) eta1 =~ 15.1749088 * s_g3 + l2 * r_g3 +  l3 * m_g3 eta2 =~ 15.1749088 * s_g5 + l2 * r_g5 +  l3 * m_g5 eta3 =~ 15.1749088 * s_g8 + l2 * r_g8 +  l3 * m_g8  # factor variances eta1 ~~ psi * eta1 eta2 ~~ psi * eta2 eta3 ~~ psi * eta3  # unique variances/covariances  s_g3 ~~ u1 * s_g3 + s_g5 + s_g8 s_g5 ~~ u1 * s_g5 + s_g8 s_g8 ~~ u1 * s_g8 r_g3 ~~ u2 * r_g3 + r_g5 + r_g8 r_g5 ~~ u2 * r_g5 + r_g8 r_g8 ~~ u2 * r_g8 m_g3 ~~ u3 * m_g3 + m_g5 + m_g8 m_g5 ~~ u3 * m_g5 + m_g8 m_g8 ~~ u3 * m_g8  # observed variable intercepts s_g3 ~ i1 * 1 s_g5 ~ i1 * 1 s_g8 ~ i1 * 1 r_g3 ~ i2 * 1 r_g5 ~ i2 * 1 r_g8 ~ i2 * 1 m_g3 ~ i3 * 1 m_g5 ~ i3 * 1 m_g8 ~ i3 * 1  # latent basis model i =~ 1 * eta1 + 1 * eta2 + 1 * eta3 s =~ 0 * eta1 + start(.5) * eta2 + 1 * eta3  i ~~ start(.8) * i s ~~ start(.5) * s i ~~ start(0) * s  i ~ 0 * 1 s ~ 1 \" jsem_growth_fit <- lavaan(jsem_growth_mod,                           data = eclsk,                           meanstructure = TRUE,                           estimator = \"ML\",                           fixed.x = FALSE)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-growth-vignette.html","id":"s-pa-latent-growth-with-strict-invariance-model","dir":"Articles","previous_headings":"","what":"2S-PA: Latent growth with strict invariance model","title":"Linear Growth Modeling with Two-Stage Path Analysis","text":"2S-PA, first specify strict invariance model, directly adopted tutorial. Based strict invariance model, obtain factor scores. second stage, use factor scores obtained strict invariance model model growth trajectory across time points. growth model “latent basis model” joint structural equation model.","code":"strict_mod <- \" # factor loadings eta1 =~ 15.1749088 * s_g3 + l2 * r_g3 + l3 * m_g3 eta2 =~ 15.1749088 * s_g5 + l2 * r_g5 + L3 * m_g5 eta3 =~ 15.1749088 * s_g8 + l2 * r_g8 + L3 * m_g8  # factor variances/covariances eta1 ~~ 1 * eta1 + eta2 + eta3 eta2 ~~ eta2 + eta3 eta3 ~~ eta3  # unique variances/covariances s_g3 ~~ u1 * s_g3 + s_g5 + s_g8 s_g5 ~~ u1 * s_g5 + s_g8 s_g8 ~~ u1 * s_g8 r_g3 ~~ u2 * r_g3 + r_g5 + r_g8 r_g5 ~~ u2 * r_g5 + r_g8 r_g8 ~~ u2 * r_g8 m_g3 ~~ u3 * m_g3 + m_g5 + m_g8 m_g5 ~~ u3 * m_g5 + m_g8 m_g8 ~~ u3 * m_g8  # latent variable intercepts eta1 ~ 0 * 1 eta2 ~ 1 eta3 ~ 1  # observed variable intercepts s_g3 ~ i1 * 1 s_g5 ~ i1 * 1 s_g8 ~ i1 * 1 r_g3 ~ i2 * 1 r_g5 ~ i2 * 1 r_g8 ~ i2 * 1 m_g3 ~ i3 * 1 m_g5 ~ i3 * 1 m_g8 ~ i3 * 1 \" # Get factor scores fs_dat <- get_fs(eclsk, model = strict_mod) # Growth model tspa_growth_mod <- \" i =~ 1 * eta1 + 1 * eta2 + 1 * eta3 s =~ 0 * eta1 + start(.5) * eta2 + 1 * eta3  # factor variances eta1 ~~ psi * eta1 eta2 ~~ psi * eta2 eta3 ~~ psi * eta3  i ~~ start(.8) * i s ~~ start(.5) * s i ~~ start(0) * s  i ~ 0 * 1 s ~ 1 \" # Fit the growth model tspa_growth_fit <- tspa(tspa_growth_mod, fs_dat,                         fsT = attr(fs_dat, \"fsT\"),                         fsL = attr(fs_dat, \"fsL\"),                         fsb = attr(fs_dat, \"fsb\"),                         estimator = \"ML\")"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-growth-vignette.html","id":"comparison","dir":"Articles","previous_headings":"","what":"Comparison","title":"Linear Growth Modeling with Two-Stage Path Analysis","text":"joint structural equation model 2S-PA yielded comparable estimates mean variances intercept slope factors.","code":"parameterestimates(tspa_growth_fit) |>   subset(subset = lhs %in% c(\"i\", \"s\") & op %in% c(\"~1\", \"~~\")) #>    lhs op rhs label    est    se       z pvalue ci.lower ci.upper #> 28   i ~~   i        0.915 0.049  18.587  0.000    0.819    1.012 #> 29   s ~~   s        0.117 0.016   7.367  0.000    0.086    0.148 #> 30   i ~~   s       -0.052 0.020  -2.679  0.007   -0.091   -0.014 #> 31   i ~1            0.000 0.000      NA     NA    0.000    0.000 #> 32   s ~1            2.000 0.018 112.572  0.000    1.965    2.034 parameterestimates(jsem_growth_fit) |>   subset(subset = lhs %in% c(\"i\", \"s\") & op  %in% c(\"~1\", \"~~\")) #>    lhs op rhs label    est    se      z pvalue ci.lower ci.upper #> 46   i ~~   i        0.941 0.052 18.138      0    0.839    1.042 #> 47   s ~~   s        0.117 0.017  6.826      0    0.083    0.150 #> 48   i ~~   s       -0.074 0.020 -3.646      0   -0.115   -0.034 #> 49   i ~1            0.000 0.000     NA     NA    0.000    0.000 #> 50   s ~1            1.991 0.024 84.586      0    1.945    2.038"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-plot-vignette.html","id":"single-group-single-predictor","dir":"Articles","previous_headings":"","what":"Single group, single predictor","title":"Diagnostic plots for `tspa` models","text":"recommend researchers examine diagnostic plot factor scores model. can use function tspa_plot() obtain two plots: () scatter plot factors, (b) residual plot factors. Features tspa_plot(): function able pass arguments base R plot() function. Users can define title scatter plot label names axis. Abbreviation argument allows users choose whether using abbreviated group names. Users can choose whether generating plot one one hitting  keyboard.","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4    # regressions     dem60 ~ ind60 '  fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60)  tspa_fit_1 <- tspa(model = \"dem60 ~ ind60\",                     data = fs_dat,                     se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472),                    meanstructure = T) # par(mar = c(2,2,3,2)) tspa_plot(tspa_fit_1,           col = \"blue\",           cex.lab = 1.2,           cex.axis = 1,           fscores_type = \"original\",           ask = TRUE)"},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-plot-vignette.html","id":"single-group-multiple-predictors","dir":"Articles","previous_headings":"","what":"Single group, multiple predictors","title":"Diagnostic plots for `tspa` models","text":"","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # # residual correlations   #   y1 ~~ y5   #   y2 ~~ y4 + y6   #   y3 ~~ y7   #   y4 ~~ y8   #   y6 ~~ y8 '  fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat_dem65 <- get_fs(data = PoliticalDemocracy,                         model = \"dem65 =~ y5 + y6 + y7 + y8\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60, fs_dat_dem65)  tspa_fit_2 <- tspa(model = \"dem60 ~ ind60                             dem65 ~ ind60 + dem60\",                     data = fs_dat,                     se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472,                                  dem65 = 0.5724405)) # Title, xlab, and ylab each with same names. tspa_plot(tspa_fit_2,           ps = 10,           col = \"blue\",           cex.lab = 1.2,           cex.axis = 1) # Title, xlab, and ylab each with separate names. tspa_plot(tspa_fit_2,           ps = 10,           col = \"darkgray\",           cex.lab = 1.2,           cex.axis = 1,           title = c(\"Scatterplot_I\", \"Scatterplot_II\", \"Scatterplot_III\"),           label_x = c(\"factor_1\", \"factor_2\", \"factor_3\"),           label_y = c(\"factor_a\", \"factor_b\", \"factor_c\"))"},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-plot-vignette.html","id":"multigroup-single-predictor","dir":"Articles","previous_headings":"","what":"Multigroup, single predictor","title":"Diagnostic plots for `tspa` models","text":"","code":"model <- '    # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9    # regressions     visual ~ speed '  # get factor scores fs_dat_visual <- get_fs(model = \"visual =~ x1 + x2 + x3\",                         data = HolzingerSwineford1939,                          group = \"school\") fs_dat_speed <- get_fs(model = \"speed =~ x7 + x8 + x9\",                         data = HolzingerSwineford1939,                         group = \"school\") fs_hs <- cbind(do.call(rbind, fs_dat_visual),                do.call(rbind, fs_dat_speed))  tspa_fit_3 <- tspa(model = \"visual ~ speed\",                    data = fs_hs,                    se_fs = data.frame(visual = c(0.3391326, 0.311828),                                       speed = c(0.2786875, 0.2740507)),                    group = \"school\"                    # group.equal = \"regressions\"                    ) tspa_plot(tspa_fit_3,           ps = 10,           col = \"darkgray\",           cex.lab = 1.2,           ask = FALSE,           cex.axis = 1) tspa_plot(tspa_fit_3,           ps = 10,           col = \"darkgray\",           cex.lab = 1.2,           cex.axis = 1,           ask = TRUE,           abbreviation = FALSE)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"combined-with-irt","dir":"Articles","previous_headings":"","what":"Combined with IRT","title":"Multi-Factor Measurement Model (OpenMx)","text":"Example Lai & Hsiao (2021, Psychological Methods) Joint model WLS Maximum Likelihood Note: extremely computational intensive Using tspa_mx_model() Alternatively, one can use named matrices (mat_ld mat_vc) specify names columns loading error covariance matrices. Compare joint model OpenMx Note: FIML couldn’t finish within 12 hours","code":"# Simulate data with mirt set.seed(1234) num_obs <- 1000 # Simulate theta eta <- MASS::mvrnorm(num_obs, mu = c(0, 0), Sigma = diag(c(1, 1 - 0.5^2)),                      empirical = TRUE) th1 <- eta[, 1] th2 <- -1 + 0.5 * th1 + eta[, 2] # items and response data a1 <- matrix(1, 10) d1 <- matrix(rnorm(10)) a2 <- matrix(runif(10, min = 0.5, max = 1.5)) d2 <- matrix(rnorm(10)) dat1 <- simdata(a = a1, d = d1, N = num_obs, itemtype = \"2PL\", Theta = th1) dat2 <- simdata(a = a2, d = d2, N = num_obs, itemtype = \"2PL\", Theta = th2) # Factor scores mod1 <- mirt(dat1, model = 1, itemtype = \"Rasch\", verbose = FALSE) mod2 <- mirt(dat2, model = 1, itemtype = \"2PL\", verbose = FALSE) fs1 <- fscores(mod1, full.scores.SE = TRUE) fs2 <- fscores(mod2, full.scores.SE = TRUE) lm(fs2[, 1] ~ fs1[, 1])  # attenuated coefficient #>  #> Call: #> lm(formula = fs2[, 1] ~ fs1[, 1]) #>  #> Coefficients: #> (Intercept)     fs1[, 1]   #>  -3.958e-06    3.135e-01 dat <- cbind(dat1, dat2) colnames(dat) <- paste0(\"i\", 1:20) wls_fit <- sem(\" f1 =~ i1 + i2 + i3 + i4 + i5 + i6 + i7 + i8 + i9 + i10 f2 =~ i11 + i12 + i13 + i14 + i15 + i16 + i17 + i18 + i19 + i20 f2 ~ f1 \", data = dat, ordered = TRUE, std.lv = TRUE) coef(wls_fit)[\"f2~f1\"] #>     f2~f1  #> 0.5398734 # Combine into data set fs_dat <- cbind(fs1, fs2) |>     as.data.frame() |>     setNames(c(\"fs1\", \"se_fs1\", \"fs2\", \"se_fs2\")) |>     # Compute reliability and error variances     within(expr = {         rel_fs1 <- 1 - se_fs1^2         rel_fs2 <- 1 - se_fs2^2         ev_fs1 <- se_fs1^2 * (1 - se_fs1^2)         ev_fs2 <- se_fs2^2 * (1 - se_fs2^2)     }) # Loading matL <- mxMatrix(     type = \"Diag\", nrow = 2, ncol = 2,     free = FALSE,     labels = c(\"data.rel_fs2\", \"data.rel_fs1\"),     name = \"L\" ) # Error matE <- mxMatrix(     type = \"Diag\", nrow = 2, ncol = 2,     free = FALSE,     labels = c(\"data.ev_fs2\", \"data.ev_fs1\"),     name = \"E\" ) fsreg_umx <- umxLav2RAM(     \"       fs2 ~ fs1       fs2 + fs1 ~ 1     \",     printTab = FALSE) #>  #> ?plot.MxModel options: std, means, digits, strip_zero, file, splines=T/F/ortho,..., min=, max =, same = , fixed, resid= 'circle|line|none' tspa_mx <- tspa_mx_model(fsreg_umx, data = fs_dat,                          mat_ld = matL, mat_vc = matE) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx) #> Running 2SPAD with 5 parameters # Summarize the results summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>           name matrix row col      Estimate  Std.Error A #> 1   fs1_to_fs2   m1.A fs2 fs1  0.4918588574 0.05137332   #> 2 fs2_with_fs2   m1.S fs2 fs2  0.7743465561 0.06820091   #> 3 fs1_with_fs1   m1.S fs1 fs1  0.9438755802 0.07108946   #> 4   one_to_fs2   m1.M   1 fs2 -0.0009927357 0.03846740   #> 5   one_to_fs1   m1.M   1 fs1  0.0019196592 0.03913422   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                   3995              4761.926 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 1000/4000 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -3228.074               4771.926                 4771.986 #> BIC:     -22834.557               4796.464                 4780.584 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-01-16 07:14:24  #> Wall clock time: 0.6010633 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.11  #> Need help?  See help(mxSummary) cross_load <- matrix(c(\"rel_fs2\", NA, NA, \"rel_fs1\"), nrow = 2) |>     `dimnames<-`(rep(list(c(\"fs2\", \"fs1\")), 2)) err_cov <- matrix(c(\"ev_fs2\", NA, NA, \"ev_fs1\"), nrow = 2) |>     `dimnames<-`(rep(list(c(\"fs2\", \"fs1\")), 2)) tspa_mx <- tspa_mx_model(fsreg_umx, data = fs_dat,                          mat_ld = cross_load, mat_vc = err_cov) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx) #> Running 2SPAD with 5 parameters # Summarize the results summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>           name matrix row col      Estimate  Std.Error A #> 1   fs1_to_fs2   m1.A fs2 fs1  0.4918588574 0.05137332   #> 2 fs2_with_fs2   m1.S fs2 fs2  0.7743465561 0.06820091   #> 3 fs1_with_fs1   m1.S fs1 fs1  0.9438755802 0.07108946   #> 4   one_to_fs2   m1.M   1 fs2 -0.0009927357 0.03846740   #> 5   one_to_fs1   m1.M   1 fs1  0.0019196592 0.03913422   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                   3995              4761.926 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 1000/4000 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -3228.074               4771.926                 4771.986 #> BIC:     -22834.557               4796.464                 4780.584 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-01-16 07:14:25  #> Wall clock time: 0.5930262 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.11  #> Need help?  See help(mxSummary) jreg_umx <- umxRAM(     \"     f1 =~ i1 + i2 + i3 + i4 + i5 + i6 + i7 + i8 + i9 + i10     f2 =~ i11 + i12 + i13 + i14 + i15 + i16 + i17 + i18 + i19 + i20     f2 ~ f1     i1 + i2 + i3 + i4 + i5 + i6 + i7 + i8 + i9 + i10 ~ 0     i11 + i12 + i13 + i14 + i15 + i16 + i17 + i18 + i19 + i20 ~ 0     i1 ~~ 1 * i1     i2 ~~ 1 * i2     i3 ~~ 1 * i3     i4 ~~ 1 * i4     i5 ~~ 1 * i5     i6 ~~ 1 * i6     i7 ~~ 1 * i7     i8 ~~ 1 * i8     i9 ~~ 1 * i9     i10 ~~ 1 * i10     i11 ~~ 1 * i11     i12 ~~ 1 * i12     i13 ~~ 1 * i13     i14 ~~ 1 * i14     i15 ~~ 1 * i15     i16 ~~ 1 * i16     i17 ~~ 1 * i17     i18 ~~ 1 * i18     i19 ~~ 1 * i19     i20 ~~ 1 * i20     \",     data = lapply(as.data.frame(dat), FUN = mxFactor,                  levels = c(0, 1)) |>                as.data.frame(),     type = \"DWLS\") #> 2 latent variables were created:f1, f2. #> object 'threshMat' created to handle: 20 binary variables:'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7', 'i8', 'i9', 'i10', 'i11', 'i12', 'i13', 'i14', 'i15', 'i16', 'i17', 'i18', 'i19', and 'i20' #> 20 trait(s) are binary: 'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7', 'i8', 'i9', 'i10', 'i11', 'i12', 'i13', 'i14', 'i15', 'i16', 'i17', 'i18', 'i19', and 'i20' #> For these, you you MUST fix the mean and variance of the latent traits driving each variable (usually 0 & 1 respectively) . #> See ?umxThresholdMatrix #> Using deviation-based model: Thresholds will be in'threshMat' based on deviations in 'deviations_for_thresh' #> Running m1 with 41 parameters #> ?umxSummary options: std=T|F', digits=, report= 'html', filter= 'NS' & more #>  #>  #> Table: Parameter loadings for model 'm1' #>  #> |   |name         | Estimate|SE    |type             | #> |:--|:------------|--------:|:-----|:----------------| #> |1  |f1_to_i1     |    1.000|0     |Factor loading   | #> |2  |f1_to_i2     |    0.697|0.118 |Factor loading   | #> |3  |f1_to_i3     |    0.784|0.119 |Factor loading   | #> |4  |f1_to_i4     |    0.684|0.121 |Factor loading   | #> |5  |f1_to_i5     |    0.659|0.119 |Factor loading   | #> |6  |f1_to_i6     |    0.772|0.13  |Factor loading   | #> |7  |f1_to_i7     |    0.699|0.124 |Factor loading   | #> |8  |f1_to_i8     |    0.764|0.124 |Factor loading   | #> |9  |f1_to_i9     |    0.678|0.111 |Factor loading   | #> |10 |f1_to_i10    |    0.868|0.152 |Factor loading   | #> |12 |f2_to_i11    |    1.000|0     |Factor loading   | #> |13 |f2_to_i12    |    1.809|0.314 |Factor loading   | #> |14 |f2_to_i13    |    0.979|0.186 |Factor loading   | #> |15 |f2_to_i14    |    1.878|0.327 |Factor loading   | #> |16 |f2_to_i15    |    1.301|0.236 |Factor loading   | #> |17 |f2_to_i16    |    2.386|0.406 |Factor loading   | #> |18 |f2_to_i17    |    1.654|0.299 |Factor loading   | #> |19 |f2_to_i18    |    2.049|0.36  |Factor loading   | #> |20 |f2_to_i19    |    0.908|0.196 |Factor loading   | #> |21 |f2_to_i20    |    1.218|0.246 |Factor loading   | #> |11 |f1_to_f2     |    0.251|0.052 |Factor to factor | #> |42 |f1_with_f1   |    0.602|0.135 |Factor Variance  | #> |43 |f2_with_f2   |    0.130|0.037 |Factor Variance  | #> |22 |i1_with_i1   |    1.000|0     |Residual         | #> |23 |i2_with_i2   |    1.000|0     |Residual         | #> |24 |i3_with_i3   |    1.000|0     |Residual         | #> |25 |i4_with_i4   |    1.000|0     |Residual         | #> |26 |i5_with_i5   |    1.000|0     |Residual         | #> |27 |i6_with_i6   |    1.000|0     |Residual         | #> |28 |i7_with_i7   |    1.000|0     |Residual         | #> |29 |i8_with_i8   |    1.000|0     |Residual         | #> |30 |i9_with_i9   |    1.000|0     |Residual         | #> |31 |i10_with_i10 |    1.000|0     |Residual         | #> |32 |i11_with_i11 |    1.000|0     |Residual         | #> |33 |i12_with_i12 |    1.000|0     |Residual         | #> |34 |i13_with_i13 |    1.000|0     |Residual         | #> |35 |i14_with_i14 |    1.000|0     |Residual         | #> |36 |i15_with_i15 |    1.000|0     |Residual         | #> |37 |i16_with_i16 |    1.000|0     |Residual         | #> |38 |i17_with_i17 |    1.000|0     |Residual         | #> |39 |i18_with_i18 |    1.000|0     |Residual         | #> |40 |i19_with_i19 |    1.000|0     |Residual         | #> |41 |i20_with_i20 |    1.000|0     |Residual         | #>  #> Model Fit: χ²(169) = 206.19, p = 0.027; CFI = NA; TLI = NA; RMSEA = 0.015 #> Algebra'threshMat': #>  #>  #> |     |    i1|    i2|     i3|     i4|    i5|    i6|    i7|     i8|   i9|   i10|    i11|   i12|   i13|   i14|   i15|   i16|   i17|   i18|    i19|  i20| #> |:----|-----:|-----:|------:|------:|-----:|-----:|-----:|------:|----:|-----:|------:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|------:|----:| #> |th_1 | 0.613| 0.111| -0.018| -0.677| 0.945| 0.642| 0.982| -0.352| 0.31| 1.113| -0.226| 0.596| 0.179| 0.531| 0.367| 0.314| 1.012| 0.745| -1.331| 1.13| mxStandardizeRAMpaths(jreg_umx_fit, SE = TRUE)[4, ] #>                      name          label matrix   row   col Raw.Value    Raw.SE #> 4 latent_variables.A[9,8] ind60_to_dem60      A dem60 ind60  1.439314 0.3768976 #>   Std.Value    Std.SE #> 4 0.4604654 0.1010572"},{"path":"https://gengrui-zhang.github.io/R2spa/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Mark Hok Chio Lai. Author, maintainer. Yixiao Li. Author. Winnie Wing-Yee Tse. Author. Gengrui Zhang Zhang. Author.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lai M, Li Y, Tse W, Zhang G (2024). R2spa: R package two-stage path analysis (2S-PA) adjust measurement errors. R package version 0.0.3, https://gengrui-zhang.github.io/R2spa/.","code":"@Manual{,   title = {R2spa: An R package for two-stage path analysis (2S-PA) to adjust for measurement errors},   author = {Mark Hok Chio Lai and Yixiao Li and Winnie Wing-Yee Tse and Gengrui Zhang Zhang},   year = {2024},   note = {R package version 0.0.3},   url = {https://gengrui-zhang.github.io/R2spa/}, }"},{"path":"https://gengrui-zhang.github.io/R2spa/index.html","id":"r2spa","dir":"","previous_headings":"","what":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","title":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","text":"R2spa free open-source R package performs two-stage path analysis (2S-PA). 2S-PA, researchers can perform path analysis first obtaining factor scores adjusting measurement errors using estimates observation-specific reliability standard error factor scores. viable alternative SEM, 2S-PA shown give equally-good estimates SEM relatively simple models large sample sizes, well give accurate parameter estimates, better control Type error rates, substantially less convergence problems complex models small sample sizes.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","text":"package still developmental stage can installed GitHub :","code":"# install.packages(\"remotes\") remotes::install_github(\"Gengrui-Zhang/R2spa\")"},{"path":"https://gengrui-zhang.github.io/R2spa/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","text":"package based upon work supported National Science Foundation Grant . 2141790.","code":"library(lavaan) library(R2spa)  # Joint model model <- '   # latent variable definitions     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4    # regression     dem60 ~ ind60 ' # 2S-PA # Stage 1: Get factor scores and standard errors for each latent construct fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                        model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                        model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60)  # get_fs() gives a dataframe with factor scores and standard errors head(fs_dat) #>     fs_ind60 fs_ind60_se ind60_by_fs_ind60 ev_fs_ind60   fs_dem60 fs_dem60_se #> 1 -0.5261683   0.1213615         0.9657673  0.01472862 -2.7487224   0.6756472 #> 2  0.1436527   0.1213615         0.9657673  0.01472862 -3.0360803   0.6756472 #> 3  0.7143559   0.1213615         0.9657673  0.01472862  2.6718589   0.6756472 #> 4  1.2399257   0.1213615         0.9657673  0.01472862  2.9936997   0.6756472 #> 5  0.8319080   0.1213615         0.9657673  0.01472862  1.9242932   0.6756472 #> 6  0.2123845   0.1213615         0.9657673  0.01472862  0.9922798   0.6756472 #>   dem60_by_fs_dem60 ev_fs_dem60 #> 1         0.8868049   0.4564991 #> 2         0.8868049   0.4564991 #> 3         0.8868049   0.4564991 #> 4         0.8868049   0.4564991 #> 5         0.8868049   0.4564991 #> 6         0.8868049   0.4564991 # Stage 2: Perform 2S-PA tspa_fit <- tspa(   model = \"dem60 ~ ind60\",   data = fs_dat,   se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472) ) parameterestimates(tspa_fit) #>        lhs op      rhs   est    se     z pvalue ci.lower ci.upper #> 1    ind60 =~ fs_ind60 1.000 0.000    NA     NA    1.000    1.000 #> 2    dem60 =~ fs_dem60 1.000 0.000    NA     NA    1.000    1.000 #> 3 fs_ind60 ~~ fs_ind60 0.015 0.000    NA     NA    0.015    0.015 #> 4 fs_dem60 ~~ fs_dem60 0.456 0.000    NA     NA    0.456    0.456 #> 5    dem60  ~    ind60 1.329 0.332 4.000      0    0.678    1.981 #> 6    ind60 ~~    ind60 0.416 0.070 5.914      0    0.278    0.553 #> 7    dem60 ~~    dem60 2.842 0.543 5.235      0    1.778    3.906"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/block_diag.html","id":null,"dir":"Reference","previous_headings":"","what":"Create block diagonal matrix — block_diag","title":"Create block diagonal matrix — block_diag","text":"Create block diagonal matrix","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/block_diag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create block diagonal matrix — block_diag","text":"","code":"block_diag(...)"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/block_diag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create block diagonal matrix — block_diag","text":"... Either multiple matrices list matrices.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute factor scores — compute_fscore","title":"Compute factor scores — compute_fscore","text":"Compute factor scores","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute factor scores — compute_fscore","text":"","code":"compute_fscore(   y,   lambda,   theta,   psi = NULL,   nu = NULL,   alpha = NULL,   method = c(\"regression\", \"Bartlett\"),   center_y = TRUE,   acov = FALSE,   fs_matrices = FALSE )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute factor scores — compute_fscore","text":"y N x p matrix row response vector. one observation, matrix one row. lambda p x q matrix factor loadings. theta p x p matrix unique variance-covariances. psi q x q matrix latent factor variance-covariances. nu vector length p measurement intercepts. alpha vector length q latent means. method character string indicating method computing factor scores. Currently, \"regression\" supported. center_y Logical indicating whether y mean-centered. Default TRUE. acov Logical indicating whether asymptotic covariance matrix factor scores returned attribute. fs_matrices Logical indicating whether covariances error portion factor scores (fsT), factor score loading matrix (\\(L\\); fsL) intercept vector (\\(b\\); fsb) returned. loading intercept matrices implied loadings intercepts model using factor scores indicators latent variables. TRUE, matrices added attributes.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute factor scores — compute_fscore","text":"N x p matrix factor scores.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute factor scores — compute_fscore","text":"","code":"library(lavaan) #> This is lavaan 0.6-17 #> lavaan is FREE software! Please report any bugs. fit <- cfa(\" ind60 =~ x1 + x2 + x3              dem60 =~ y1 + y2 + y3 + y4 \",            data = PoliticalDemocracy) fs_lavaan <- lavPredict(fit, method = \"Bartlett\") # Using R2spa::compute_fscore() est <- lavInspect(fit, what = \"est\") fs_hand <- compute_fscore(lavInspect(fit, what = \"data\"),                           lambda = est$lambda,                           theta = est$theta,                           psi = est$psi,                           method = \"Bartlett\") fs_hand - fs_lavaan  # same scores #>       ind60 dem60 #>  [1,]     0     0 #>  [2,]     0     0 #>  [3,]     0     0 #>  [4,]     0     0 #>  [5,]     0     0 #>  [6,]     0     0 #>  [7,]     0     0 #>  [8,]     0     0 #>  [9,]     0     0 #> [10,]     0     0 #> [11,]     0     0 #> [12,]     0     0 #> [13,]     0     0 #> [14,]     0     0 #> [15,]     0     0 #> [16,]     0     0 #> [17,]     0     0 #> [18,]     0     0 #> [19,]     0     0 #> [20,]     0     0 #> [21,]     0     0 #> [22,]     0     0 #> [23,]     0     0 #> [24,]     0     0 #> [25,]     0     0 #> [26,]     0     0 #> [27,]     0     0 #> [28,]     0     0 #> [29,]     0     0 #> [30,]     0     0 #> [31,]     0     0 #> [32,]     0     0 #> [33,]     0     0 #> [34,]     0     0 #> [35,]     0     0 #> [36,]     0     0 #> [37,]     0     0 #> [38,]     0     0 #> [39,]     0     0 #> [40,]     0     0 #> [41,]     0     0 #> [42,]     0     0 #> [43,]     0     0 #> [44,]     0     0 #> [45,]     0     0 #> [46,]     0     0 #> [47,]     0     0 #> [48,]     0     0 #> [49,]     0     0 #> [50,]     0     0 #> [51,]     0     0 #> [52,]     0     0 #> [53,]     0     0 #> [54,]     0     0 #> [55,]     0     0 #> [56,]     0     0 #> [57,]     0     0 #> [58,]     0     0 #> [59,]     0     0 #> [60,]     0     0 #> [61,]     0     0 #> [62,]     0     0 #> [63,]     0     0 #> [64,]     0     0 #> [65,]     0     0 #> [66,]     0     0 #> [67,]     0     0 #> [68,]     0     0 #> [69,]     0     0 #> [70,]     0     0 #> [71,]     0     0 #> [72,]     0     0 #> [73,]     0     0 #> [74,]     0     0 #> [75,]     0     0"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"Get Factor Scores Corresponding Standard Error Measurement","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"","code":"get_fs(   data,   model = NULL,   group = NULL,   method = c(\"regression\", \"Bartlett\"),   corrected_fsT = FALSE,   vfsLT = FALSE,   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"data data frame containing indicators. model optional string specifying measurement model lavaan syntax. See model.syntax information. group Character. Name grouping variable multiple group analysis, passed cfa. method Character. Method computing factor scores (options \"regression\" \"Bartlett\"). Currently, default \"regression\" consistent lavPredict, Bartlett scores desirable properties may preferred 2S-PA. corrected_fsT Logical. Whether correct sampling error factor score weights computing error variance estimates factor scores. vfsLT Logical. Whether return covariance matrix fsT fsL, can used input vcov_corrected() obtain corrected covariances standard errors tspa() results. currently ignored. ... additional arguments passed cfa. See lavOptions complete list.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"data frame containing factor scores (prefix \"fs_\"), standard errors (suffix \"_se\"), implied loadings factor \"_by_\" factor scores, error variance-covariance factor scores (prefix \"evfs_\"). following also returned attributes: * fsT: error covariance factor scores * fsL: loading matrix factor scores * fsb: intercepts factor scores * scoring_matrix: weights computing factor scores items","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"","code":"library(lavaan) get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\")]) #>          fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1 #> 1  -0.52616832 0.1213615   0.9657673 0.01472862 #> 2   0.14365274 0.1213615   0.9657673 0.01472862 #> 3   0.71435592 0.1213615   0.9657673 0.01472862 #> 4   1.23992565 0.1213615   0.9657673 0.01472862 #> 5   0.83190803 0.1213615   0.9657673 0.01472862 #> 6   0.21238453 0.1213615   0.9657673 0.01472862 #> 7   0.11880855 0.1213615   0.9657673 0.01472862 #> 8   0.11322703 0.1213615   0.9657673 0.01472862 #> 9   0.25617279 0.1213615   0.9657673 0.01472862 #> 10  0.37112496 0.1213615   0.9657673 0.01472862 #> 11  0.67281395 0.1213615   0.9657673 0.01472862 #> 12  0.56885577 0.1213615   0.9657673 0.01472862 #> 13  1.31369791 0.1213615   0.9657673 0.01472862 #> 14  0.22042629 0.1213615   0.9657673 0.01472862 #> 15  0.57849228 0.1213615   0.9657673 0.01472862 #> 16  0.37805983 0.1213615   0.9657673 0.01472862 #> 17  0.05734046 0.1213615   0.9657673 0.01472862 #> 18 -0.01609202 0.1213615   0.9657673 0.01472862 #> 19  0.88923616 0.1213615   0.9657673 0.01472862 #> 20  1.11445897 0.1213615   0.9657673 0.01472862 #> 21  0.94657339 0.1213615   0.9657673 0.01472862 #> 22  0.90122770 0.1213615   0.9657673 0.01472862 #> 23  0.58409450 0.1213615   0.9657673 0.01472862 #> 24  0.64089192 0.1213615   0.9657673 0.01472862 #> 25  0.91021968 0.1213615   0.9657673 0.01472862 #> 26 -0.89660969 0.1213615   0.9657673 0.01472862 #> 27 -0.13195991 0.1213615   0.9657673 0.01472862 #> 28 -0.52968769 0.1213615   0.9657673 0.01472862 #> 29 -0.81799629 0.1213615   0.9657673 0.01472862 #> 30 -1.27199371 0.1213615   0.9657673 0.01472862 #> 31 -0.32096024 0.1213615   0.9657673 0.01472862 #> 32 -1.16780103 0.1213615   0.9657673 0.01472862 #> 33 -0.12295473 0.1213615   0.9657673 0.01472862 #> 34 -0.04285945 0.1213615   0.9657673 0.01472862 #> 35 -0.34323505 0.1213615   0.9657673 0.01472862 #> 36 -0.60541633 0.1213615   0.9657673 0.01472862 #> 37  0.17688718 0.1213615   0.9657673 0.01472862 #> 38 -0.55066055 0.1213615   0.9657673 0.01472862 #> 39 -1.05988219 0.1213615   0.9657673 0.01472862 #> 40 -0.04138802 0.1213615   0.9657673 0.01472862 #> 41 -0.12611837 0.1213615   0.9657673 0.01472862 #> 42 -0.60322892 0.1213615   0.9657673 0.01472862 #> 43 -0.11057176 0.1213615   0.9657673 0.01472862 #> 44 -1.06423085 0.1213615   0.9657673 0.01472862 #> 45 -1.08354999 0.1213615   0.9657673 0.01472862 #> 46 -0.84009484 0.1213615   0.9657673 0.01472862 #> 47 -1.14678213 0.1213615   0.9657673 0.01472862 #> 48 -0.57578976 0.1213615   0.9657673 0.01472862 #> 49  0.07186692 0.1213615   0.9657673 0.01472862 #> 50  0.14682421 0.1213615   0.9657673 0.01472862 #> 51  0.35871830 0.1213615   0.9657673 0.01472862 #> 52 -0.43403195 0.1213615   0.9657673 0.01472862 #> 53  0.44603111 0.1213615   0.9657673 0.01472862 #> 54  0.26352000 0.1213615   0.9657673 0.01472862 #> 55  0.55051165 0.1213615   0.9657673 0.01472862 #> 56  0.23453122 0.1213615   0.9657673 0.01472862 #> 57  0.27968138 0.1213615   0.9657673 0.01472862 #> 58  0.70960640 0.1213615   0.9657673 0.01472862 #> 59  0.25227978 0.1213615   0.9657673 0.01472862 #> 60  1.18849297 0.1213615   0.9657673 0.01472862 #> 61  0.21104946 0.1213615   0.9657673 0.01472862 #> 62 -1.16516281 0.1213615   0.9657673 0.01472862 #> 63 -0.85560065 0.1213615   0.9657673 0.01472862 #> 64  0.13398476 0.1213615   0.9657673 0.01472862 #> 65 -0.07912189 0.1213615   0.9657673 0.01472862 #> 66 -0.27146711 0.1213615   0.9657673 0.01472862 #> 67 -0.04417217 0.1213615   0.9657673 0.01472862 #> 68 -1.33425662 0.1213615   0.9657673 0.01472862 #> 69 -0.38720750 0.1213615   0.9657673 0.01472862 #> 70 -0.55355511 0.1213615   0.9657673 0.01472862 #> 71 -0.72242623 0.1213615   0.9657673 0.01472862 #> 72  0.30607449 0.1213615   0.9657673 0.01472862 #> 73  0.77707950 0.1213615   0.9657673 0.01472862 #> 74  0.06847481 0.1213615   0.9657673 0.01472862 #> 75 -0.11052927 0.1213615   0.9657673 0.01472862  # Multiple factors get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\", \"y1\", \"y2\", \"y3\", \"y4\")],        model = \" ind60 =~ x1 + x2 + x3                  dem60 =~ y1 + y2 + y3 + y4 \") #>       fs_ind60    fs_dem60 fs_ind60_se fs_dem60_se ind60_by_fs_ind60 #> 1  -0.54258816 -2.74640573   0.1245694   0.6307323         0.9553858 #> 2   0.12647664 -2.85646114   0.1245694   0.6307323         0.9553858 #> 3   0.73408891  2.74401728   0.1245694   0.6307323         0.9553858 #> 4   1.25253604  3.10856431   0.1245694   0.6307323         0.9553858 #> 5   0.83355267  1.92455641   0.1245694   0.6307323         0.9553858 #> 6   0.22426801  1.02292332   0.1245694   0.6307323         0.9553858 #> 7   0.12517739  1.00406461   0.1245694   0.6307323         0.9553858 #> 8   0.11783867 -0.37216403   0.1245694   0.6307323         0.9553858 #> 9   0.25175134 -1.24897911   0.1245694   0.6307323         0.9553858 #> 10  0.39938631  2.85267059   0.1245694   0.6307323         0.9553858 #> 11  0.67497777  1.41959595   0.1245694   0.6307323         0.9553858 #> 12  0.56462020  1.08769844   0.1245694   0.6307323         0.9553858 #> 13  1.31236592  1.54090232   0.1245694   0.6307323         0.9553858 #> 14  0.23246021  1.77370863   0.1245694   0.6307323         0.9553858 #> 15  0.58638481  2.45676871   0.1245694   0.6307323         0.9553858 #> 16  0.38404785  2.35887573   0.1245694   0.6307323         0.9553858 #> 17  0.05076465  0.04034088   0.1245694   0.6307323         0.9553858 #> 18 -0.01747337 -1.86718064   0.1245694   0.6307323         0.9553858 #> 19  0.90920762  3.61477756   0.1245694   0.6307323         0.9553858 #> 20  1.12553557  0.88355273   0.1245694   0.6307323         0.9553858 #> 21  0.97202590  3.62673300   0.1245694   0.6307323         0.9553858 #> 22  0.87820036 -3.02428925   0.1245694   0.6307323         0.9553858 #> 23  0.57540754 -1.51695438   0.1245694   0.6307323         0.9553858 #> 24  0.66221224  2.76341635   0.1245694   0.6307323         0.9553858 #> 25  0.92358281  2.00507336   0.1245694   0.6307323         0.9553858 #> 26 -0.89353051 -0.92008050   0.1245694   0.6307323         0.9553858 #> 27 -0.13984744 -1.19025576   0.1245694   0.6307323         0.9553858 #> 28 -0.53828496 -1.01247764   0.1245694   0.6307323         0.9553858 #> 29 -0.80834865  0.10456709   0.1245694   0.6307323         0.9553858 #> 30 -1.25324343 -0.71847055   0.1245694   0.6307323         0.9553858 #> 31 -0.33373641 -1.61401581   0.1245694   0.6307323         0.9553858 #> 32 -1.17441075 -3.27250363   0.1245694   0.6307323         0.9553858 #> 33 -0.12409974 -1.17530231   0.1245694   0.6307323         0.9553858 #> 34 -0.04239173 -0.53796274   0.1245694   0.6307323         0.9553858 #> 35 -0.34010528  0.74552889   0.1245694   0.6307323         0.9553858 #> 36 -0.58953870  1.61018662   0.1245694   0.6307323         0.9553858 #> 37  0.17453657 -0.28144814   0.1245694   0.6307323         0.9553858 #> 38 -0.54457243  0.37694690   0.1245694   0.6307323         0.9553858 #> 39 -1.05196602 -0.62919501   0.1245694   0.6307323         0.9553858 #> 40 -0.05504697 -0.03346842   0.1245694   0.6307323         0.9553858 #> 41 -0.12364358 -0.38394102   0.1245694   0.6307323         0.9553858 #> 42 -0.59058710  1.35347275   0.1245694   0.6307323         0.9553858 #> 43 -0.11968796  0.89227782   0.1245694   0.6307323         0.9553858 #> 44 -1.07176064 -2.08481096   0.1245694   0.6307323         0.9553858 #> 45 -1.09139097 -2.07944291   0.1245694   0.6307323         0.9553858 #> 46 -0.83287255  1.59590721   0.1245694   0.6307323         0.9553858 #> 47 -1.14519896 -1.53352201   0.1245694   0.6307323         0.9553858 #> 48 -0.56115378  2.08138051   0.1245694   0.6307323         0.9553858 #> 49  0.06493340 -1.04044137   0.1245694   0.6307323         0.9553858 #> 50  0.15671638  1.72618633   0.1245694   0.6307323         0.9553858 #> 51  0.34626130 -1.24967043   0.1245694   0.6307323         0.9553858 #> 52 -0.45158373 -2.31742576   0.1245694   0.6307323         0.9553858 #> 53  0.43233465 -1.07533341   0.1245694   0.6307323         0.9553858 #> 54  0.25779725 -0.02904676   0.1245694   0.6307323         0.9553858 #> 55  0.51730650 -2.78207923   0.1245694   0.6307323         0.9553858 #> 56  0.20104991 -2.49001474   0.1245694   0.6307323         0.9553858 #> 57  0.25318620 -2.52145147   0.1245694   0.6307323         0.9553858 #> 58  0.72354623  1.86717109   0.1245694   0.6307323         0.9553858 #> 59  0.24619740 -0.93321102   0.1245694   0.6307323         0.9553858 #> 60  1.21681210  3.19853937   0.1245694   0.6307323         0.9553858 #> 61  0.18167599 -3.15685030   0.1245694   0.6307323         0.9553858 #> 62 -1.16605067 -3.41334680   0.1245694   0.6307323         0.9553858 #> 63 -0.86491026 -3.11864398   0.1245694   0.6307323         0.9553858 #> 64  0.10990059 -0.47238885   0.1245694   0.6307323         0.9553858 #> 65 -0.07376176  2.95292007   0.1245694   0.6307323         0.9553858 #> 66 -0.28782931 -1.96509718   0.1245694   0.6307323         0.9553858 #> 67 -0.02508160  2.96218478   0.1245694   0.6307323         0.9553858 #> 68 -1.31843215 -1.59567027   0.1245694   0.6307323         0.9553858 #> 69 -0.40462357 -1.79146161   0.1245694   0.6307323         0.9553858 #> 70 -0.55568363 -1.01578892   0.1245694   0.6307323         0.9553858 #> 71 -0.71308015  0.08818212   0.1245694   0.6307323         0.9553858 #> 72  0.31014319  1.70765911   0.1245694   0.6307323         0.9553858 #> 73  0.79092897  1.86102556   0.1245694   0.6307323         0.9553858 #> 74  0.08770237  3.12885767   0.1245694   0.6307323         0.9553858 #> 75 -0.14138149 -2.41398025   0.1245694   0.6307323         0.9553858 #>    ind60_by_fs_dem60 dem60_by_fs_ind60 dem60_by_fs_dem60 ev_fs_ind60 #> 1           0.181827       0.005867694         0.8688887  0.01551752 #> 2           0.181827       0.005867694         0.8688887  0.01551752 #> 3           0.181827       0.005867694         0.8688887  0.01551752 #> 4           0.181827       0.005867694         0.8688887  0.01551752 #> 5           0.181827       0.005867694         0.8688887  0.01551752 #> 6           0.181827       0.005867694         0.8688887  0.01551752 #> 7           0.181827       0.005867694         0.8688887  0.01551752 #> 8           0.181827       0.005867694         0.8688887  0.01551752 #> 9           0.181827       0.005867694         0.8688887  0.01551752 #> 10          0.181827       0.005867694         0.8688887  0.01551752 #> 11          0.181827       0.005867694         0.8688887  0.01551752 #> 12          0.181827       0.005867694         0.8688887  0.01551752 #> 13          0.181827       0.005867694         0.8688887  0.01551752 #> 14          0.181827       0.005867694         0.8688887  0.01551752 #> 15          0.181827       0.005867694         0.8688887  0.01551752 #> 16          0.181827       0.005867694         0.8688887  0.01551752 #> 17          0.181827       0.005867694         0.8688887  0.01551752 #> 18          0.181827       0.005867694         0.8688887  0.01551752 #> 19          0.181827       0.005867694         0.8688887  0.01551752 #> 20          0.181827       0.005867694         0.8688887  0.01551752 #> 21          0.181827       0.005867694         0.8688887  0.01551752 #> 22          0.181827       0.005867694         0.8688887  0.01551752 #> 23          0.181827       0.005867694         0.8688887  0.01551752 #> 24          0.181827       0.005867694         0.8688887  0.01551752 #> 25          0.181827       0.005867694         0.8688887  0.01551752 #> 26          0.181827       0.005867694         0.8688887  0.01551752 #> 27          0.181827       0.005867694         0.8688887  0.01551752 #> 28          0.181827       0.005867694         0.8688887  0.01551752 #> 29          0.181827       0.005867694         0.8688887  0.01551752 #> 30          0.181827       0.005867694         0.8688887  0.01551752 #> 31          0.181827       0.005867694         0.8688887  0.01551752 #> 32          0.181827       0.005867694         0.8688887  0.01551752 #> 33          0.181827       0.005867694         0.8688887  0.01551752 #> 34          0.181827       0.005867694         0.8688887  0.01551752 #> 35          0.181827       0.005867694         0.8688887  0.01551752 #> 36          0.181827       0.005867694         0.8688887  0.01551752 #> 37          0.181827       0.005867694         0.8688887  0.01551752 #> 38          0.181827       0.005867694         0.8688887  0.01551752 #> 39          0.181827       0.005867694         0.8688887  0.01551752 #> 40          0.181827       0.005867694         0.8688887  0.01551752 #> 41          0.181827       0.005867694         0.8688887  0.01551752 #> 42          0.181827       0.005867694         0.8688887  0.01551752 #> 43          0.181827       0.005867694         0.8688887  0.01551752 #> 44          0.181827       0.005867694         0.8688887  0.01551752 #> 45          0.181827       0.005867694         0.8688887  0.01551752 #> 46          0.181827       0.005867694         0.8688887  0.01551752 #> 47          0.181827       0.005867694         0.8688887  0.01551752 #> 48          0.181827       0.005867694         0.8688887  0.01551752 #> 49          0.181827       0.005867694         0.8688887  0.01551752 #> 50          0.181827       0.005867694         0.8688887  0.01551752 #> 51          0.181827       0.005867694         0.8688887  0.01551752 #> 52          0.181827       0.005867694         0.8688887  0.01551752 #> 53          0.181827       0.005867694         0.8688887  0.01551752 #> 54          0.181827       0.005867694         0.8688887  0.01551752 #> 55          0.181827       0.005867694         0.8688887  0.01551752 #> 56          0.181827       0.005867694         0.8688887  0.01551752 #> 57          0.181827       0.005867694         0.8688887  0.01551752 #> 58          0.181827       0.005867694         0.8688887  0.01551752 #> 59          0.181827       0.005867694         0.8688887  0.01551752 #> 60          0.181827       0.005867694         0.8688887  0.01551752 #> 61          0.181827       0.005867694         0.8688887  0.01551752 #> 62          0.181827       0.005867694         0.8688887  0.01551752 #> 63          0.181827       0.005867694         0.8688887  0.01551752 #> 64          0.181827       0.005867694         0.8688887  0.01551752 #> 65          0.181827       0.005867694         0.8688887  0.01551752 #> 66          0.181827       0.005867694         0.8688887  0.01551752 #> 67          0.181827       0.005867694         0.8688887  0.01551752 #> 68          0.181827       0.005867694         0.8688887  0.01551752 #> 69          0.181827       0.005867694         0.8688887  0.01551752 #> 70          0.181827       0.005867694         0.8688887  0.01551752 #> 71          0.181827       0.005867694         0.8688887  0.01551752 #> 72          0.181827       0.005867694         0.8688887  0.01551752 #> 73          0.181827       0.005867694         0.8688887  0.01551752 #> 74          0.181827       0.005867694         0.8688887  0.01551752 #> 75          0.181827       0.005867694         0.8688887  0.01551752 #>    ecov_fs_dem60_fs_ind60 ev_fs_dem60 #> 1             0.005632564   0.3978232 #> 2             0.005632564   0.3978232 #> 3             0.005632564   0.3978232 #> 4             0.005632564   0.3978232 #> 5             0.005632564   0.3978232 #> 6             0.005632564   0.3978232 #> 7             0.005632564   0.3978232 #> 8             0.005632564   0.3978232 #> 9             0.005632564   0.3978232 #> 10            0.005632564   0.3978232 #> 11            0.005632564   0.3978232 #> 12            0.005632564   0.3978232 #> 13            0.005632564   0.3978232 #> 14            0.005632564   0.3978232 #> 15            0.005632564   0.3978232 #> 16            0.005632564   0.3978232 #> 17            0.005632564   0.3978232 #> 18            0.005632564   0.3978232 #> 19            0.005632564   0.3978232 #> 20            0.005632564   0.3978232 #> 21            0.005632564   0.3978232 #> 22            0.005632564   0.3978232 #> 23            0.005632564   0.3978232 #> 24            0.005632564   0.3978232 #> 25            0.005632564   0.3978232 #> 26            0.005632564   0.3978232 #> 27            0.005632564   0.3978232 #> 28            0.005632564   0.3978232 #> 29            0.005632564   0.3978232 #> 30            0.005632564   0.3978232 #> 31            0.005632564   0.3978232 #> 32            0.005632564   0.3978232 #> 33            0.005632564   0.3978232 #> 34            0.005632564   0.3978232 #> 35            0.005632564   0.3978232 #> 36            0.005632564   0.3978232 #> 37            0.005632564   0.3978232 #> 38            0.005632564   0.3978232 #> 39            0.005632564   0.3978232 #> 40            0.005632564   0.3978232 #> 41            0.005632564   0.3978232 #> 42            0.005632564   0.3978232 #> 43            0.005632564   0.3978232 #> 44            0.005632564   0.3978232 #> 45            0.005632564   0.3978232 #> 46            0.005632564   0.3978232 #> 47            0.005632564   0.3978232 #> 48            0.005632564   0.3978232 #> 49            0.005632564   0.3978232 #> 50            0.005632564   0.3978232 #> 51            0.005632564   0.3978232 #> 52            0.005632564   0.3978232 #> 53            0.005632564   0.3978232 #> 54            0.005632564   0.3978232 #> 55            0.005632564   0.3978232 #> 56            0.005632564   0.3978232 #> 57            0.005632564   0.3978232 #> 58            0.005632564   0.3978232 #> 59            0.005632564   0.3978232 #> 60            0.005632564   0.3978232 #> 61            0.005632564   0.3978232 #> 62            0.005632564   0.3978232 #> 63            0.005632564   0.3978232 #> 64            0.005632564   0.3978232 #> 65            0.005632564   0.3978232 #> 66            0.005632564   0.3978232 #> 67            0.005632564   0.3978232 #> 68            0.005632564   0.3978232 #> 69            0.005632564   0.3978232 #> 70            0.005632564   0.3978232 #> 71            0.005632564   0.3978232 #> 72            0.005632564   0.3978232 #> 73            0.005632564   0.3978232 #> 74            0.005632564   0.3978232 #> 75            0.005632564   0.3978232  # Multiple-group hs_model <- ' visual  =~ x1 + x2 + x3 ' fit <- cfa(hs_model,            data = HolzingerSwineford1939,            group = \"school\") get_fs(HolzingerSwineford1939, hs_model, group = \"school\") #> $Pasteur #>        fs_visual fs_visual_se visual_by_fs_visual ev_fs_visual  school #> 1   -0.821165191    0.3391326           0.6734826    0.1150109 Pasteur #> 2   -0.124009418    0.3391326           0.6734826    0.1150109 Pasteur #> 3   -0.370072089    0.3391326           0.6734826    0.1150109 Pasteur #> 4    0.440928618    0.3391326           0.6734826    0.1150109 Pasteur #> 5   -0.691389016    0.3391326           0.6734826    0.1150109 Pasteur #> 6   -0.110032619    0.3391326           0.6734826    0.1150109 Pasteur #> 7   -0.904127845    0.3391326           0.6734826    0.1150109 Pasteur #> 8   -0.031747573    0.3391326           0.6734826    0.1150109 Pasteur #> 9   -0.439478981    0.3391326           0.6734826    0.1150109 Pasteur #> 10  -0.938939050    0.3391326           0.6734826    0.1150109 Pasteur #> 11  -0.436821880    0.3391326           0.6734826    0.1150109 Pasteur #> 12   0.305033497    0.3391326           0.6734826    0.1150109 Pasteur #> 13   0.522076263    0.3391326           0.6734826    0.1150109 Pasteur #> 14  -0.090367931    0.3391326           0.6734826    0.1150109 Pasteur #> 15   0.526276771    0.3391326           0.6734826    0.1150109 Pasteur #> 16  -0.226580678    0.3391326           0.6734826    0.1150109 Pasteur #> 17  -0.582016192    0.3391326           0.6734826    0.1150109 Pasteur #> 18   0.017040431    0.3391326           0.6734826    0.1150109 Pasteur #> 19   0.563052459    0.3391326           0.6734826    0.1150109 Pasteur #> 20   0.746621910    0.3391326           0.6734826    0.1150109 Pasteur #> 21   0.234672405    0.3391326           0.6734826    0.1150109 Pasteur #> 22   1.157487518    0.3391326           0.6734826    0.1150109 Pasteur #> 23  -0.162272449    0.3391326           0.6734826    0.1150109 Pasteur #> 24  -0.556027059    0.3391326           0.6734826    0.1150109 Pasteur #> 25  -0.321443540    0.3391326           0.6734826    0.1150109 Pasteur #> 26   0.153141050    0.3391326           0.6734826    0.1150109 Pasteur #> 27   0.696234416    0.3391326           0.6734826    0.1150109 Pasteur #> 28  -0.020961039    0.3391326           0.6734826    0.1150109 Pasteur #> 29   0.532601236    0.3391326           0.6734826    0.1150109 Pasteur #> 30  -0.727687585    0.3391326           0.6734826    0.1150109 Pasteur #> 31  -0.676719580    0.3391326           0.6734826    0.1150109 Pasteur #> 32  -1.120216393    0.3391326           0.6734826    0.1150109 Pasteur #> 33  -0.313631732    0.3391326           0.6734826    0.1150109 Pasteur #> 34  -0.187091845    0.3391326           0.6734826    0.1150109 Pasteur #> 35  -0.887709484    0.3391326           0.6734826    0.1150109 Pasteur #> 36  -0.760795908    0.3391326           0.6734826    0.1150109 Pasteur #> 37   0.556943532    0.3391326           0.6734826    0.1150109 Pasteur #> 38  -0.458666570    0.3391326           0.6734826    0.1150109 Pasteur #> 39   0.514741536    0.3391326           0.6734826    0.1150109 Pasteur #> 40   0.373009089    0.3391326           0.6734826    0.1150109 Pasteur #> 41  -0.528550562    0.3391326           0.6734826    0.1150109 Pasteur #> 42  -0.865864795    0.3391326           0.6734826    0.1150109 Pasteur #> 43  -1.182344640    0.3391326           0.6734826    0.1150109 Pasteur #> 44  -0.435334517    0.3391326           0.6734826    0.1150109 Pasteur #> 45   0.306520860    0.3391326           0.6734826    0.1150109 Pasteur #> 46   0.821604565    0.3391326           0.6734826    0.1150109 Pasteur #> 47   1.213927875    0.3391326           0.6734826    0.1150109 Pasteur #> 48  -0.851887996    0.3391326           0.6734826    0.1150109 Pasteur #> 49  -0.085053749    0.3391326           0.6734826    0.1150109 Pasteur #> 50  -0.508885873    0.3391326           0.6734826    0.1150109 Pasteur #> 51   0.502467638    0.3391326           0.6734826    0.1150109 Pasteur #> 52   0.284732253    0.3391326           0.6734826    0.1150109 Pasteur #> 53   0.202677755    0.3391326           0.6734826    0.1150109 Pasteur #> 54  -0.335953502    0.3391326           0.6734826    0.1150109 Pasteur #> 55   0.556410369    0.3391326           0.6734826    0.1150109 Pasteur #> 56  -0.058746970    0.3391326           0.6734826    0.1150109 Pasteur #> 57  -0.066932487    0.3391326           0.6734826    0.1150109 Pasteur #> 58   0.554230368    0.3391326           0.6734826    0.1150109 Pasteur #> 59  -0.321761185    0.3391326           0.6734826    0.1150109 Pasteur #> 60  -0.421834819    0.3391326           0.6734826    0.1150109 Pasteur #> 61   0.345476529    0.3391326           0.6734826    0.1150109 Pasteur #> 62   0.194809883    0.3391326           0.6734826    0.1150109 Pasteur #> 63  -0.207870208    0.3391326           0.6734826    0.1150109 Pasteur #> 64  -0.441658981    0.3391326           0.6734826    0.1150109 Pasteur #> 65   0.102070958    0.3391326           0.6734826    0.1150109 Pasteur #> 66   0.311198487    0.3391326           0.6734826    0.1150109 Pasteur #> 67   0.676364229    0.3391326           0.6734826    0.1150109 Pasteur #> 68   0.297858262    0.3391326           0.6734826    0.1150109 Pasteur #> 69  -1.055487128    0.3391326           0.6734826    0.1150109 Pasteur #> 70  -0.737997019    0.3391326           0.6734826    0.1150109 Pasteur #> 71  -1.576099236    0.3391326           0.6734826    0.1150109 Pasteur #> 72   0.534360181    0.3391326           0.6734826    0.1150109 Pasteur #> 73  -0.105888156    0.3391326           0.6734826    0.1150109 Pasteur #> 74   0.266237302    0.3391326           0.6734826    0.1150109 Pasteur #> 75  -0.352427927    0.3391326           0.6734826    0.1150109 Pasteur #> 76  -0.334783784    0.3391326           0.6734826    0.1150109 Pasteur #> 77   0.133588508    0.3391326           0.6734826    0.1150109 Pasteur #> 78  -1.035662965    0.3391326           0.6734826    0.1150109 Pasteur #> 79   0.762507108    0.3391326           0.6734826    0.1150109 Pasteur #> 80  -0.260699265    0.3391326           0.6734826    0.1150109 Pasteur #> 81  -0.329095893    0.3391326           0.6734826    0.1150109 Pasteur #> 82   0.752413211    0.3391326           0.6734826    0.1150109 Pasteur #> 83   0.149268188    0.3391326           0.6734826    0.1150109 Pasteur #> 84  -0.208880471    0.3391326           0.6734826    0.1150109 Pasteur #> 85  -1.078285998    0.3391326           0.6734826    0.1150109 Pasteur #> 86   0.306043760    0.3391326           0.6734826    0.1150109 Pasteur #> 87   0.349677056    0.3391326           0.6734826    0.1150109 Pasteur #> 88   0.165686549    0.3391326           0.6734826    0.1150109 Pasteur #> 89   0.077307606    0.3391326           0.6734826    0.1150109 Pasteur #> 90  -0.077401396    0.3391326           0.6734826    0.1150109 Pasteur #> 91  -0.081863485    0.3391326           0.6734826    0.1150109 Pasteur #> 92   0.106748566    0.3391326           0.6734826    0.1150109 Pasteur #> 93  -0.211593616    0.3391326           0.6734826    0.1150109 Pasteur #> 94  -0.926665153    0.3391326           0.6734826    0.1150109 Pasteur #> 95  -0.739484382    0.3391326           0.6734826    0.1150109 Pasteur #> 96   0.570387167    0.3391326           0.6734826    0.1150109 Pasteur #> 97  -0.913642554    0.3391326           0.6734826    0.1150109 Pasteur #> 98   0.547484887    0.3391326           0.6734826    0.1150109 Pasteur #> 99  -0.602850599    0.3391326           0.6734826    0.1150109 Pasteur #> 100  0.225794270    0.3391326           0.6734826    0.1150109 Pasteur #> 101  0.620447015    0.3391326           0.6734826    0.1150109 Pasteur #> 102  0.158885005    0.3391326           0.6734826    0.1150109 Pasteur #> 103 -0.127938344    0.3391326           0.6734826    0.1150109 Pasteur #> 104 -0.420347455    0.3391326           0.6734826    0.1150109 Pasteur #> 105  1.327978307    0.3391326           0.6734826    0.1150109 Pasteur #> 106  0.181843348    0.3391326           0.6734826    0.1150109 Pasteur #> 107 -0.148932224    0.3391326           0.6734826    0.1150109 Pasteur #> 108  0.612373626    0.3391326           0.6734826    0.1150109 Pasteur #> 109 -0.066558798    0.3391326           0.6734826    0.1150109 Pasteur #> 110 -0.420880619    0.3391326           0.6734826    0.1150109 Pasteur #> 111  1.127036295    0.3391326           0.6734826    0.1150109 Pasteur #> 112  0.237591068    0.3391326           0.6734826    0.1150109 Pasteur #> 113  0.853758689    0.3391326           0.6734826    0.1150109 Pasteur #> 114 -0.143618023    0.3391326           0.6734826    0.1150109 Pasteur #> 115  0.475206679    0.3391326           0.6734826    0.1150109 Pasteur #> 116 -0.670554590    0.3391326           0.6734826    0.1150109 Pasteur #> 117  0.022672257    0.3391326           0.6734826    0.1150109 Pasteur #> 118  0.302002707    0.3391326           0.6734826    0.1150109 Pasteur #> 119  0.151392125    0.3391326           0.6734826    0.1150109 Pasteur #> 120 -0.475300449    0.3391326           0.6734826    0.1150109 Pasteur #> 121 -0.346740056    0.3391326           0.6734826    0.1150109 Pasteur #> 122 -0.078888759    0.3391326           0.6734826    0.1150109 Pasteur #> 123  1.197237913    0.3391326           0.6734826    0.1150109 Pasteur #> 124  0.539243306    0.3391326           0.6734826    0.1150109 Pasteur #> 125  0.867258388    0.3391326           0.6734826    0.1150109 Pasteur #> 126  0.592287901    0.3391326           0.6734826    0.1150109 Pasteur #> 127 -0.500540901    0.3391326           0.6734826    0.1150109 Pasteur #> 128 -0.361193954    0.3391326           0.6734826    0.1150109 Pasteur #> 129  0.626883588    0.3391326           0.6734826    0.1150109 Pasteur #> 130 -0.437514518    0.3391326           0.6734826    0.1150109 Pasteur #> 131  0.695972854    0.3391326           0.6734826    0.1150109 Pasteur #> 132  0.424715775    0.3391326           0.6734826    0.1150109 Pasteur #> 133 -0.203725744    0.3391326           0.6734826    0.1150109 Pasteur #> 134 -0.441499507    0.3391326           0.6734826    0.1150109 Pasteur #> 135  0.735619838    0.3391326           0.6734826    0.1150109 Pasteur #> 136  0.783874697    0.3391326           0.6734826    0.1150109 Pasteur #> 137  0.565709540    0.3391326           0.6734826    0.1150109 Pasteur #> 138  0.258425494    0.3391326           0.6734826    0.1150109 Pasteur #> 139  0.861093397    0.3391326           0.6734826    0.1150109 Pasteur #> 140 -0.059757233    0.3391326           0.6734826    0.1150109 Pasteur #> 141 -0.920340689    0.3391326           0.6734826    0.1150109 Pasteur #> 142  0.845629236    0.3391326           0.6734826    0.1150109 Pasteur #> 143  1.227427574    0.3391326           0.6734826    0.1150109 Pasteur #> 144  1.054223601    0.3391326           0.6734826    0.1150109 Pasteur #> 145 -1.246596805    0.3391326           0.6734826    0.1150109 Pasteur #> 146 -0.473120468    0.3391326           0.6734826    0.1150109 Pasteur #> 147 -0.560171503    0.3391326           0.6734826    0.1150109 Pasteur #> 148 -0.365394462    0.3391326           0.6734826    0.1150109 Pasteur #> 149  0.084744422    0.3391326           0.6734826    0.1150109 Pasteur #> 150  0.910676146    0.3391326           0.6734826    0.1150109 Pasteur #> 151  1.094189533    0.3391326           0.6734826    0.1150109 Pasteur #> 152 -0.013149231    0.3391326           0.6734826    0.1150109 Pasteur #> 153 -0.166472976    0.3391326           0.6734826    0.1150109 Pasteur #> 154  0.008695459    0.3391326           0.6734826    0.1150109 Pasteur #> 155 -0.094989494    0.3391326           0.6734826    0.1150109 Pasteur #> 156 -0.457123143    0.3391326           0.6734826    0.1150109 Pasteur #>  #> $`Grant-White` #>        fs_visual fs_visual_se visual_by_fs_visual ev_fs_visual      school #> 1   -0.915287109     0.311828           0.6990509   0.09723667 Grant-White #> 2    0.035963597     0.311828           0.6990509   0.09723667 Grant-White #> 3    0.355636604     0.311828           0.6990509   0.09723667 Grant-White #> 4   -0.387353871     0.311828           0.6990509   0.09723667 Grant-White #> 5   -0.622393942     0.311828           0.6990509   0.09723667 Grant-White #> 6    0.195944561     0.311828           0.6990509   0.09723667 Grant-White #> 7    1.353023831     0.311828           0.6990509   0.09723667 Grant-White #> 8   -0.341506254     0.311828           0.6990509   0.09723667 Grant-White #> 9   -0.199493575     0.311828           0.6990509   0.09723667 Grant-White #> 10  -0.689869149     0.311828           0.6990509   0.09723667 Grant-White #> 11  -0.463929554     0.311828           0.6990509   0.09723667 Grant-White #> 12  -0.423001505     0.311828           0.6990509   0.09723667 Grant-White #> 13   0.279743296     0.311828           0.6990509   0.09723667 Grant-White #> 14  -0.916908219     0.311828           0.6990509   0.09723667 Grant-White #> 15   0.589344501     0.311828           0.6990509   0.09723667 Grant-White #> 16   0.191474701     0.311828           0.6990509   0.09723667 Grant-White #> 17   0.935275715     0.311828           0.6990509   0.09723667 Grant-White #> 18   0.393715904     0.311828           0.6990509   0.09723667 Grant-White #> 19   0.086569994     0.311828           0.6990509   0.09723667 Grant-White #> 20   0.555606898     0.311828           0.6990509   0.09723667 Grant-White #> 21  -0.558217193     0.311828           0.6990509   0.09723667 Grant-White #> 22   0.766715894     0.311828           0.6990509   0.09723667 Grant-White #> 23   0.115548801     0.311828           0.6990509   0.09723667 Grant-White #> 24   0.901249191     0.311828           0.6990509   0.09723667 Grant-White #> 25   0.174316971     0.311828           0.6990509   0.09723667 Grant-White #> 26  -0.078980322     0.311828           0.6990509   0.09723667 Grant-White #> 27  -0.581882977     0.311828           0.6990509   0.09723667 Grant-White #> 28  -0.661179262     0.311828           0.6990509   0.09723667 Grant-White #> 29  -0.245341176     0.311828           0.6990509   0.09723667 Grant-White #> 30  -0.195801662     0.311828           0.6990509   0.09723667 Grant-White #> 31  -0.281221524     0.311828           0.6990509   0.09723667 Grant-White #> 32  -0.293909378     0.311828           0.6990509   0.09723667 Grant-White #> 33  -0.604192958     0.311828           0.6990509   0.09723667 Grant-White #> 34  -0.738437335     0.311828           0.6990509   0.09723667 Grant-White #> 35  -0.304109345     0.311828           0.6990509   0.09723667 Grant-White #> 36   0.104931733     0.311828           0.6990509   0.09723667 Grant-White #> 37  -0.025781487     0.311828           0.6990509   0.09723667 Grant-White #> 38  -0.897318824     0.311828           0.6990509   0.09723667 Grant-White #> 39  -0.892560027     0.311828           0.6990509   0.09723667 Grant-White #> 40  -0.078402465     0.311828           0.6990509   0.09723667 Grant-White #> 41  -0.379063934     0.311828           0.6990509   0.09723667 Grant-White #> 42  -0.324926380     0.311828           0.6990509   0.09723667 Grant-White #> 43  -0.684299797     0.311828           0.6990509   0.09723667 Grant-White #> 44  -0.304109345     0.311828           0.6990509   0.09723667 Grant-White #> 45   0.622793169     0.311828           0.6990509   0.09723667 Grant-White #> 46  -0.152835419     0.311828           0.6990509   0.09723667 Grant-White #> 47  -0.421902013     0.311828           0.6990509   0.09723667 Grant-White #> 48  -0.060883872     0.311828           0.6990509   0.09723667 Grant-White #> 49  -0.303298790     0.311828           0.6990509   0.09723667 Grant-White #> 50   0.425021826     0.311828           0.6990509   0.09723667 Grant-White #> 51   0.131478875     0.311828           0.6990509   0.09723667 Grant-White #> 52  -0.914998172     0.311828           0.6990509   0.09723667 Grant-White #> 53   0.324226132     0.311828           0.6990509   0.09723667 Grant-White #> 54  -0.086170767     0.311828           0.6990509   0.09723667 Grant-White #> 55   0.428424818     0.311828           0.6990509   0.09723667 Grant-White #> 56   0.188465179     0.311828           0.6990509   0.09723667 Grant-White #> 57  -0.306958111     0.311828           0.6990509   0.09723667 Grant-White #> 58   0.581736955     0.311828           0.6990509   0.09723667 Grant-White #> 59  -0.743485051     0.311828           0.6990509   0.09723667 Grant-White #> 60   0.263580524     0.311828           0.6990509   0.09723667 Grant-White #> 61   0.178786831     0.311828           0.6990509   0.09723667 Grant-White #> 62  -0.064832113     0.311828           0.6990509   0.09723667 Grant-White #> 63   0.499976414     0.311828           0.6990509   0.09723667 Grant-White #> 64  -0.092839593     0.311828           0.6990509   0.09723667 Grant-White #> 65  -0.263702931     0.311828           0.6990509   0.09723667 Grant-White #> 66  -0.983966325     0.311828           0.6990509   0.09723667 Grant-White #> 67   1.434912536     0.311828           0.6990509   0.09723667 Grant-White #> 68  -1.037582228     0.311828           0.6990509   0.09723667 Grant-White #> 69  -0.047569849     0.311828           0.6990509   0.09723667 Grant-White #> 70   1.084767775     0.311828           0.6990509   0.09723667 Grant-White #> 71   0.092011181     0.311828           0.6990509   0.09723667 Grant-White #> 72  -0.562687052     0.311828           0.6990509   0.09723667 Grant-White #> 73  -0.304919900     0.311828           0.6990509   0.09723667 Grant-White #> 74   1.038920175     0.311828           0.6990509   0.09723667 Grant-White #> 75  -0.789854287     0.311828           0.6990509   0.09723667 Grant-White #> 76  -0.602282928     0.311828           0.6990509   0.09723667 Grant-White #> 77  -0.894630830     0.311828           0.6990509   0.09723667 Grant-White #> 78   0.532614527     0.311828           0.6990509   0.09723667 Grant-White #> 79  -0.548955945     0.311828           0.6990509   0.09723667 Grant-White #> 80  -0.221514635     0.311828           0.6990509   0.09723667 Grant-White #> 81  -0.095849115     0.311828           0.6990509   0.09723667 Grant-White #> 82  -0.122235502     0.311828           0.6990509   0.09723667 Grant-White #> 83   0.892276864     0.311828           0.6990509   0.09723667 Grant-White #> 84   0.328439663     0.311828           0.6990509   0.09723667 Grant-White #> 85   1.217391042     0.311828           0.6990509   0.09723667 Grant-White #> 86   0.574513902     0.311828           0.6990509   0.09723667 Grant-White #> 87   0.160168762     0.311828           0.6990509   0.09723667 Grant-White #> 88   0.654909662     0.311828           0.6990509   0.09723667 Grant-White #> 89  -0.509777155     0.311828           0.6990509   0.09723667 Grant-White #> 90   1.201493560     0.311828           0.6990509   0.09723667 Grant-White #> 91   0.584874625     0.311828           0.6990509   0.09723667 Grant-White #> 92   0.075142371     0.311828           0.6990509   0.09723667 Grant-White #> 93   0.550976266     0.311828           0.6990509   0.09723667 Grant-White #> 94  -0.886308302     0.311828           0.6990509   0.09723667 Grant-White #> 95   0.552075757     0.311828           0.6990509   0.09723667 Grant-White #> 96   1.415972940     0.311828           0.6990509   0.09723667 Grant-White #> 97   0.298650301     0.311828           0.6990509   0.09723667 Grant-White #> 98  -0.143028906     0.311828           0.6990509   0.09723667 Grant-White #> 99   0.245195154     0.311828           0.6990509   0.09723667 Grant-White #> 100  0.247072593     0.311828           0.6990509   0.09723667 Grant-White #> 101  0.817322291     0.311828           0.6990509   0.09723667 Grant-White #> 102  0.651771976     0.311828           0.6990509   0.09723667 Grant-White #> 103  1.338875623     0.311828           0.6990509   0.09723667 Grant-White #> 104 -1.160005528     0.311828           0.6990509   0.09723667 Grant-White #> 105  0.163306449     0.311828           0.6990509   0.09723667 Grant-White #> 106 -0.387353871     0.311828           0.6990509   0.09723667 Grant-White #> 107 -0.517128372     0.311828           0.6990509   0.09723667 Grant-White #> 108  0.065103160     0.311828           0.6990509   0.09723667 Grant-White #> 109 -0.115438510     0.311828           0.6990509   0.09723667 Grant-White #> 110  0.094049376     0.311828           0.6990509   0.09723667 Grant-White #> 111  0.396725409     0.311828           0.6990509   0.09723667 Grant-White #> 112  0.672356312     0.311828           0.6990509   0.09723667 Grant-White #> 113  1.165974090     0.311828           0.6990509   0.09723667 Grant-White #> 114 -0.483518949     0.311828           0.6990509   0.09723667 Grant-White #> 115  0.035024877     0.311828           0.6990509   0.09723667 Grant-White #> 116  0.741974248     0.311828           0.6990509   0.09723667 Grant-White #> 117 -0.170386603     0.311828           0.6990509   0.09723667 Grant-White #> 118 -0.205873481     0.311828           0.6990509   0.09723667 Grant-White #> 119  0.714777307     0.311828           0.6990509   0.09723667 Grant-White #> 120 -0.620772831     0.311828           0.6990509   0.09723667 Grant-White #> 121 -0.313626938     0.311828           0.6990509   0.09723667 Grant-White #> 122 -0.157466035     0.311828           0.6990509   0.09723667 Grant-White #> 123  0.118269386     0.311828           0.6990509   0.09723667 Grant-White #> 124  0.101111673     0.311828           0.6990509   0.09723667 Grant-White #> 125 -0.625403463     0.311828           0.6990509   0.09723667 Grant-White #> 126  0.486638761     0.311828           0.6990509   0.09723667 Grant-White #> 127 -0.178676540     0.311828           0.6990509   0.09723667 Grant-White #> 128  0.274013189     0.311828           0.6990509   0.09723667 Grant-White #> 129 -0.316347523     0.311828           0.6990509   0.09723667 Grant-White #> 130 -0.026752814     0.311828           0.6990509   0.09723667 Grant-White #> 131  0.245323318     0.311828           0.6990509   0.09723667 Grant-White #> 132 -0.356336853     0.311828           0.6990509   0.09723667 Grant-White #> 133 -0.581594057     0.311828           0.6990509   0.09723667 Grant-White #> 134  0.263002667     0.311828           0.6990509   0.09723667 Grant-White #> 135 -0.864680712     0.311828           0.6990509   0.09723667 Grant-White #> 136 -0.377964443     0.311828           0.6990509   0.09723667 Grant-White #> 137 -0.112717909     0.311828           0.6990509   0.09723667 Grant-White #> 138  0.114449326     0.311828           0.6990509   0.09723667 Grant-White #> 139  0.001287274     0.311828           0.6990509   0.09723667 Grant-White #> 140  0.597634438     0.311828           0.6990509   0.09723667 Grant-White #> 141 -0.252531637     0.311828           0.6990509   0.09723667 Grant-White #> 142 -0.472901881     0.311828           0.6990509   0.09723667 Grant-White #> 143 -0.187255397     0.311828           0.6990509   0.09723667 Grant-White #> 144 -0.542415283     0.311828           0.6990509   0.09723667 Grant-White #> 145  0.358774274     0.311828           0.6990509   0.09723667 Grant-White #>  #> attr(,\"fsT\") #> attr(,\"fsT\")$Pasteur #>           fs_visual #> fs_visual 0.1150109 #>  #> attr(,\"fsT\")$`Grant-White` #>            fs_visual #> fs_visual 0.09723667 #>  #> attr(,\"fsL\") #> attr(,\"fsL\")$Pasteur #>              visual #> fs_visual 0.6734826 #>  #> attr(,\"fsL\")$`Grant-White` #>              visual #> fs_visual 0.6990509 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #> fs_visual  #>         0  #>  #> attr(,\"fsb\")$`Grant-White` #> fs_visual  #>         0  #>  #> attr(,\"scoring_matrix\") #> attr(,\"scoring_matrix\")$Pasteur #>             [,1]     [,2]      [,3] #> visual 0.1957873 0.109906 0.3316264 #>  #> attr(,\"scoring_matrix\")$`Grant-White` #>             [,1]      [,2]      [,3] #> visual 0.1624126 0.1458328 0.3515006 #>  # Or without the model get_fs(HolzingerSwineford1939[c(\"school\", \"x4\", \"x5\", \"x6\")],        group = \"school\") #> $Pasteur #>             fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1  school #> 1    0.3074500370 0.2999315   0.8833584 0.08995892 Pasteur #> 2   -0.7746062892 0.2999315   0.8833584 0.08995892 Pasteur #> 3   -1.5843019574 0.2999315   0.8833584 0.08995892 Pasteur #> 4    0.2739579120 0.2999315   0.8833584 0.08995892 Pasteur #> 5    0.1440153923 0.2999315   0.8833584 0.08995892 Pasteur #> 6   -1.0440895948 0.2999315   0.8833584 0.08995892 Pasteur #> 7    1.0507357396 0.2999315   0.8833584 0.08995892 Pasteur #> 8    0.1041882698 0.2999315   0.8833584 0.08995892 Pasteur #> 9    0.7750146375 0.2999315   0.8833584 0.08995892 Pasteur #> 10   0.4822117444 0.2999315   0.8833584 0.08995892 Pasteur #> 11  -0.4511886490 0.2999315   0.8833584 0.08995892 Pasteur #> 12   0.3522691973 0.2999315   0.8833584 0.08995892 Pasteur #> 13   0.0657041070 0.2999315   0.8833584 0.08995892 Pasteur #> 14   0.3259750264 0.2999315   0.8833584 0.08995892 Pasteur #> 15   1.3008341323 0.2999315   0.8833584 0.08995892 Pasteur #> 16  -0.2804588573 0.2999315   0.8833584 0.08995892 Pasteur #> 17  -0.3604017581 0.2999315   0.8833584 0.08995892 Pasteur #> 18  -0.7502293722 0.2999315   0.8833584 0.08995892 Pasteur #> 19   1.2600468631 0.2999315   0.8833584 0.08995892 Pasteur #> 20   0.2874908636 0.2999315   0.8833584 0.08995892 Pasteur #> 21  -1.1394826729 0.2999315   0.8833584 0.08995892 Pasteur #> 22  -0.3791151940 0.2999315   0.8833584 0.08995892 Pasteur #> 23   1.0444979094 0.2999315   0.8833584 0.08995892 Pasteur #> 24  -0.6248930150 0.2999315   0.8833584 0.08995892 Pasteur #> 25  -0.3689426673 0.2999315   0.8833584 0.08995892 Pasteur #> 26  -0.5663524842 0.2999315   0.8833584 0.08995892 Pasteur #> 27  -0.9568515617 0.2999315   0.8833584 0.08995892 Pasteur #> 28  -0.8137619455 0.2999315   0.8833584 0.08995892 Pasteur #> 29  -0.5028199109 0.2999315   0.8833584 0.08995892 Pasteur #> 30  -0.3054100713 0.2999315   0.8833584 0.08995892 Pasteur #> 31  -0.3728773637 0.2999315   0.8833584 0.08995892 Pasteur #> 32  -0.8529175744 0.2999315   0.8833584 0.08995892 Pasteur #> 33   0.4101382620 0.2999315   0.8833584 0.08995892 Pasteur #> 34   0.1848026386 0.2999315   0.8833584 0.08995892 Pasteur #> 35  -0.9680814159 0.2999315   0.8833584 0.08995892 Pasteur #> 36  -0.1436070439 0.2999315   0.8833584 0.08995892 Pasteur #> 37  -0.0126071782 0.2999315   0.8833584 0.08995892 Pasteur #> 38  -0.3531066368 0.2999315   0.8833584 0.08995892 Pasteur #> 39   1.0665717701 0.2999315   0.8833584 0.08995892 Pasteur #> 40   0.7993915544 0.2999315   0.8833584 0.08995892 Pasteur #> 41   0.1110975387 0.2999315   0.8833584 0.08995892 Pasteur #> 42  -0.2735495637 0.2999315   0.8833584 0.08995892 Pasteur #> 43  -0.7167372327 0.2999315   0.8833584 0.08995892 Pasteur #> 44  -0.6594424814 0.2999315   0.8833584 0.08995892 Pasteur #> 45   0.9507364688 0.2999315   0.8833584 0.08995892 Pasteur #> 46  -0.0478281107 0.2999315   0.8833584 0.08995892 Pasteur #> 47  -1.7573348450 0.2999315   0.8833584 0.08995892 Pasteur #> 48  -0.4620326417 0.2999315   0.8833584 0.08995892 Pasteur #> 49  -1.0305566597 0.2999315   0.8833584 0.08995892 Pasteur #> 50   0.7618675383 0.2999315   0.8833584 0.08995892 Pasteur #> 51  -1.3414986833 0.2999315   0.8833584 0.08995892 Pasteur #> 52  -0.0761397743 0.2999315   0.8833584 0.08995892 Pasteur #> 53  -1.8231705136 0.2999315   0.8833584 0.08995892 Pasteur #> 54   0.0094666323 0.2999315   0.8833584 0.08995892 Pasteur #> 55   0.0436302463 0.2999315   0.8833584 0.08995892 Pasteur #> 56  -0.0001315726 0.2999315   0.8833584 0.08995892 Pasteur #> 57  -0.6571393750 0.2999315   0.8833584 0.08995892 Pasteur #> 58   0.6121542642 0.2999315   0.8833584 0.08995892 Pasteur #> 59  -1.0957208458 0.2999315   0.8833584 0.08995892 Pasteur #> 60  -0.9289257761 0.2999315   0.8833584 0.08995892 Pasteur #> 61   0.9553426588 0.2999315   0.8833584 0.08995892 Pasteur #> 62   0.3647448029 0.2999315   0.8833584 0.08995892 Pasteur #> 63  -1.1003270330 0.2999315   0.8833584 0.08995892 Pasteur #> 64   0.4259742697 0.2999315   0.8833584 0.08995892 Pasteur #> 65   0.1689666035 0.2999315   0.8833584 0.08995892 Pasteur #> 66   0.6586050419 0.2999315   0.8833584 0.08995892 Pasteur #> 67  -0.2952375720 0.2999315   0.8833584 0.08995892 Pasteur #> 68  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 69  -0.5613604418 0.2999315   0.8833584 0.08995892 Pasteur #> 70   0.5632119383 0.2999315   0.8833584 0.08995892 Pasteur #> 71  -0.7769093955 0.2999315   0.8833584 0.08995892 Pasteur #> 72  -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 73  -0.7808440920 0.2999315   0.8833584 0.08995892 Pasteur #> 74   0.9270309952 0.2999315   0.8833584 0.08995892 Pasteur #> 75  -0.5426470306 0.2999315   0.8833584 0.08995892 Pasteur #> 76  -1.1065648385 0.2999315   0.8833584 0.08995892 Pasteur #> 77   0.6233841094 0.2999315   0.8833584 0.08995892 Pasteur #> 78  -1.7010974136 0.2999315   0.8833584 0.08995892 Pasteur #> 79  -0.0013773330 0.2999315   0.8833584 0.08995892 Pasteur #> 80  -1.2772946275 0.2999315   0.8833584 0.08995892 Pasteur #> 81  -1.0344913561 0.2999315   0.8833584 0.08995892 Pasteur #> 82   0.4345151789 0.2999315   0.8833584 0.08995892 Pasteur #> 83  -1.5280645151 0.2999315   0.8833584 0.08995892 Pasteur #> 84  -0.6101143231 0.2999315   0.8833584 0.08995892 Pasteur #> 85  -1.5122284832 0.2999315   0.8833584 0.08995892 Pasteur #> 86   1.2011204798 0.2999315   0.8833584 0.08995892 Pasteur #> 87  -0.3258523027 0.2999315   0.8833584 0.08995892 Pasteur #> 88   0.6687775411 0.2999315   0.8833584 0.08995892 Pasteur #> 89  -1.6382363147 0.2999315   0.8833584 0.08995892 Pasteur #> 90   0.3062042720 0.2999315   0.8833584 0.08995892 Pasteur #> 91  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 92  -0.6798846937 0.2999315   0.8833584 0.08995892 Pasteur #> 93  -1.1865077366 0.2999315   0.8833584 0.08995892 Pasteur #> 94  -0.5011882981 0.2999315   0.8833584 0.08995892 Pasteur #> 95   0.7362448336 0.2999315   0.8833584 0.08995892 Pasteur #> 96   0.4934415622 0.2999315   0.8833584 0.08995892 Pasteur #> 97   0.9661866515 0.2999315   0.8833584 0.08995892 Pasteur #> 98   1.7212764661 0.2999315   0.8833584 0.08995892 Pasteur #> 99  -0.8199997483 0.2999315   0.8833584 0.08995892 Pasteur #> 100  0.7369163271 0.2999315   0.8833584 0.08995892 Pasteur #> 101 -0.5403439270 0.2999315   0.8833584 0.08995892 Pasteur #> 102  0.0525570079 0.2999315   0.8833584 0.08995892 Pasteur #> 103  0.4973762860 0.2999315   0.8833584 0.08995892 Pasteur #> 104  0.5434412113 0.2999315   0.8833584 0.08995892 Pasteur #> 105  1.4015048920 0.2999315   0.8833584 0.08995892 Pasteur #> 106  0.5338429790 0.2999315   0.8833584 0.08995892 Pasteur #> 107  1.5005470281 0.2999315   0.8833584 0.08995892 Pasteur #> 108 -0.4353526184 0.2999315   0.8833584 0.08995892 Pasteur #> 109  1.7269399974 0.2999315   0.8833584 0.08995892 Pasteur #> 110 -0.1863115671 0.2999315   0.8833584 0.08995892 Pasteur #> 111  0.7431541299 0.2999315   0.8833584 0.08995892 Pasteur #> 112  0.3345159128 0.2999315   0.8833584 0.08995892 Pasteur #> 113  0.3111963144 0.2999315   0.8833584 0.08995892 Pasteur #> 114  0.6750153713 0.2999315   0.8833584 0.08995892 Pasteur #> 115 -1.3822859470 0.2999315   0.8833584 0.08995892 Pasteur #> 116  0.4299090164 0.2999315   0.8833584 0.08995892 Pasteur #> 117  0.4368182853 0.2999315   0.8833584 0.08995892 Pasteur #> 118 -0.6334339242 0.2999315   0.8833584 0.08995892 Pasteur #> 119 -0.9153928519 0.2999315   0.8833584 0.08995892 Pasteur #> 120 -0.2662544424 0.2999315   0.8833584 0.08995892 Pasteur #> 121 -0.1238362896 0.2999315   0.8833584 0.08995892 Pasteur #> 122 -0.1987871727 0.2999315   0.8833584 0.08995892 Pasteur #> 123  1.5676284956 0.2999315   0.8833584 0.08995892 Pasteur #> 124 -0.2906313821 0.2999315   0.8833584 0.08995892 Pasteur #> 125  0.7125393874 0.2999315   0.8833584 0.08995892 Pasteur #> 126  0.1324998831 0.2999315   0.8833584 0.08995892 Pasteur #> 127  1.1488177518 0.2999315   0.8833584 0.08995892 Pasteur #> 128  0.5559168169 0.2999315   0.8833584 0.08995892 Pasteur #> 129  0.8572606191 0.2999315   0.8833584 0.08995892 Pasteur #> 130 -0.9789254060 0.2999315   0.8833584 0.08995892 Pasteur #> 131  1.4416206448 0.2999315   0.8833584 0.08995892 Pasteur #> 132  0.4542859333 0.2999315   0.8833584 0.08995892 Pasteur #> 133 -1.3845890506 0.2999315   0.8833584 0.08995892 Pasteur #> 134 -0.2883282757 0.2999315   0.8833584 0.08995892 Pasteur #> 135  0.4430560881 0.2999315   0.8833584 0.08995892 Pasteur #> 136  1.2089899229 0.2999315   0.8833584 0.08995892 Pasteur #> 137  1.1942112109 0.2999315   0.8833584 0.08995892 Pasteur #> 138  0.6013102486 0.2999315   0.8833584 0.08995892 Pasteur #> 139  0.2371053667 0.2999315   0.8833584 0.08995892 Pasteur #> 140  1.0053422804 0.2999315   0.8833584 0.08995892 Pasteur #> 141  0.8095640810 0.2999315   0.8833584 0.08995892 Pasteur #> 142 -1.4408264861 0.2999315   0.8833584 0.08995892 Pasteur #> 143  1.3821199900 0.2999315   0.8833584 0.08995892 Pasteur #> 144  2.7284791267 0.2999315   0.8833584 0.08995892 Pasteur #> 145  0.0123440494 0.2999315   0.8833584 0.08995892 Pasteur #> 146  0.8277032180 0.2999315   0.8833584 0.08995892 Pasteur #> 147  0.7862444827 0.2999315   0.8833584 0.08995892 Pasteur #> 148 -1.1325734026 0.2999315   0.8833584 0.08995892 Pasteur #> 149  1.7660956264 0.2999315   0.8833584 0.08995892 Pasteur #> 150 -0.3712457509 0.2999315   0.8833584 0.08995892 Pasteur #> 151  1.8944065607 0.2999315   0.8833584 0.08995892 Pasteur #> 152  0.6098511578 0.2999315   0.8833584 0.08995892 Pasteur #> 153  0.2654170028 0.2999315   0.8833584 0.08995892 Pasteur #> 154 -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 155 -0.6005160981 0.2999315   0.8833584 0.08995892 Pasteur #> 156  0.3562038937 0.2999315   0.8833584 0.08995892 Pasteur #>  #> $`Grant-White` #>           fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1      school #> 1   -0.39525603 0.3152173   0.8801489 0.09936192 Grant-White #> 2   -0.63397248 0.3152173   0.8801489 0.09936192 Grant-White #> 3    0.20062403 0.3152173   0.8801489 0.09936192 Grant-White #> 4   -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 5    0.43519197 0.3152173   0.8801489 0.09936192 Grant-White #> 6    0.31151246 0.3152173   0.8801489 0.09936192 Grant-White #> 7    2.15612913 0.3152173   0.8801489 0.09936192 Grant-White #> 8   -0.29014192 0.3152173   0.8801489 0.09936192 Grant-White #> 9   -0.08369303 0.3152173   0.8801489 0.09936192 Grant-White #> 10  -0.01807396 0.3152173   0.8801489 0.09936192 Grant-White #> 11  -0.34820234 0.3152173   0.8801489 0.09936192 Grant-White #> 12  -2.10215974 0.3152173   0.8801489 0.09936192 Grant-White #> 13  -0.64552119 0.3152173   0.8801489 0.09936192 Grant-White #> 14  -1.46155228 0.3152173   0.8801489 0.09936192 Grant-White #> 15   1.02620880 0.3152173   0.8801489 0.09936192 Grant-White #> 16  -1.05064956 0.3152173   0.8801489 0.09936192 Grant-White #> 17   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 18   0.95822548 0.3152173   0.8801489 0.09936192 Grant-White #> 19  -0.25355303 0.3152173   0.8801489 0.09936192 Grant-White #> 20   1.42141427 0.3152173   0.8801489 0.09936192 Grant-White #> 21  -0.95400523 0.3152173   0.8801489 0.09936192 Grant-White #> 22   1.00295292 0.3152173   0.8801489 0.09936192 Grant-White #> 23   1.21841354 0.3152173   0.8801489 0.09936192 Grant-White #> 24  -1.24987101 0.3152173   0.8801489 0.09936192 Grant-White #> 25  -0.51984663 0.3152173   0.8801489 0.09936192 Grant-White #> 26  -0.04710419 0.3152173   0.8801489 0.09936192 Grant-White #> 27   0.43934048 0.3152173   0.8801489 0.09936192 Grant-White #> 28  -1.03117301 0.3152173   0.8801489 0.09936192 Grant-White #> 29  -0.95022593 0.3152173   0.8801489 0.09936192 Grant-White #> 30  -0.11417634 0.3152173   0.8801489 0.09936192 Grant-White #> 31  -0.40048841 0.3152173   0.8801489 0.09936192 Grant-White #> 32   0.10506359 0.3152173   0.8801489 0.09936192 Grant-White #> 33  -0.33541128 0.3152173   0.8801489 0.09936192 Grant-White #> 34  -1.55565961 0.3152173   0.8801489 0.09936192 Grant-White #> 35  -0.84420068 0.3152173   0.8801489 0.09936192 Grant-White #> 36   0.07802839 0.3152173   0.8801489 0.09936192 Grant-White #> 37   0.20116597 0.3152173   0.8801489 0.09936192 Grant-White #> 38  -2.52639546 0.3152173   0.8801489 0.09936192 Grant-White #> 39  -0.69149096 0.3152173   0.8801489 0.09936192 Grant-White #> 40   2.02343787 0.3152173   0.8801489 0.09936192 Grant-White #> 41   0.97338078 0.3152173   0.8801489 0.09936192 Grant-White #> 42   0.42040588 0.3152173   0.8801489 0.09936192 Grant-White #> 43  -0.92605892 0.3152173   0.8801489 0.09936192 Grant-White #> 44  -0.56690032 0.3152173   0.8801489 0.09936192 Grant-White #> 45   0.12021889 0.3152173   0.8801489 0.09936192 Grant-White #> 46   0.62108043 0.3152173   0.8801489 0.09936192 Grant-White #> 47  -1.34219405 0.3152173   0.8801489 0.09936192 Grant-White #> 48   0.16258210 0.3152173   0.8801489 0.09936192 Grant-White #> 49  -0.03231810 0.3152173   0.8801489 0.09936192 Grant-White #> 50   0.24444031 0.3152173   0.8801489 0.09936192 Grant-White #> 51  -0.74431900 0.3152173   0.8801489 0.09936192 Grant-White #> 52  -0.43798842 0.3152173   0.8801489 0.09936192 Grant-White #> 53  -1.85297845 0.3152173   0.8801489 0.09936192 Grant-White #> 54  -0.82563527 0.3152173   0.8801489 0.09936192 Grant-White #> 55   1.20039010 0.3152173   0.8801489 0.09936192 Grant-White #> 56  -0.33287433 0.3152173   0.8801489 0.09936192 Grant-White #> 57   0.01996797 0.3152173   0.8801489 0.09936192 Grant-White #> 58   1.67582800 0.3152173   0.8801489 0.09936192 Grant-White #> 59  -0.71906808 0.3152173   0.8801489 0.09936192 Grant-White #> 60  -0.20504632 0.3152173   0.8801489 0.09936192 Grant-White #> 61   1.96214007 0.3152173   0.8801489 0.09936192 Grant-White #> 62  -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 63  -0.35343474 0.3152173   0.8801489 0.09936192 Grant-White #> 64  -1.95809255 0.3152173   0.8801489 0.09936192 Grant-White #> 65  -1.36021750 0.3152173   0.8801489 0.09936192 Grant-White #> 66   0.08595623 0.3152173   0.8801489 0.09936192 Grant-White #> 67  -0.23407652 0.3152173   0.8801489 0.09936192 Grant-White #> 68   0.67805696 0.3152173   0.8801489 0.09936192 Grant-White #> 69  -0.42951863 0.3152173   0.8801489 0.09936192 Grant-White #> 70  -0.69203290 0.3152173   0.8801489 0.09936192 Grant-White #> 71  -0.71583072 0.3152173   0.8801489 0.09936192 Grant-White #> 72  -0.19603459 0.3152173   0.8801489 0.09936192 Grant-White #> 73  -0.36767888 0.3152173   0.8801489 0.09936192 Grant-White #> 74   1.77425660 0.3152173   0.8801489 0.09936192 Grant-White #> 75  -0.67924187 0.3152173   0.8801489 0.09936192 Grant-White #> 76   0.07603337 0.3152173   0.8801489 0.09936192 Grant-White #> 77   1.49895127 0.3152173   0.8801489 0.09936192 Grant-White #> 78  -0.78813525 0.3152173   0.8801489 0.09936192 Grant-White #> 79  -1.35643816 0.3152173   0.8801489 0.09936192 Grant-White #> 80  -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 81   1.10806701 0.3152173   0.8801489 0.09936192 Grant-White #> 82   1.04768034 0.3152173   0.8801489 0.09936192 Grant-White #> 83   1.02242947 0.3152173   0.8801489 0.09936192 Grant-White #> 84   0.38236395 0.3152173   0.8801489 0.09936192 Grant-White #> 85   0.56447306 0.3152173   0.8801489 0.09936192 Grant-White #> 86   0.78803426 0.3152173   0.8801489 0.09936192 Grant-White #> 87   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 88   0.40383552 0.3152173   0.8801489 0.09936192 Grant-White #> 89  -0.44376277 0.3152173   0.8801489 0.09936192 Grant-White #> 90   0.08126577 0.3152173   0.8801489 0.09936192 Grant-White #> 91   0.14833793 0.3152173   0.8801489 0.09936192 Grant-White #> 92   0.53922219 0.3152173   0.8801489 0.09936192 Grant-White #> 93   0.53598481 0.3152173   0.8801489 0.09936192 Grant-White #> 94  -0.17024174 0.3152173   0.8801489 0.09936192 Grant-White #> 95   0.43610310 0.3152173   0.8801489 0.09936192 Grant-White #> 96   2.22843366 0.3152173   0.8801489 0.09936192 Grant-White #> 97   1.36480696 0.3152173   0.8801489 0.09936192 Grant-White #> 98   0.65135298 0.3152173   0.8801489 0.09936192 Grant-White #> 99   1.69439338 0.3152173   0.8801489 0.09936192 Grant-White #> 100 -0.15745068 0.3152173   0.8801489 0.09936192 Grant-White #> 101 -0.27680894 0.3152173   0.8801489 0.09936192 Grant-White #> 102  1.80473991 0.3152173   0.8801489 0.09936192 Grant-White #> 103 -0.03286005 0.3152173   0.8801489 0.09936192 Grant-White #> 104  0.21541008 0.3152173   0.8801489 0.09936192 Grant-White #> 105  0.63586648 0.3152173   0.8801489 0.09936192 Grant-White #> 106 -0.44122580 0.3152173   0.8801489 0.09936192 Grant-White #> 107  0.24389836 0.3152173   0.8801489 0.09936192 Grant-White #> 108  0.97392270 0.3152173   0.8801489 0.09936192 Grant-White #> 109  1.00619031 0.3152173   0.8801489 0.09936192 Grant-White #> 110  0.95967859 0.3152173   0.8801489 0.09936192 Grant-White #> 111  2.10529611 0.3152173   0.8801489 0.09936192 Grant-White #> 112  0.95012491 0.3152173   0.8801489 0.09936192 Grant-White #> 113  1.14033462 0.3152173   0.8801489 0.09936192 Grant-White #> 114 -0.41527450 0.3152173   0.8801489 0.09936192 Grant-White #> 115  0.50263334 0.3152173   0.8801489 0.09936192 Grant-White #> 116  0.00518188 0.3152173   0.8801489 0.09936192 Grant-White #> 117 -0.30961846 0.3152173   0.8801489 0.09936192 Grant-White #> 118 -0.38570232 0.3152173   0.8801489 0.09936192 Grant-White #> 119 -0.88747505 0.3152173   0.8801489 0.09936192 Grant-White #> 120 -1.11340047 0.3152173   0.8801489 0.09936192 Grant-White #> 121 -0.22830216 0.3152173   0.8801489 0.09936192 Grant-White #> 122  0.16781447 0.3152173   0.8801489 0.09936192 Grant-White #> 123 -1.16622845 0.3152173   0.8801489 0.09936192 Grant-White #> 124 -0.45277447 0.3152173   0.8801489 0.09936192 Grant-White #> 125 -0.69527031 0.3152173   0.8801489 0.09936192 Grant-White #> 126  1.16558552 0.3152173   0.8801489 0.09936192 Grant-White #> 127 -0.49081643 0.3152173   0.8801489 0.09936192 Grant-White #> 128  0.45412656 0.3152173   0.8801489 0.09936192 Grant-White #> 129 -0.75910506 0.3152173   0.8801489 0.09936192 Grant-White #> 130 -0.46232819 0.3152173   0.8801489 0.09936192 Grant-White #> 131  1.33631868 0.3152173   0.8801489 0.09936192 Grant-White #> 132 -0.78236094 0.3152173   0.8801489 0.09936192 Grant-White #> 133  0.11407532 0.3152173   0.8801489 0.09936192 Grant-White #> 134 -0.26111172 0.3152173   0.8801489 0.09936192 Grant-White #> 135 -0.58492377 0.3152173   0.8801489 0.09936192 Grant-White #> 136 -1.40872427 0.3152173   0.8801489 0.09936192 Grant-White #> 137 -0.24308825 0.3152173   0.8801489 0.09936192 Grant-White #> 138 -0.17601610 0.3152173   0.8801489 0.09936192 Grant-White #> 139 -0.74486092 0.3152173   0.8801489 0.09936192 Grant-White #> 140 -0.13419481 0.3152173   0.8801489 0.09936192 Grant-White #> 141 -0.74809830 0.3152173   0.8801489 0.09936192 Grant-White #> 142 -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 143  0.88737403 0.3152173   0.8801489 0.09936192 Grant-White #> 144 -0.05665784 0.3152173   0.8801489 0.09936192 Grant-White #> 145  0.58303847 0.3152173   0.8801489 0.09936192 Grant-White #>  #> attr(,\"fsT\") #> attr(,\"fsT\")$Pasteur #>            fs_f1 #> fs_f1 0.08995892 #>  #> attr(,\"fsT\")$`Grant-White` #>            fs_f1 #> fs_f1 0.09936192 #>  #> attr(,\"fsL\") #> attr(,\"fsL\")$Pasteur #>              f1 #> fs_f1 0.8833584 #>  #> attr(,\"fsL\")$`Grant-White` #>              f1 #> fs_f1 0.8801489 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #> fs_f1  #>     0  #>  #> attr(,\"fsb\")$`Grant-White` #> fs_f1  #>     0  #>  #> attr(,\"scoring_matrix\") #> attr(,\"scoring_matrix\")$Pasteur #>         [,1]      [,2]      [,3] #> f1 0.2280246 0.3381964 0.2740895 #>  #> attr(,\"scoring_matrix\")$`Grant-White` #>         [,1]      [,2]      [,3] #> f1 0.3580747 0.2682886 0.2662936 #>"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"Get Factor Scores Corresponding Standard Error Measurement","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"","code":"get_fs_lavaan(   lavobj,   method = c(\"regression\", \"Bartlett\"),   corrected_fsT = FALSE,   vfsLT = FALSE )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"lavobj lavaan model object using get_fs_lavaan(). method Character. Method computing factor scores (options \"regression\" \"Bartlett\"). Currently, default \"regression\" consistent lavPredict, Bartlett scores desirable properties may preferred 2S-PA. corrected_fsT Logical. Whether correct sampling error factor score weights computing error variance estimates factor scores. vfsLT Logical. Whether return covariance matrix fsT fsL, can used input vcov_corrected() obtain corrected covariances standard errors tspa() results. currently ignored.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"data frame containing factor scores (prefix \"fs_\"), standard errors (suffix \"_se\"), implied loadings factor \"_by_\" factor scores, error variance-covariance factor scores (prefix \"evfs_\"). following also returned attributes: * fsT: error covariance factor scores * fsL: loading matrix factor scores * fsb: intercepts factor scores * scoring_matrix: weights computing factor scores items","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"","code":"library(lavaan) get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\")]) #>          fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1 #> 1  -0.52616832 0.1213615   0.9657673 0.01472862 #> 2   0.14365274 0.1213615   0.9657673 0.01472862 #> 3   0.71435592 0.1213615   0.9657673 0.01472862 #> 4   1.23992565 0.1213615   0.9657673 0.01472862 #> 5   0.83190803 0.1213615   0.9657673 0.01472862 #> 6   0.21238453 0.1213615   0.9657673 0.01472862 #> 7   0.11880855 0.1213615   0.9657673 0.01472862 #> 8   0.11322703 0.1213615   0.9657673 0.01472862 #> 9   0.25617279 0.1213615   0.9657673 0.01472862 #> 10  0.37112496 0.1213615   0.9657673 0.01472862 #> 11  0.67281395 0.1213615   0.9657673 0.01472862 #> 12  0.56885577 0.1213615   0.9657673 0.01472862 #> 13  1.31369791 0.1213615   0.9657673 0.01472862 #> 14  0.22042629 0.1213615   0.9657673 0.01472862 #> 15  0.57849228 0.1213615   0.9657673 0.01472862 #> 16  0.37805983 0.1213615   0.9657673 0.01472862 #> 17  0.05734046 0.1213615   0.9657673 0.01472862 #> 18 -0.01609202 0.1213615   0.9657673 0.01472862 #> 19  0.88923616 0.1213615   0.9657673 0.01472862 #> 20  1.11445897 0.1213615   0.9657673 0.01472862 #> 21  0.94657339 0.1213615   0.9657673 0.01472862 #> 22  0.90122770 0.1213615   0.9657673 0.01472862 #> 23  0.58409450 0.1213615   0.9657673 0.01472862 #> 24  0.64089192 0.1213615   0.9657673 0.01472862 #> 25  0.91021968 0.1213615   0.9657673 0.01472862 #> 26 -0.89660969 0.1213615   0.9657673 0.01472862 #> 27 -0.13195991 0.1213615   0.9657673 0.01472862 #> 28 -0.52968769 0.1213615   0.9657673 0.01472862 #> 29 -0.81799629 0.1213615   0.9657673 0.01472862 #> 30 -1.27199371 0.1213615   0.9657673 0.01472862 #> 31 -0.32096024 0.1213615   0.9657673 0.01472862 #> 32 -1.16780103 0.1213615   0.9657673 0.01472862 #> 33 -0.12295473 0.1213615   0.9657673 0.01472862 #> 34 -0.04285945 0.1213615   0.9657673 0.01472862 #> 35 -0.34323505 0.1213615   0.9657673 0.01472862 #> 36 -0.60541633 0.1213615   0.9657673 0.01472862 #> 37  0.17688718 0.1213615   0.9657673 0.01472862 #> 38 -0.55066055 0.1213615   0.9657673 0.01472862 #> 39 -1.05988219 0.1213615   0.9657673 0.01472862 #> 40 -0.04138802 0.1213615   0.9657673 0.01472862 #> 41 -0.12611837 0.1213615   0.9657673 0.01472862 #> 42 -0.60322892 0.1213615   0.9657673 0.01472862 #> 43 -0.11057176 0.1213615   0.9657673 0.01472862 #> 44 -1.06423085 0.1213615   0.9657673 0.01472862 #> 45 -1.08354999 0.1213615   0.9657673 0.01472862 #> 46 -0.84009484 0.1213615   0.9657673 0.01472862 #> 47 -1.14678213 0.1213615   0.9657673 0.01472862 #> 48 -0.57578976 0.1213615   0.9657673 0.01472862 #> 49  0.07186692 0.1213615   0.9657673 0.01472862 #> 50  0.14682421 0.1213615   0.9657673 0.01472862 #> 51  0.35871830 0.1213615   0.9657673 0.01472862 #> 52 -0.43403195 0.1213615   0.9657673 0.01472862 #> 53  0.44603111 0.1213615   0.9657673 0.01472862 #> 54  0.26352000 0.1213615   0.9657673 0.01472862 #> 55  0.55051165 0.1213615   0.9657673 0.01472862 #> 56  0.23453122 0.1213615   0.9657673 0.01472862 #> 57  0.27968138 0.1213615   0.9657673 0.01472862 #> 58  0.70960640 0.1213615   0.9657673 0.01472862 #> 59  0.25227978 0.1213615   0.9657673 0.01472862 #> 60  1.18849297 0.1213615   0.9657673 0.01472862 #> 61  0.21104946 0.1213615   0.9657673 0.01472862 #> 62 -1.16516281 0.1213615   0.9657673 0.01472862 #> 63 -0.85560065 0.1213615   0.9657673 0.01472862 #> 64  0.13398476 0.1213615   0.9657673 0.01472862 #> 65 -0.07912189 0.1213615   0.9657673 0.01472862 #> 66 -0.27146711 0.1213615   0.9657673 0.01472862 #> 67 -0.04417217 0.1213615   0.9657673 0.01472862 #> 68 -1.33425662 0.1213615   0.9657673 0.01472862 #> 69 -0.38720750 0.1213615   0.9657673 0.01472862 #> 70 -0.55355511 0.1213615   0.9657673 0.01472862 #> 71 -0.72242623 0.1213615   0.9657673 0.01472862 #> 72  0.30607449 0.1213615   0.9657673 0.01472862 #> 73  0.77707950 0.1213615   0.9657673 0.01472862 #> 74  0.06847481 0.1213615   0.9657673 0.01472862 #> 75 -0.11052927 0.1213615   0.9657673 0.01472862  # Multiple factors get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\", \"y1\", \"y2\", \"y3\", \"y4\")],        model = \" ind60 =~ x1 + x2 + x3                  dem60 =~ y1 + y2 + y3 + y4 \") #>       fs_ind60    fs_dem60 fs_ind60_se fs_dem60_se ind60_by_fs_ind60 #> 1  -0.54258816 -2.74640573   0.1245694   0.6307323         0.9553858 #> 2   0.12647664 -2.85646114   0.1245694   0.6307323         0.9553858 #> 3   0.73408891  2.74401728   0.1245694   0.6307323         0.9553858 #> 4   1.25253604  3.10856431   0.1245694   0.6307323         0.9553858 #> 5   0.83355267  1.92455641   0.1245694   0.6307323         0.9553858 #> 6   0.22426801  1.02292332   0.1245694   0.6307323         0.9553858 #> 7   0.12517739  1.00406461   0.1245694   0.6307323         0.9553858 #> 8   0.11783867 -0.37216403   0.1245694   0.6307323         0.9553858 #> 9   0.25175134 -1.24897911   0.1245694   0.6307323         0.9553858 #> 10  0.39938631  2.85267059   0.1245694   0.6307323         0.9553858 #> 11  0.67497777  1.41959595   0.1245694   0.6307323         0.9553858 #> 12  0.56462020  1.08769844   0.1245694   0.6307323         0.9553858 #> 13  1.31236592  1.54090232   0.1245694   0.6307323         0.9553858 #> 14  0.23246021  1.77370863   0.1245694   0.6307323         0.9553858 #> 15  0.58638481  2.45676871   0.1245694   0.6307323         0.9553858 #> 16  0.38404785  2.35887573   0.1245694   0.6307323         0.9553858 #> 17  0.05076465  0.04034088   0.1245694   0.6307323         0.9553858 #> 18 -0.01747337 -1.86718064   0.1245694   0.6307323         0.9553858 #> 19  0.90920762  3.61477756   0.1245694   0.6307323         0.9553858 #> 20  1.12553557  0.88355273   0.1245694   0.6307323         0.9553858 #> 21  0.97202590  3.62673300   0.1245694   0.6307323         0.9553858 #> 22  0.87820036 -3.02428925   0.1245694   0.6307323         0.9553858 #> 23  0.57540754 -1.51695438   0.1245694   0.6307323         0.9553858 #> 24  0.66221224  2.76341635   0.1245694   0.6307323         0.9553858 #> 25  0.92358281  2.00507336   0.1245694   0.6307323         0.9553858 #> 26 -0.89353051 -0.92008050   0.1245694   0.6307323         0.9553858 #> 27 -0.13984744 -1.19025576   0.1245694   0.6307323         0.9553858 #> 28 -0.53828496 -1.01247764   0.1245694   0.6307323         0.9553858 #> 29 -0.80834865  0.10456709   0.1245694   0.6307323         0.9553858 #> 30 -1.25324343 -0.71847055   0.1245694   0.6307323         0.9553858 #> 31 -0.33373641 -1.61401581   0.1245694   0.6307323         0.9553858 #> 32 -1.17441075 -3.27250363   0.1245694   0.6307323         0.9553858 #> 33 -0.12409974 -1.17530231   0.1245694   0.6307323         0.9553858 #> 34 -0.04239173 -0.53796274   0.1245694   0.6307323         0.9553858 #> 35 -0.34010528  0.74552889   0.1245694   0.6307323         0.9553858 #> 36 -0.58953870  1.61018662   0.1245694   0.6307323         0.9553858 #> 37  0.17453657 -0.28144814   0.1245694   0.6307323         0.9553858 #> 38 -0.54457243  0.37694690   0.1245694   0.6307323         0.9553858 #> 39 -1.05196602 -0.62919501   0.1245694   0.6307323         0.9553858 #> 40 -0.05504697 -0.03346842   0.1245694   0.6307323         0.9553858 #> 41 -0.12364358 -0.38394102   0.1245694   0.6307323         0.9553858 #> 42 -0.59058710  1.35347275   0.1245694   0.6307323         0.9553858 #> 43 -0.11968796  0.89227782   0.1245694   0.6307323         0.9553858 #> 44 -1.07176064 -2.08481096   0.1245694   0.6307323         0.9553858 #> 45 -1.09139097 -2.07944291   0.1245694   0.6307323         0.9553858 #> 46 -0.83287255  1.59590721   0.1245694   0.6307323         0.9553858 #> 47 -1.14519896 -1.53352201   0.1245694   0.6307323         0.9553858 #> 48 -0.56115378  2.08138051   0.1245694   0.6307323         0.9553858 #> 49  0.06493340 -1.04044137   0.1245694   0.6307323         0.9553858 #> 50  0.15671638  1.72618633   0.1245694   0.6307323         0.9553858 #> 51  0.34626130 -1.24967043   0.1245694   0.6307323         0.9553858 #> 52 -0.45158373 -2.31742576   0.1245694   0.6307323         0.9553858 #> 53  0.43233465 -1.07533341   0.1245694   0.6307323         0.9553858 #> 54  0.25779725 -0.02904676   0.1245694   0.6307323         0.9553858 #> 55  0.51730650 -2.78207923   0.1245694   0.6307323         0.9553858 #> 56  0.20104991 -2.49001474   0.1245694   0.6307323         0.9553858 #> 57  0.25318620 -2.52145147   0.1245694   0.6307323         0.9553858 #> 58  0.72354623  1.86717109   0.1245694   0.6307323         0.9553858 #> 59  0.24619740 -0.93321102   0.1245694   0.6307323         0.9553858 #> 60  1.21681210  3.19853937   0.1245694   0.6307323         0.9553858 #> 61  0.18167599 -3.15685030   0.1245694   0.6307323         0.9553858 #> 62 -1.16605067 -3.41334680   0.1245694   0.6307323         0.9553858 #> 63 -0.86491026 -3.11864398   0.1245694   0.6307323         0.9553858 #> 64  0.10990059 -0.47238885   0.1245694   0.6307323         0.9553858 #> 65 -0.07376176  2.95292007   0.1245694   0.6307323         0.9553858 #> 66 -0.28782931 -1.96509718   0.1245694   0.6307323         0.9553858 #> 67 -0.02508160  2.96218478   0.1245694   0.6307323         0.9553858 #> 68 -1.31843215 -1.59567027   0.1245694   0.6307323         0.9553858 #> 69 -0.40462357 -1.79146161   0.1245694   0.6307323         0.9553858 #> 70 -0.55568363 -1.01578892   0.1245694   0.6307323         0.9553858 #> 71 -0.71308015  0.08818212   0.1245694   0.6307323         0.9553858 #> 72  0.31014319  1.70765911   0.1245694   0.6307323         0.9553858 #> 73  0.79092897  1.86102556   0.1245694   0.6307323         0.9553858 #> 74  0.08770237  3.12885767   0.1245694   0.6307323         0.9553858 #> 75 -0.14138149 -2.41398025   0.1245694   0.6307323         0.9553858 #>    ind60_by_fs_dem60 dem60_by_fs_ind60 dem60_by_fs_dem60 ev_fs_ind60 #> 1           0.181827       0.005867694         0.8688887  0.01551752 #> 2           0.181827       0.005867694         0.8688887  0.01551752 #> 3           0.181827       0.005867694         0.8688887  0.01551752 #> 4           0.181827       0.005867694         0.8688887  0.01551752 #> 5           0.181827       0.005867694         0.8688887  0.01551752 #> 6           0.181827       0.005867694         0.8688887  0.01551752 #> 7           0.181827       0.005867694         0.8688887  0.01551752 #> 8           0.181827       0.005867694         0.8688887  0.01551752 #> 9           0.181827       0.005867694         0.8688887  0.01551752 #> 10          0.181827       0.005867694         0.8688887  0.01551752 #> 11          0.181827       0.005867694         0.8688887  0.01551752 #> 12          0.181827       0.005867694         0.8688887  0.01551752 #> 13          0.181827       0.005867694         0.8688887  0.01551752 #> 14          0.181827       0.005867694         0.8688887  0.01551752 #> 15          0.181827       0.005867694         0.8688887  0.01551752 #> 16          0.181827       0.005867694         0.8688887  0.01551752 #> 17          0.181827       0.005867694         0.8688887  0.01551752 #> 18          0.181827       0.005867694         0.8688887  0.01551752 #> 19          0.181827       0.005867694         0.8688887  0.01551752 #> 20          0.181827       0.005867694         0.8688887  0.01551752 #> 21          0.181827       0.005867694         0.8688887  0.01551752 #> 22          0.181827       0.005867694         0.8688887  0.01551752 #> 23          0.181827       0.005867694         0.8688887  0.01551752 #> 24          0.181827       0.005867694         0.8688887  0.01551752 #> 25          0.181827       0.005867694         0.8688887  0.01551752 #> 26          0.181827       0.005867694         0.8688887  0.01551752 #> 27          0.181827       0.005867694         0.8688887  0.01551752 #> 28          0.181827       0.005867694         0.8688887  0.01551752 #> 29          0.181827       0.005867694         0.8688887  0.01551752 #> 30          0.181827       0.005867694         0.8688887  0.01551752 #> 31          0.181827       0.005867694         0.8688887  0.01551752 #> 32          0.181827       0.005867694         0.8688887  0.01551752 #> 33          0.181827       0.005867694         0.8688887  0.01551752 #> 34          0.181827       0.005867694         0.8688887  0.01551752 #> 35          0.181827       0.005867694         0.8688887  0.01551752 #> 36          0.181827       0.005867694         0.8688887  0.01551752 #> 37          0.181827       0.005867694         0.8688887  0.01551752 #> 38          0.181827       0.005867694         0.8688887  0.01551752 #> 39          0.181827       0.005867694         0.8688887  0.01551752 #> 40          0.181827       0.005867694         0.8688887  0.01551752 #> 41          0.181827       0.005867694         0.8688887  0.01551752 #> 42          0.181827       0.005867694         0.8688887  0.01551752 #> 43          0.181827       0.005867694         0.8688887  0.01551752 #> 44          0.181827       0.005867694         0.8688887  0.01551752 #> 45          0.181827       0.005867694         0.8688887  0.01551752 #> 46          0.181827       0.005867694         0.8688887  0.01551752 #> 47          0.181827       0.005867694         0.8688887  0.01551752 #> 48          0.181827       0.005867694         0.8688887  0.01551752 #> 49          0.181827       0.005867694         0.8688887  0.01551752 #> 50          0.181827       0.005867694         0.8688887  0.01551752 #> 51          0.181827       0.005867694         0.8688887  0.01551752 #> 52          0.181827       0.005867694         0.8688887  0.01551752 #> 53          0.181827       0.005867694         0.8688887  0.01551752 #> 54          0.181827       0.005867694         0.8688887  0.01551752 #> 55          0.181827       0.005867694         0.8688887  0.01551752 #> 56          0.181827       0.005867694         0.8688887  0.01551752 #> 57          0.181827       0.005867694         0.8688887  0.01551752 #> 58          0.181827       0.005867694         0.8688887  0.01551752 #> 59          0.181827       0.005867694         0.8688887  0.01551752 #> 60          0.181827       0.005867694         0.8688887  0.01551752 #> 61          0.181827       0.005867694         0.8688887  0.01551752 #> 62          0.181827       0.005867694         0.8688887  0.01551752 #> 63          0.181827       0.005867694         0.8688887  0.01551752 #> 64          0.181827       0.005867694         0.8688887  0.01551752 #> 65          0.181827       0.005867694         0.8688887  0.01551752 #> 66          0.181827       0.005867694         0.8688887  0.01551752 #> 67          0.181827       0.005867694         0.8688887  0.01551752 #> 68          0.181827       0.005867694         0.8688887  0.01551752 #> 69          0.181827       0.005867694         0.8688887  0.01551752 #> 70          0.181827       0.005867694         0.8688887  0.01551752 #> 71          0.181827       0.005867694         0.8688887  0.01551752 #> 72          0.181827       0.005867694         0.8688887  0.01551752 #> 73          0.181827       0.005867694         0.8688887  0.01551752 #> 74          0.181827       0.005867694         0.8688887  0.01551752 #> 75          0.181827       0.005867694         0.8688887  0.01551752 #>    ecov_fs_dem60_fs_ind60 ev_fs_dem60 #> 1             0.005632564   0.3978232 #> 2             0.005632564   0.3978232 #> 3             0.005632564   0.3978232 #> 4             0.005632564   0.3978232 #> 5             0.005632564   0.3978232 #> 6             0.005632564   0.3978232 #> 7             0.005632564   0.3978232 #> 8             0.005632564   0.3978232 #> 9             0.005632564   0.3978232 #> 10            0.005632564   0.3978232 #> 11            0.005632564   0.3978232 #> 12            0.005632564   0.3978232 #> 13            0.005632564   0.3978232 #> 14            0.005632564   0.3978232 #> 15            0.005632564   0.3978232 #> 16            0.005632564   0.3978232 #> 17            0.005632564   0.3978232 #> 18            0.005632564   0.3978232 #> 19            0.005632564   0.3978232 #> 20            0.005632564   0.3978232 #> 21            0.005632564   0.3978232 #> 22            0.005632564   0.3978232 #> 23            0.005632564   0.3978232 #> 24            0.005632564   0.3978232 #> 25            0.005632564   0.3978232 #> 26            0.005632564   0.3978232 #> 27            0.005632564   0.3978232 #> 28            0.005632564   0.3978232 #> 29            0.005632564   0.3978232 #> 30            0.005632564   0.3978232 #> 31            0.005632564   0.3978232 #> 32            0.005632564   0.3978232 #> 33            0.005632564   0.3978232 #> 34            0.005632564   0.3978232 #> 35            0.005632564   0.3978232 #> 36            0.005632564   0.3978232 #> 37            0.005632564   0.3978232 #> 38            0.005632564   0.3978232 #> 39            0.005632564   0.3978232 #> 40            0.005632564   0.3978232 #> 41            0.005632564   0.3978232 #> 42            0.005632564   0.3978232 #> 43            0.005632564   0.3978232 #> 44            0.005632564   0.3978232 #> 45            0.005632564   0.3978232 #> 46            0.005632564   0.3978232 #> 47            0.005632564   0.3978232 #> 48            0.005632564   0.3978232 #> 49            0.005632564   0.3978232 #> 50            0.005632564   0.3978232 #> 51            0.005632564   0.3978232 #> 52            0.005632564   0.3978232 #> 53            0.005632564   0.3978232 #> 54            0.005632564   0.3978232 #> 55            0.005632564   0.3978232 #> 56            0.005632564   0.3978232 #> 57            0.005632564   0.3978232 #> 58            0.005632564   0.3978232 #> 59            0.005632564   0.3978232 #> 60            0.005632564   0.3978232 #> 61            0.005632564   0.3978232 #> 62            0.005632564   0.3978232 #> 63            0.005632564   0.3978232 #> 64            0.005632564   0.3978232 #> 65            0.005632564   0.3978232 #> 66            0.005632564   0.3978232 #> 67            0.005632564   0.3978232 #> 68            0.005632564   0.3978232 #> 69            0.005632564   0.3978232 #> 70            0.005632564   0.3978232 #> 71            0.005632564   0.3978232 #> 72            0.005632564   0.3978232 #> 73            0.005632564   0.3978232 #> 74            0.005632564   0.3978232 #> 75            0.005632564   0.3978232  # Multiple-group hs_model <- ' visual  =~ x1 + x2 + x3 ' fit <- cfa(hs_model,            data = HolzingerSwineford1939,            group = \"school\") get_fs(HolzingerSwineford1939, hs_model, group = \"school\") #> $Pasteur #>        fs_visual fs_visual_se visual_by_fs_visual ev_fs_visual  school #> 1   -0.821165191    0.3391326           0.6734826    0.1150109 Pasteur #> 2   -0.124009418    0.3391326           0.6734826    0.1150109 Pasteur #> 3   -0.370072089    0.3391326           0.6734826    0.1150109 Pasteur #> 4    0.440928618    0.3391326           0.6734826    0.1150109 Pasteur #> 5   -0.691389016    0.3391326           0.6734826    0.1150109 Pasteur #> 6   -0.110032619    0.3391326           0.6734826    0.1150109 Pasteur #> 7   -0.904127845    0.3391326           0.6734826    0.1150109 Pasteur #> 8   -0.031747573    0.3391326           0.6734826    0.1150109 Pasteur #> 9   -0.439478981    0.3391326           0.6734826    0.1150109 Pasteur #> 10  -0.938939050    0.3391326           0.6734826    0.1150109 Pasteur #> 11  -0.436821880    0.3391326           0.6734826    0.1150109 Pasteur #> 12   0.305033497    0.3391326           0.6734826    0.1150109 Pasteur #> 13   0.522076263    0.3391326           0.6734826    0.1150109 Pasteur #> 14  -0.090367931    0.3391326           0.6734826    0.1150109 Pasteur #> 15   0.526276771    0.3391326           0.6734826    0.1150109 Pasteur #> 16  -0.226580678    0.3391326           0.6734826    0.1150109 Pasteur #> 17  -0.582016192    0.3391326           0.6734826    0.1150109 Pasteur #> 18   0.017040431    0.3391326           0.6734826    0.1150109 Pasteur #> 19   0.563052459    0.3391326           0.6734826    0.1150109 Pasteur #> 20   0.746621910    0.3391326           0.6734826    0.1150109 Pasteur #> 21   0.234672405    0.3391326           0.6734826    0.1150109 Pasteur #> 22   1.157487518    0.3391326           0.6734826    0.1150109 Pasteur #> 23  -0.162272449    0.3391326           0.6734826    0.1150109 Pasteur #> 24  -0.556027059    0.3391326           0.6734826    0.1150109 Pasteur #> 25  -0.321443540    0.3391326           0.6734826    0.1150109 Pasteur #> 26   0.153141050    0.3391326           0.6734826    0.1150109 Pasteur #> 27   0.696234416    0.3391326           0.6734826    0.1150109 Pasteur #> 28  -0.020961039    0.3391326           0.6734826    0.1150109 Pasteur #> 29   0.532601236    0.3391326           0.6734826    0.1150109 Pasteur #> 30  -0.727687585    0.3391326           0.6734826    0.1150109 Pasteur #> 31  -0.676719580    0.3391326           0.6734826    0.1150109 Pasteur #> 32  -1.120216393    0.3391326           0.6734826    0.1150109 Pasteur #> 33  -0.313631732    0.3391326           0.6734826    0.1150109 Pasteur #> 34  -0.187091845    0.3391326           0.6734826    0.1150109 Pasteur #> 35  -0.887709484    0.3391326           0.6734826    0.1150109 Pasteur #> 36  -0.760795908    0.3391326           0.6734826    0.1150109 Pasteur #> 37   0.556943532    0.3391326           0.6734826    0.1150109 Pasteur #> 38  -0.458666570    0.3391326           0.6734826    0.1150109 Pasteur #> 39   0.514741536    0.3391326           0.6734826    0.1150109 Pasteur #> 40   0.373009089    0.3391326           0.6734826    0.1150109 Pasteur #> 41  -0.528550562    0.3391326           0.6734826    0.1150109 Pasteur #> 42  -0.865864795    0.3391326           0.6734826    0.1150109 Pasteur #> 43  -1.182344640    0.3391326           0.6734826    0.1150109 Pasteur #> 44  -0.435334517    0.3391326           0.6734826    0.1150109 Pasteur #> 45   0.306520860    0.3391326           0.6734826    0.1150109 Pasteur #> 46   0.821604565    0.3391326           0.6734826    0.1150109 Pasteur #> 47   1.213927875    0.3391326           0.6734826    0.1150109 Pasteur #> 48  -0.851887996    0.3391326           0.6734826    0.1150109 Pasteur #> 49  -0.085053749    0.3391326           0.6734826    0.1150109 Pasteur #> 50  -0.508885873    0.3391326           0.6734826    0.1150109 Pasteur #> 51   0.502467638    0.3391326           0.6734826    0.1150109 Pasteur #> 52   0.284732253    0.3391326           0.6734826    0.1150109 Pasteur #> 53   0.202677755    0.3391326           0.6734826    0.1150109 Pasteur #> 54  -0.335953502    0.3391326           0.6734826    0.1150109 Pasteur #> 55   0.556410369    0.3391326           0.6734826    0.1150109 Pasteur #> 56  -0.058746970    0.3391326           0.6734826    0.1150109 Pasteur #> 57  -0.066932487    0.3391326           0.6734826    0.1150109 Pasteur #> 58   0.554230368    0.3391326           0.6734826    0.1150109 Pasteur #> 59  -0.321761185    0.3391326           0.6734826    0.1150109 Pasteur #> 60  -0.421834819    0.3391326           0.6734826    0.1150109 Pasteur #> 61   0.345476529    0.3391326           0.6734826    0.1150109 Pasteur #> 62   0.194809883    0.3391326           0.6734826    0.1150109 Pasteur #> 63  -0.207870208    0.3391326           0.6734826    0.1150109 Pasteur #> 64  -0.441658981    0.3391326           0.6734826    0.1150109 Pasteur #> 65   0.102070958    0.3391326           0.6734826    0.1150109 Pasteur #> 66   0.311198487    0.3391326           0.6734826    0.1150109 Pasteur #> 67   0.676364229    0.3391326           0.6734826    0.1150109 Pasteur #> 68   0.297858262    0.3391326           0.6734826    0.1150109 Pasteur #> 69  -1.055487128    0.3391326           0.6734826    0.1150109 Pasteur #> 70  -0.737997019    0.3391326           0.6734826    0.1150109 Pasteur #> 71  -1.576099236    0.3391326           0.6734826    0.1150109 Pasteur #> 72   0.534360181    0.3391326           0.6734826    0.1150109 Pasteur #> 73  -0.105888156    0.3391326           0.6734826    0.1150109 Pasteur #> 74   0.266237302    0.3391326           0.6734826    0.1150109 Pasteur #> 75  -0.352427927    0.3391326           0.6734826    0.1150109 Pasteur #> 76  -0.334783784    0.3391326           0.6734826    0.1150109 Pasteur #> 77   0.133588508    0.3391326           0.6734826    0.1150109 Pasteur #> 78  -1.035662965    0.3391326           0.6734826    0.1150109 Pasteur #> 79   0.762507108    0.3391326           0.6734826    0.1150109 Pasteur #> 80  -0.260699265    0.3391326           0.6734826    0.1150109 Pasteur #> 81  -0.329095893    0.3391326           0.6734826    0.1150109 Pasteur #> 82   0.752413211    0.3391326           0.6734826    0.1150109 Pasteur #> 83   0.149268188    0.3391326           0.6734826    0.1150109 Pasteur #> 84  -0.208880471    0.3391326           0.6734826    0.1150109 Pasteur #> 85  -1.078285998    0.3391326           0.6734826    0.1150109 Pasteur #> 86   0.306043760    0.3391326           0.6734826    0.1150109 Pasteur #> 87   0.349677056    0.3391326           0.6734826    0.1150109 Pasteur #> 88   0.165686549    0.3391326           0.6734826    0.1150109 Pasteur #> 89   0.077307606    0.3391326           0.6734826    0.1150109 Pasteur #> 90  -0.077401396    0.3391326           0.6734826    0.1150109 Pasteur #> 91  -0.081863485    0.3391326           0.6734826    0.1150109 Pasteur #> 92   0.106748566    0.3391326           0.6734826    0.1150109 Pasteur #> 93  -0.211593616    0.3391326           0.6734826    0.1150109 Pasteur #> 94  -0.926665153    0.3391326           0.6734826    0.1150109 Pasteur #> 95  -0.739484382    0.3391326           0.6734826    0.1150109 Pasteur #> 96   0.570387167    0.3391326           0.6734826    0.1150109 Pasteur #> 97  -0.913642554    0.3391326           0.6734826    0.1150109 Pasteur #> 98   0.547484887    0.3391326           0.6734826    0.1150109 Pasteur #> 99  -0.602850599    0.3391326           0.6734826    0.1150109 Pasteur #> 100  0.225794270    0.3391326           0.6734826    0.1150109 Pasteur #> 101  0.620447015    0.3391326           0.6734826    0.1150109 Pasteur #> 102  0.158885005    0.3391326           0.6734826    0.1150109 Pasteur #> 103 -0.127938344    0.3391326           0.6734826    0.1150109 Pasteur #> 104 -0.420347455    0.3391326           0.6734826    0.1150109 Pasteur #> 105  1.327978307    0.3391326           0.6734826    0.1150109 Pasteur #> 106  0.181843348    0.3391326           0.6734826    0.1150109 Pasteur #> 107 -0.148932224    0.3391326           0.6734826    0.1150109 Pasteur #> 108  0.612373626    0.3391326           0.6734826    0.1150109 Pasteur #> 109 -0.066558798    0.3391326           0.6734826    0.1150109 Pasteur #> 110 -0.420880619    0.3391326           0.6734826    0.1150109 Pasteur #> 111  1.127036295    0.3391326           0.6734826    0.1150109 Pasteur #> 112  0.237591068    0.3391326           0.6734826    0.1150109 Pasteur #> 113  0.853758689    0.3391326           0.6734826    0.1150109 Pasteur #> 114 -0.143618023    0.3391326           0.6734826    0.1150109 Pasteur #> 115  0.475206679    0.3391326           0.6734826    0.1150109 Pasteur #> 116 -0.670554590    0.3391326           0.6734826    0.1150109 Pasteur #> 117  0.022672257    0.3391326           0.6734826    0.1150109 Pasteur #> 118  0.302002707    0.3391326           0.6734826    0.1150109 Pasteur #> 119  0.151392125    0.3391326           0.6734826    0.1150109 Pasteur #> 120 -0.475300449    0.3391326           0.6734826    0.1150109 Pasteur #> 121 -0.346740056    0.3391326           0.6734826    0.1150109 Pasteur #> 122 -0.078888759    0.3391326           0.6734826    0.1150109 Pasteur #> 123  1.197237913    0.3391326           0.6734826    0.1150109 Pasteur #> 124  0.539243306    0.3391326           0.6734826    0.1150109 Pasteur #> 125  0.867258388    0.3391326           0.6734826    0.1150109 Pasteur #> 126  0.592287901    0.3391326           0.6734826    0.1150109 Pasteur #> 127 -0.500540901    0.3391326           0.6734826    0.1150109 Pasteur #> 128 -0.361193954    0.3391326           0.6734826    0.1150109 Pasteur #> 129  0.626883588    0.3391326           0.6734826    0.1150109 Pasteur #> 130 -0.437514518    0.3391326           0.6734826    0.1150109 Pasteur #> 131  0.695972854    0.3391326           0.6734826    0.1150109 Pasteur #> 132  0.424715775    0.3391326           0.6734826    0.1150109 Pasteur #> 133 -0.203725744    0.3391326           0.6734826    0.1150109 Pasteur #> 134 -0.441499507    0.3391326           0.6734826    0.1150109 Pasteur #> 135  0.735619838    0.3391326           0.6734826    0.1150109 Pasteur #> 136  0.783874697    0.3391326           0.6734826    0.1150109 Pasteur #> 137  0.565709540    0.3391326           0.6734826    0.1150109 Pasteur #> 138  0.258425494    0.3391326           0.6734826    0.1150109 Pasteur #> 139  0.861093397    0.3391326           0.6734826    0.1150109 Pasteur #> 140 -0.059757233    0.3391326           0.6734826    0.1150109 Pasteur #> 141 -0.920340689    0.3391326           0.6734826    0.1150109 Pasteur #> 142  0.845629236    0.3391326           0.6734826    0.1150109 Pasteur #> 143  1.227427574    0.3391326           0.6734826    0.1150109 Pasteur #> 144  1.054223601    0.3391326           0.6734826    0.1150109 Pasteur #> 145 -1.246596805    0.3391326           0.6734826    0.1150109 Pasteur #> 146 -0.473120468    0.3391326           0.6734826    0.1150109 Pasteur #> 147 -0.560171503    0.3391326           0.6734826    0.1150109 Pasteur #> 148 -0.365394462    0.3391326           0.6734826    0.1150109 Pasteur #> 149  0.084744422    0.3391326           0.6734826    0.1150109 Pasteur #> 150  0.910676146    0.3391326           0.6734826    0.1150109 Pasteur #> 151  1.094189533    0.3391326           0.6734826    0.1150109 Pasteur #> 152 -0.013149231    0.3391326           0.6734826    0.1150109 Pasteur #> 153 -0.166472976    0.3391326           0.6734826    0.1150109 Pasteur #> 154  0.008695459    0.3391326           0.6734826    0.1150109 Pasteur #> 155 -0.094989494    0.3391326           0.6734826    0.1150109 Pasteur #> 156 -0.457123143    0.3391326           0.6734826    0.1150109 Pasteur #>  #> $`Grant-White` #>        fs_visual fs_visual_se visual_by_fs_visual ev_fs_visual      school #> 1   -0.915287109     0.311828           0.6990509   0.09723667 Grant-White #> 2    0.035963597     0.311828           0.6990509   0.09723667 Grant-White #> 3    0.355636604     0.311828           0.6990509   0.09723667 Grant-White #> 4   -0.387353871     0.311828           0.6990509   0.09723667 Grant-White #> 5   -0.622393942     0.311828           0.6990509   0.09723667 Grant-White #> 6    0.195944561     0.311828           0.6990509   0.09723667 Grant-White #> 7    1.353023831     0.311828           0.6990509   0.09723667 Grant-White #> 8   -0.341506254     0.311828           0.6990509   0.09723667 Grant-White #> 9   -0.199493575     0.311828           0.6990509   0.09723667 Grant-White #> 10  -0.689869149     0.311828           0.6990509   0.09723667 Grant-White #> 11  -0.463929554     0.311828           0.6990509   0.09723667 Grant-White #> 12  -0.423001505     0.311828           0.6990509   0.09723667 Grant-White #> 13   0.279743296     0.311828           0.6990509   0.09723667 Grant-White #> 14  -0.916908219     0.311828           0.6990509   0.09723667 Grant-White #> 15   0.589344501     0.311828           0.6990509   0.09723667 Grant-White #> 16   0.191474701     0.311828           0.6990509   0.09723667 Grant-White #> 17   0.935275715     0.311828           0.6990509   0.09723667 Grant-White #> 18   0.393715904     0.311828           0.6990509   0.09723667 Grant-White #> 19   0.086569994     0.311828           0.6990509   0.09723667 Grant-White #> 20   0.555606898     0.311828           0.6990509   0.09723667 Grant-White #> 21  -0.558217193     0.311828           0.6990509   0.09723667 Grant-White #> 22   0.766715894     0.311828           0.6990509   0.09723667 Grant-White #> 23   0.115548801     0.311828           0.6990509   0.09723667 Grant-White #> 24   0.901249191     0.311828           0.6990509   0.09723667 Grant-White #> 25   0.174316971     0.311828           0.6990509   0.09723667 Grant-White #> 26  -0.078980322     0.311828           0.6990509   0.09723667 Grant-White #> 27  -0.581882977     0.311828           0.6990509   0.09723667 Grant-White #> 28  -0.661179262     0.311828           0.6990509   0.09723667 Grant-White #> 29  -0.245341176     0.311828           0.6990509   0.09723667 Grant-White #> 30  -0.195801662     0.311828           0.6990509   0.09723667 Grant-White #> 31  -0.281221524     0.311828           0.6990509   0.09723667 Grant-White #> 32  -0.293909378     0.311828           0.6990509   0.09723667 Grant-White #> 33  -0.604192958     0.311828           0.6990509   0.09723667 Grant-White #> 34  -0.738437335     0.311828           0.6990509   0.09723667 Grant-White #> 35  -0.304109345     0.311828           0.6990509   0.09723667 Grant-White #> 36   0.104931733     0.311828           0.6990509   0.09723667 Grant-White #> 37  -0.025781487     0.311828           0.6990509   0.09723667 Grant-White #> 38  -0.897318824     0.311828           0.6990509   0.09723667 Grant-White #> 39  -0.892560027     0.311828           0.6990509   0.09723667 Grant-White #> 40  -0.078402465     0.311828           0.6990509   0.09723667 Grant-White #> 41  -0.379063934     0.311828           0.6990509   0.09723667 Grant-White #> 42  -0.324926380     0.311828           0.6990509   0.09723667 Grant-White #> 43  -0.684299797     0.311828           0.6990509   0.09723667 Grant-White #> 44  -0.304109345     0.311828           0.6990509   0.09723667 Grant-White #> 45   0.622793169     0.311828           0.6990509   0.09723667 Grant-White #> 46  -0.152835419     0.311828           0.6990509   0.09723667 Grant-White #> 47  -0.421902013     0.311828           0.6990509   0.09723667 Grant-White #> 48  -0.060883872     0.311828           0.6990509   0.09723667 Grant-White #> 49  -0.303298790     0.311828           0.6990509   0.09723667 Grant-White #> 50   0.425021826     0.311828           0.6990509   0.09723667 Grant-White #> 51   0.131478875     0.311828           0.6990509   0.09723667 Grant-White #> 52  -0.914998172     0.311828           0.6990509   0.09723667 Grant-White #> 53   0.324226132     0.311828           0.6990509   0.09723667 Grant-White #> 54  -0.086170767     0.311828           0.6990509   0.09723667 Grant-White #> 55   0.428424818     0.311828           0.6990509   0.09723667 Grant-White #> 56   0.188465179     0.311828           0.6990509   0.09723667 Grant-White #> 57  -0.306958111     0.311828           0.6990509   0.09723667 Grant-White #> 58   0.581736955     0.311828           0.6990509   0.09723667 Grant-White #> 59  -0.743485051     0.311828           0.6990509   0.09723667 Grant-White #> 60   0.263580524     0.311828           0.6990509   0.09723667 Grant-White #> 61   0.178786831     0.311828           0.6990509   0.09723667 Grant-White #> 62  -0.064832113     0.311828           0.6990509   0.09723667 Grant-White #> 63   0.499976414     0.311828           0.6990509   0.09723667 Grant-White #> 64  -0.092839593     0.311828           0.6990509   0.09723667 Grant-White #> 65  -0.263702931     0.311828           0.6990509   0.09723667 Grant-White #> 66  -0.983966325     0.311828           0.6990509   0.09723667 Grant-White #> 67   1.434912536     0.311828           0.6990509   0.09723667 Grant-White #> 68  -1.037582228     0.311828           0.6990509   0.09723667 Grant-White #> 69  -0.047569849     0.311828           0.6990509   0.09723667 Grant-White #> 70   1.084767775     0.311828           0.6990509   0.09723667 Grant-White #> 71   0.092011181     0.311828           0.6990509   0.09723667 Grant-White #> 72  -0.562687052     0.311828           0.6990509   0.09723667 Grant-White #> 73  -0.304919900     0.311828           0.6990509   0.09723667 Grant-White #> 74   1.038920175     0.311828           0.6990509   0.09723667 Grant-White #> 75  -0.789854287     0.311828           0.6990509   0.09723667 Grant-White #> 76  -0.602282928     0.311828           0.6990509   0.09723667 Grant-White #> 77  -0.894630830     0.311828           0.6990509   0.09723667 Grant-White #> 78   0.532614527     0.311828           0.6990509   0.09723667 Grant-White #> 79  -0.548955945     0.311828           0.6990509   0.09723667 Grant-White #> 80  -0.221514635     0.311828           0.6990509   0.09723667 Grant-White #> 81  -0.095849115     0.311828           0.6990509   0.09723667 Grant-White #> 82  -0.122235502     0.311828           0.6990509   0.09723667 Grant-White #> 83   0.892276864     0.311828           0.6990509   0.09723667 Grant-White #> 84   0.328439663     0.311828           0.6990509   0.09723667 Grant-White #> 85   1.217391042     0.311828           0.6990509   0.09723667 Grant-White #> 86   0.574513902     0.311828           0.6990509   0.09723667 Grant-White #> 87   0.160168762     0.311828           0.6990509   0.09723667 Grant-White #> 88   0.654909662     0.311828           0.6990509   0.09723667 Grant-White #> 89  -0.509777155     0.311828           0.6990509   0.09723667 Grant-White #> 90   1.201493560     0.311828           0.6990509   0.09723667 Grant-White #> 91   0.584874625     0.311828           0.6990509   0.09723667 Grant-White #> 92   0.075142371     0.311828           0.6990509   0.09723667 Grant-White #> 93   0.550976266     0.311828           0.6990509   0.09723667 Grant-White #> 94  -0.886308302     0.311828           0.6990509   0.09723667 Grant-White #> 95   0.552075757     0.311828           0.6990509   0.09723667 Grant-White #> 96   1.415972940     0.311828           0.6990509   0.09723667 Grant-White #> 97   0.298650301     0.311828           0.6990509   0.09723667 Grant-White #> 98  -0.143028906     0.311828           0.6990509   0.09723667 Grant-White #> 99   0.245195154     0.311828           0.6990509   0.09723667 Grant-White #> 100  0.247072593     0.311828           0.6990509   0.09723667 Grant-White #> 101  0.817322291     0.311828           0.6990509   0.09723667 Grant-White #> 102  0.651771976     0.311828           0.6990509   0.09723667 Grant-White #> 103  1.338875623     0.311828           0.6990509   0.09723667 Grant-White #> 104 -1.160005528     0.311828           0.6990509   0.09723667 Grant-White #> 105  0.163306449     0.311828           0.6990509   0.09723667 Grant-White #> 106 -0.387353871     0.311828           0.6990509   0.09723667 Grant-White #> 107 -0.517128372     0.311828           0.6990509   0.09723667 Grant-White #> 108  0.065103160     0.311828           0.6990509   0.09723667 Grant-White #> 109 -0.115438510     0.311828           0.6990509   0.09723667 Grant-White #> 110  0.094049376     0.311828           0.6990509   0.09723667 Grant-White #> 111  0.396725409     0.311828           0.6990509   0.09723667 Grant-White #> 112  0.672356312     0.311828           0.6990509   0.09723667 Grant-White #> 113  1.165974090     0.311828           0.6990509   0.09723667 Grant-White #> 114 -0.483518949     0.311828           0.6990509   0.09723667 Grant-White #> 115  0.035024877     0.311828           0.6990509   0.09723667 Grant-White #> 116  0.741974248     0.311828           0.6990509   0.09723667 Grant-White #> 117 -0.170386603     0.311828           0.6990509   0.09723667 Grant-White #> 118 -0.205873481     0.311828           0.6990509   0.09723667 Grant-White #> 119  0.714777307     0.311828           0.6990509   0.09723667 Grant-White #> 120 -0.620772831     0.311828           0.6990509   0.09723667 Grant-White #> 121 -0.313626938     0.311828           0.6990509   0.09723667 Grant-White #> 122 -0.157466035     0.311828           0.6990509   0.09723667 Grant-White #> 123  0.118269386     0.311828           0.6990509   0.09723667 Grant-White #> 124  0.101111673     0.311828           0.6990509   0.09723667 Grant-White #> 125 -0.625403463     0.311828           0.6990509   0.09723667 Grant-White #> 126  0.486638761     0.311828           0.6990509   0.09723667 Grant-White #> 127 -0.178676540     0.311828           0.6990509   0.09723667 Grant-White #> 128  0.274013189     0.311828           0.6990509   0.09723667 Grant-White #> 129 -0.316347523     0.311828           0.6990509   0.09723667 Grant-White #> 130 -0.026752814     0.311828           0.6990509   0.09723667 Grant-White #> 131  0.245323318     0.311828           0.6990509   0.09723667 Grant-White #> 132 -0.356336853     0.311828           0.6990509   0.09723667 Grant-White #> 133 -0.581594057     0.311828           0.6990509   0.09723667 Grant-White #> 134  0.263002667     0.311828           0.6990509   0.09723667 Grant-White #> 135 -0.864680712     0.311828           0.6990509   0.09723667 Grant-White #> 136 -0.377964443     0.311828           0.6990509   0.09723667 Grant-White #> 137 -0.112717909     0.311828           0.6990509   0.09723667 Grant-White #> 138  0.114449326     0.311828           0.6990509   0.09723667 Grant-White #> 139  0.001287274     0.311828           0.6990509   0.09723667 Grant-White #> 140  0.597634438     0.311828           0.6990509   0.09723667 Grant-White #> 141 -0.252531637     0.311828           0.6990509   0.09723667 Grant-White #> 142 -0.472901881     0.311828           0.6990509   0.09723667 Grant-White #> 143 -0.187255397     0.311828           0.6990509   0.09723667 Grant-White #> 144 -0.542415283     0.311828           0.6990509   0.09723667 Grant-White #> 145  0.358774274     0.311828           0.6990509   0.09723667 Grant-White #>  #> attr(,\"fsT\") #> attr(,\"fsT\")$Pasteur #>           fs_visual #> fs_visual 0.1150109 #>  #> attr(,\"fsT\")$`Grant-White` #>            fs_visual #> fs_visual 0.09723667 #>  #> attr(,\"fsL\") #> attr(,\"fsL\")$Pasteur #>              visual #> fs_visual 0.6734826 #>  #> attr(,\"fsL\")$`Grant-White` #>              visual #> fs_visual 0.6990509 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #> fs_visual  #>         0  #>  #> attr(,\"fsb\")$`Grant-White` #> fs_visual  #>         0  #>  #> attr(,\"scoring_matrix\") #> attr(,\"scoring_matrix\")$Pasteur #>             [,1]     [,2]      [,3] #> visual 0.1957873 0.109906 0.3316264 #>  #> attr(,\"scoring_matrix\")$`Grant-White` #>             [,1]      [,2]      [,3] #> visual 0.1624126 0.1458328 0.3515006 #>  # Or without the model get_fs(HolzingerSwineford1939[c(\"school\", \"x4\", \"x5\", \"x6\")],        group = \"school\") #> $Pasteur #>             fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1  school #> 1    0.3074500370 0.2999315   0.8833584 0.08995892 Pasteur #> 2   -0.7746062892 0.2999315   0.8833584 0.08995892 Pasteur #> 3   -1.5843019574 0.2999315   0.8833584 0.08995892 Pasteur #> 4    0.2739579120 0.2999315   0.8833584 0.08995892 Pasteur #> 5    0.1440153923 0.2999315   0.8833584 0.08995892 Pasteur #> 6   -1.0440895948 0.2999315   0.8833584 0.08995892 Pasteur #> 7    1.0507357396 0.2999315   0.8833584 0.08995892 Pasteur #> 8    0.1041882698 0.2999315   0.8833584 0.08995892 Pasteur #> 9    0.7750146375 0.2999315   0.8833584 0.08995892 Pasteur #> 10   0.4822117444 0.2999315   0.8833584 0.08995892 Pasteur #> 11  -0.4511886490 0.2999315   0.8833584 0.08995892 Pasteur #> 12   0.3522691973 0.2999315   0.8833584 0.08995892 Pasteur #> 13   0.0657041070 0.2999315   0.8833584 0.08995892 Pasteur #> 14   0.3259750264 0.2999315   0.8833584 0.08995892 Pasteur #> 15   1.3008341323 0.2999315   0.8833584 0.08995892 Pasteur #> 16  -0.2804588573 0.2999315   0.8833584 0.08995892 Pasteur #> 17  -0.3604017581 0.2999315   0.8833584 0.08995892 Pasteur #> 18  -0.7502293722 0.2999315   0.8833584 0.08995892 Pasteur #> 19   1.2600468631 0.2999315   0.8833584 0.08995892 Pasteur #> 20   0.2874908636 0.2999315   0.8833584 0.08995892 Pasteur #> 21  -1.1394826729 0.2999315   0.8833584 0.08995892 Pasteur #> 22  -0.3791151940 0.2999315   0.8833584 0.08995892 Pasteur #> 23   1.0444979094 0.2999315   0.8833584 0.08995892 Pasteur #> 24  -0.6248930150 0.2999315   0.8833584 0.08995892 Pasteur #> 25  -0.3689426673 0.2999315   0.8833584 0.08995892 Pasteur #> 26  -0.5663524842 0.2999315   0.8833584 0.08995892 Pasteur #> 27  -0.9568515617 0.2999315   0.8833584 0.08995892 Pasteur #> 28  -0.8137619455 0.2999315   0.8833584 0.08995892 Pasteur #> 29  -0.5028199109 0.2999315   0.8833584 0.08995892 Pasteur #> 30  -0.3054100713 0.2999315   0.8833584 0.08995892 Pasteur #> 31  -0.3728773637 0.2999315   0.8833584 0.08995892 Pasteur #> 32  -0.8529175744 0.2999315   0.8833584 0.08995892 Pasteur #> 33   0.4101382620 0.2999315   0.8833584 0.08995892 Pasteur #> 34   0.1848026386 0.2999315   0.8833584 0.08995892 Pasteur #> 35  -0.9680814159 0.2999315   0.8833584 0.08995892 Pasteur #> 36  -0.1436070439 0.2999315   0.8833584 0.08995892 Pasteur #> 37  -0.0126071782 0.2999315   0.8833584 0.08995892 Pasteur #> 38  -0.3531066368 0.2999315   0.8833584 0.08995892 Pasteur #> 39   1.0665717701 0.2999315   0.8833584 0.08995892 Pasteur #> 40   0.7993915544 0.2999315   0.8833584 0.08995892 Pasteur #> 41   0.1110975387 0.2999315   0.8833584 0.08995892 Pasteur #> 42  -0.2735495637 0.2999315   0.8833584 0.08995892 Pasteur #> 43  -0.7167372327 0.2999315   0.8833584 0.08995892 Pasteur #> 44  -0.6594424814 0.2999315   0.8833584 0.08995892 Pasteur #> 45   0.9507364688 0.2999315   0.8833584 0.08995892 Pasteur #> 46  -0.0478281107 0.2999315   0.8833584 0.08995892 Pasteur #> 47  -1.7573348450 0.2999315   0.8833584 0.08995892 Pasteur #> 48  -0.4620326417 0.2999315   0.8833584 0.08995892 Pasteur #> 49  -1.0305566597 0.2999315   0.8833584 0.08995892 Pasteur #> 50   0.7618675383 0.2999315   0.8833584 0.08995892 Pasteur #> 51  -1.3414986833 0.2999315   0.8833584 0.08995892 Pasteur #> 52  -0.0761397743 0.2999315   0.8833584 0.08995892 Pasteur #> 53  -1.8231705136 0.2999315   0.8833584 0.08995892 Pasteur #> 54   0.0094666323 0.2999315   0.8833584 0.08995892 Pasteur #> 55   0.0436302463 0.2999315   0.8833584 0.08995892 Pasteur #> 56  -0.0001315726 0.2999315   0.8833584 0.08995892 Pasteur #> 57  -0.6571393750 0.2999315   0.8833584 0.08995892 Pasteur #> 58   0.6121542642 0.2999315   0.8833584 0.08995892 Pasteur #> 59  -1.0957208458 0.2999315   0.8833584 0.08995892 Pasteur #> 60  -0.9289257761 0.2999315   0.8833584 0.08995892 Pasteur #> 61   0.9553426588 0.2999315   0.8833584 0.08995892 Pasteur #> 62   0.3647448029 0.2999315   0.8833584 0.08995892 Pasteur #> 63  -1.1003270330 0.2999315   0.8833584 0.08995892 Pasteur #> 64   0.4259742697 0.2999315   0.8833584 0.08995892 Pasteur #> 65   0.1689666035 0.2999315   0.8833584 0.08995892 Pasteur #> 66   0.6586050419 0.2999315   0.8833584 0.08995892 Pasteur #> 67  -0.2952375720 0.2999315   0.8833584 0.08995892 Pasteur #> 68  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 69  -0.5613604418 0.2999315   0.8833584 0.08995892 Pasteur #> 70   0.5632119383 0.2999315   0.8833584 0.08995892 Pasteur #> 71  -0.7769093955 0.2999315   0.8833584 0.08995892 Pasteur #> 72  -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 73  -0.7808440920 0.2999315   0.8833584 0.08995892 Pasteur #> 74   0.9270309952 0.2999315   0.8833584 0.08995892 Pasteur #> 75  -0.5426470306 0.2999315   0.8833584 0.08995892 Pasteur #> 76  -1.1065648385 0.2999315   0.8833584 0.08995892 Pasteur #> 77   0.6233841094 0.2999315   0.8833584 0.08995892 Pasteur #> 78  -1.7010974136 0.2999315   0.8833584 0.08995892 Pasteur #> 79  -0.0013773330 0.2999315   0.8833584 0.08995892 Pasteur #> 80  -1.2772946275 0.2999315   0.8833584 0.08995892 Pasteur #> 81  -1.0344913561 0.2999315   0.8833584 0.08995892 Pasteur #> 82   0.4345151789 0.2999315   0.8833584 0.08995892 Pasteur #> 83  -1.5280645151 0.2999315   0.8833584 0.08995892 Pasteur #> 84  -0.6101143231 0.2999315   0.8833584 0.08995892 Pasteur #> 85  -1.5122284832 0.2999315   0.8833584 0.08995892 Pasteur #> 86   1.2011204798 0.2999315   0.8833584 0.08995892 Pasteur #> 87  -0.3258523027 0.2999315   0.8833584 0.08995892 Pasteur #> 88   0.6687775411 0.2999315   0.8833584 0.08995892 Pasteur #> 89  -1.6382363147 0.2999315   0.8833584 0.08995892 Pasteur #> 90   0.3062042720 0.2999315   0.8833584 0.08995892 Pasteur #> 91  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 92  -0.6798846937 0.2999315   0.8833584 0.08995892 Pasteur #> 93  -1.1865077366 0.2999315   0.8833584 0.08995892 Pasteur #> 94  -0.5011882981 0.2999315   0.8833584 0.08995892 Pasteur #> 95   0.7362448336 0.2999315   0.8833584 0.08995892 Pasteur #> 96   0.4934415622 0.2999315   0.8833584 0.08995892 Pasteur #> 97   0.9661866515 0.2999315   0.8833584 0.08995892 Pasteur #> 98   1.7212764661 0.2999315   0.8833584 0.08995892 Pasteur #> 99  -0.8199997483 0.2999315   0.8833584 0.08995892 Pasteur #> 100  0.7369163271 0.2999315   0.8833584 0.08995892 Pasteur #> 101 -0.5403439270 0.2999315   0.8833584 0.08995892 Pasteur #> 102  0.0525570079 0.2999315   0.8833584 0.08995892 Pasteur #> 103  0.4973762860 0.2999315   0.8833584 0.08995892 Pasteur #> 104  0.5434412113 0.2999315   0.8833584 0.08995892 Pasteur #> 105  1.4015048920 0.2999315   0.8833584 0.08995892 Pasteur #> 106  0.5338429790 0.2999315   0.8833584 0.08995892 Pasteur #> 107  1.5005470281 0.2999315   0.8833584 0.08995892 Pasteur #> 108 -0.4353526184 0.2999315   0.8833584 0.08995892 Pasteur #> 109  1.7269399974 0.2999315   0.8833584 0.08995892 Pasteur #> 110 -0.1863115671 0.2999315   0.8833584 0.08995892 Pasteur #> 111  0.7431541299 0.2999315   0.8833584 0.08995892 Pasteur #> 112  0.3345159128 0.2999315   0.8833584 0.08995892 Pasteur #> 113  0.3111963144 0.2999315   0.8833584 0.08995892 Pasteur #> 114  0.6750153713 0.2999315   0.8833584 0.08995892 Pasteur #> 115 -1.3822859470 0.2999315   0.8833584 0.08995892 Pasteur #> 116  0.4299090164 0.2999315   0.8833584 0.08995892 Pasteur #> 117  0.4368182853 0.2999315   0.8833584 0.08995892 Pasteur #> 118 -0.6334339242 0.2999315   0.8833584 0.08995892 Pasteur #> 119 -0.9153928519 0.2999315   0.8833584 0.08995892 Pasteur #> 120 -0.2662544424 0.2999315   0.8833584 0.08995892 Pasteur #> 121 -0.1238362896 0.2999315   0.8833584 0.08995892 Pasteur #> 122 -0.1987871727 0.2999315   0.8833584 0.08995892 Pasteur #> 123  1.5676284956 0.2999315   0.8833584 0.08995892 Pasteur #> 124 -0.2906313821 0.2999315   0.8833584 0.08995892 Pasteur #> 125  0.7125393874 0.2999315   0.8833584 0.08995892 Pasteur #> 126  0.1324998831 0.2999315   0.8833584 0.08995892 Pasteur #> 127  1.1488177518 0.2999315   0.8833584 0.08995892 Pasteur #> 128  0.5559168169 0.2999315   0.8833584 0.08995892 Pasteur #> 129  0.8572606191 0.2999315   0.8833584 0.08995892 Pasteur #> 130 -0.9789254060 0.2999315   0.8833584 0.08995892 Pasteur #> 131  1.4416206448 0.2999315   0.8833584 0.08995892 Pasteur #> 132  0.4542859333 0.2999315   0.8833584 0.08995892 Pasteur #> 133 -1.3845890506 0.2999315   0.8833584 0.08995892 Pasteur #> 134 -0.2883282757 0.2999315   0.8833584 0.08995892 Pasteur #> 135  0.4430560881 0.2999315   0.8833584 0.08995892 Pasteur #> 136  1.2089899229 0.2999315   0.8833584 0.08995892 Pasteur #> 137  1.1942112109 0.2999315   0.8833584 0.08995892 Pasteur #> 138  0.6013102486 0.2999315   0.8833584 0.08995892 Pasteur #> 139  0.2371053667 0.2999315   0.8833584 0.08995892 Pasteur #> 140  1.0053422804 0.2999315   0.8833584 0.08995892 Pasteur #> 141  0.8095640810 0.2999315   0.8833584 0.08995892 Pasteur #> 142 -1.4408264861 0.2999315   0.8833584 0.08995892 Pasteur #> 143  1.3821199900 0.2999315   0.8833584 0.08995892 Pasteur #> 144  2.7284791267 0.2999315   0.8833584 0.08995892 Pasteur #> 145  0.0123440494 0.2999315   0.8833584 0.08995892 Pasteur #> 146  0.8277032180 0.2999315   0.8833584 0.08995892 Pasteur #> 147  0.7862444827 0.2999315   0.8833584 0.08995892 Pasteur #> 148 -1.1325734026 0.2999315   0.8833584 0.08995892 Pasteur #> 149  1.7660956264 0.2999315   0.8833584 0.08995892 Pasteur #> 150 -0.3712457509 0.2999315   0.8833584 0.08995892 Pasteur #> 151  1.8944065607 0.2999315   0.8833584 0.08995892 Pasteur #> 152  0.6098511578 0.2999315   0.8833584 0.08995892 Pasteur #> 153  0.2654170028 0.2999315   0.8833584 0.08995892 Pasteur #> 154 -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 155 -0.6005160981 0.2999315   0.8833584 0.08995892 Pasteur #> 156  0.3562038937 0.2999315   0.8833584 0.08995892 Pasteur #>  #> $`Grant-White` #>           fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1      school #> 1   -0.39525603 0.3152173   0.8801489 0.09936192 Grant-White #> 2   -0.63397248 0.3152173   0.8801489 0.09936192 Grant-White #> 3    0.20062403 0.3152173   0.8801489 0.09936192 Grant-White #> 4   -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 5    0.43519197 0.3152173   0.8801489 0.09936192 Grant-White #> 6    0.31151246 0.3152173   0.8801489 0.09936192 Grant-White #> 7    2.15612913 0.3152173   0.8801489 0.09936192 Grant-White #> 8   -0.29014192 0.3152173   0.8801489 0.09936192 Grant-White #> 9   -0.08369303 0.3152173   0.8801489 0.09936192 Grant-White #> 10  -0.01807396 0.3152173   0.8801489 0.09936192 Grant-White #> 11  -0.34820234 0.3152173   0.8801489 0.09936192 Grant-White #> 12  -2.10215974 0.3152173   0.8801489 0.09936192 Grant-White #> 13  -0.64552119 0.3152173   0.8801489 0.09936192 Grant-White #> 14  -1.46155228 0.3152173   0.8801489 0.09936192 Grant-White #> 15   1.02620880 0.3152173   0.8801489 0.09936192 Grant-White #> 16  -1.05064956 0.3152173   0.8801489 0.09936192 Grant-White #> 17   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 18   0.95822548 0.3152173   0.8801489 0.09936192 Grant-White #> 19  -0.25355303 0.3152173   0.8801489 0.09936192 Grant-White #> 20   1.42141427 0.3152173   0.8801489 0.09936192 Grant-White #> 21  -0.95400523 0.3152173   0.8801489 0.09936192 Grant-White #> 22   1.00295292 0.3152173   0.8801489 0.09936192 Grant-White #> 23   1.21841354 0.3152173   0.8801489 0.09936192 Grant-White #> 24  -1.24987101 0.3152173   0.8801489 0.09936192 Grant-White #> 25  -0.51984663 0.3152173   0.8801489 0.09936192 Grant-White #> 26  -0.04710419 0.3152173   0.8801489 0.09936192 Grant-White #> 27   0.43934048 0.3152173   0.8801489 0.09936192 Grant-White #> 28  -1.03117301 0.3152173   0.8801489 0.09936192 Grant-White #> 29  -0.95022593 0.3152173   0.8801489 0.09936192 Grant-White #> 30  -0.11417634 0.3152173   0.8801489 0.09936192 Grant-White #> 31  -0.40048841 0.3152173   0.8801489 0.09936192 Grant-White #> 32   0.10506359 0.3152173   0.8801489 0.09936192 Grant-White #> 33  -0.33541128 0.3152173   0.8801489 0.09936192 Grant-White #> 34  -1.55565961 0.3152173   0.8801489 0.09936192 Grant-White #> 35  -0.84420068 0.3152173   0.8801489 0.09936192 Grant-White #> 36   0.07802839 0.3152173   0.8801489 0.09936192 Grant-White #> 37   0.20116597 0.3152173   0.8801489 0.09936192 Grant-White #> 38  -2.52639546 0.3152173   0.8801489 0.09936192 Grant-White #> 39  -0.69149096 0.3152173   0.8801489 0.09936192 Grant-White #> 40   2.02343787 0.3152173   0.8801489 0.09936192 Grant-White #> 41   0.97338078 0.3152173   0.8801489 0.09936192 Grant-White #> 42   0.42040588 0.3152173   0.8801489 0.09936192 Grant-White #> 43  -0.92605892 0.3152173   0.8801489 0.09936192 Grant-White #> 44  -0.56690032 0.3152173   0.8801489 0.09936192 Grant-White #> 45   0.12021889 0.3152173   0.8801489 0.09936192 Grant-White #> 46   0.62108043 0.3152173   0.8801489 0.09936192 Grant-White #> 47  -1.34219405 0.3152173   0.8801489 0.09936192 Grant-White #> 48   0.16258210 0.3152173   0.8801489 0.09936192 Grant-White #> 49  -0.03231810 0.3152173   0.8801489 0.09936192 Grant-White #> 50   0.24444031 0.3152173   0.8801489 0.09936192 Grant-White #> 51  -0.74431900 0.3152173   0.8801489 0.09936192 Grant-White #> 52  -0.43798842 0.3152173   0.8801489 0.09936192 Grant-White #> 53  -1.85297845 0.3152173   0.8801489 0.09936192 Grant-White #> 54  -0.82563527 0.3152173   0.8801489 0.09936192 Grant-White #> 55   1.20039010 0.3152173   0.8801489 0.09936192 Grant-White #> 56  -0.33287433 0.3152173   0.8801489 0.09936192 Grant-White #> 57   0.01996797 0.3152173   0.8801489 0.09936192 Grant-White #> 58   1.67582800 0.3152173   0.8801489 0.09936192 Grant-White #> 59  -0.71906808 0.3152173   0.8801489 0.09936192 Grant-White #> 60  -0.20504632 0.3152173   0.8801489 0.09936192 Grant-White #> 61   1.96214007 0.3152173   0.8801489 0.09936192 Grant-White #> 62  -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 63  -0.35343474 0.3152173   0.8801489 0.09936192 Grant-White #> 64  -1.95809255 0.3152173   0.8801489 0.09936192 Grant-White #> 65  -1.36021750 0.3152173   0.8801489 0.09936192 Grant-White #> 66   0.08595623 0.3152173   0.8801489 0.09936192 Grant-White #> 67  -0.23407652 0.3152173   0.8801489 0.09936192 Grant-White #> 68   0.67805696 0.3152173   0.8801489 0.09936192 Grant-White #> 69  -0.42951863 0.3152173   0.8801489 0.09936192 Grant-White #> 70  -0.69203290 0.3152173   0.8801489 0.09936192 Grant-White #> 71  -0.71583072 0.3152173   0.8801489 0.09936192 Grant-White #> 72  -0.19603459 0.3152173   0.8801489 0.09936192 Grant-White #> 73  -0.36767888 0.3152173   0.8801489 0.09936192 Grant-White #> 74   1.77425660 0.3152173   0.8801489 0.09936192 Grant-White #> 75  -0.67924187 0.3152173   0.8801489 0.09936192 Grant-White #> 76   0.07603337 0.3152173   0.8801489 0.09936192 Grant-White #> 77   1.49895127 0.3152173   0.8801489 0.09936192 Grant-White #> 78  -0.78813525 0.3152173   0.8801489 0.09936192 Grant-White #> 79  -1.35643816 0.3152173   0.8801489 0.09936192 Grant-White #> 80  -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 81   1.10806701 0.3152173   0.8801489 0.09936192 Grant-White #> 82   1.04768034 0.3152173   0.8801489 0.09936192 Grant-White #> 83   1.02242947 0.3152173   0.8801489 0.09936192 Grant-White #> 84   0.38236395 0.3152173   0.8801489 0.09936192 Grant-White #> 85   0.56447306 0.3152173   0.8801489 0.09936192 Grant-White #> 86   0.78803426 0.3152173   0.8801489 0.09936192 Grant-White #> 87   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 88   0.40383552 0.3152173   0.8801489 0.09936192 Grant-White #> 89  -0.44376277 0.3152173   0.8801489 0.09936192 Grant-White #> 90   0.08126577 0.3152173   0.8801489 0.09936192 Grant-White #> 91   0.14833793 0.3152173   0.8801489 0.09936192 Grant-White #> 92   0.53922219 0.3152173   0.8801489 0.09936192 Grant-White #> 93   0.53598481 0.3152173   0.8801489 0.09936192 Grant-White #> 94  -0.17024174 0.3152173   0.8801489 0.09936192 Grant-White #> 95   0.43610310 0.3152173   0.8801489 0.09936192 Grant-White #> 96   2.22843366 0.3152173   0.8801489 0.09936192 Grant-White #> 97   1.36480696 0.3152173   0.8801489 0.09936192 Grant-White #> 98   0.65135298 0.3152173   0.8801489 0.09936192 Grant-White #> 99   1.69439338 0.3152173   0.8801489 0.09936192 Grant-White #> 100 -0.15745068 0.3152173   0.8801489 0.09936192 Grant-White #> 101 -0.27680894 0.3152173   0.8801489 0.09936192 Grant-White #> 102  1.80473991 0.3152173   0.8801489 0.09936192 Grant-White #> 103 -0.03286005 0.3152173   0.8801489 0.09936192 Grant-White #> 104  0.21541008 0.3152173   0.8801489 0.09936192 Grant-White #> 105  0.63586648 0.3152173   0.8801489 0.09936192 Grant-White #> 106 -0.44122580 0.3152173   0.8801489 0.09936192 Grant-White #> 107  0.24389836 0.3152173   0.8801489 0.09936192 Grant-White #> 108  0.97392270 0.3152173   0.8801489 0.09936192 Grant-White #> 109  1.00619031 0.3152173   0.8801489 0.09936192 Grant-White #> 110  0.95967859 0.3152173   0.8801489 0.09936192 Grant-White #> 111  2.10529611 0.3152173   0.8801489 0.09936192 Grant-White #> 112  0.95012491 0.3152173   0.8801489 0.09936192 Grant-White #> 113  1.14033462 0.3152173   0.8801489 0.09936192 Grant-White #> 114 -0.41527450 0.3152173   0.8801489 0.09936192 Grant-White #> 115  0.50263334 0.3152173   0.8801489 0.09936192 Grant-White #> 116  0.00518188 0.3152173   0.8801489 0.09936192 Grant-White #> 117 -0.30961846 0.3152173   0.8801489 0.09936192 Grant-White #> 118 -0.38570232 0.3152173   0.8801489 0.09936192 Grant-White #> 119 -0.88747505 0.3152173   0.8801489 0.09936192 Grant-White #> 120 -1.11340047 0.3152173   0.8801489 0.09936192 Grant-White #> 121 -0.22830216 0.3152173   0.8801489 0.09936192 Grant-White #> 122  0.16781447 0.3152173   0.8801489 0.09936192 Grant-White #> 123 -1.16622845 0.3152173   0.8801489 0.09936192 Grant-White #> 124 -0.45277447 0.3152173   0.8801489 0.09936192 Grant-White #> 125 -0.69527031 0.3152173   0.8801489 0.09936192 Grant-White #> 126  1.16558552 0.3152173   0.8801489 0.09936192 Grant-White #> 127 -0.49081643 0.3152173   0.8801489 0.09936192 Grant-White #> 128  0.45412656 0.3152173   0.8801489 0.09936192 Grant-White #> 129 -0.75910506 0.3152173   0.8801489 0.09936192 Grant-White #> 130 -0.46232819 0.3152173   0.8801489 0.09936192 Grant-White #> 131  1.33631868 0.3152173   0.8801489 0.09936192 Grant-White #> 132 -0.78236094 0.3152173   0.8801489 0.09936192 Grant-White #> 133  0.11407532 0.3152173   0.8801489 0.09936192 Grant-White #> 134 -0.26111172 0.3152173   0.8801489 0.09936192 Grant-White #> 135 -0.58492377 0.3152173   0.8801489 0.09936192 Grant-White #> 136 -1.40872427 0.3152173   0.8801489 0.09936192 Grant-White #> 137 -0.24308825 0.3152173   0.8801489 0.09936192 Grant-White #> 138 -0.17601610 0.3152173   0.8801489 0.09936192 Grant-White #> 139 -0.74486092 0.3152173   0.8801489 0.09936192 Grant-White #> 140 -0.13419481 0.3152173   0.8801489 0.09936192 Grant-White #> 141 -0.74809830 0.3152173   0.8801489 0.09936192 Grant-White #> 142 -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 143  0.88737403 0.3152173   0.8801489 0.09936192 Grant-White #> 144 -0.05665784 0.3152173   0.8801489 0.09936192 Grant-White #> 145  0.58303847 0.3152173   0.8801489 0.09936192 Grant-White #>  #> attr(,\"fsT\") #> attr(,\"fsT\")$Pasteur #>            fs_f1 #> fs_f1 0.08995892 #>  #> attr(,\"fsT\")$`Grant-White` #>            fs_f1 #> fs_f1 0.09936192 #>  #> attr(,\"fsL\") #> attr(,\"fsL\")$Pasteur #>              f1 #> fs_f1 0.8833584 #>  #> attr(,\"fsL\")$`Grant-White` #>              f1 #> fs_f1 0.8801489 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #> fs_f1  #>     0  #>  #> attr(,\"fsb\")$`Grant-White` #> fs_f1  #>     0  #>  #> attr(,\"scoring_matrix\") #> attr(,\"scoring_matrix\")$Pasteur #>         [,1]      [,2]      [,3] #> f1 0.2280246 0.3381964 0.2740895 #>  #> attr(,\"scoring_matrix\")$`Grant-White` #>         [,1]      [,2]      [,3] #> f1 0.3580747 0.2682886 0.2662936 #>"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":null,"dir":"Reference","previous_headings":"","what":"Grand Standardized Solution — grand_standardized_solution","title":"Grand Standardized Solution — grand_standardized_solution","text":"Grand standardized solution two-stage path analysis model.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grand Standardized Solution — grand_standardized_solution","text":"","code":"grand_standardized_solution(   object,   model_list = NULL,   se = TRUE,   acov_par = NULL,   free_list = NULL,   level = 0.95 )  grandStandardizedSolution(   object,   model_list = NULL,   se = TRUE,   acov_par = NULL,   free_list = NULL,   level = 0.95 )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grand Standardized Solution — grand_standardized_solution","text":"object object class lavaan. model_list list string variable describing structural path model, lavaan syntax. se Boolean variable. TRUE, standard errors grand standardized parameters computed. acov_par asymptotic variance-covariance matrix fitted model object. free_list list model matrices indicate position free parameters parameter vector. level confidence level required.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Grand Standardized Solution — grand_standardized_solution","text":"matrix standardized model parameters standard errors.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Grand Standardized Solution — grand_standardized_solution","text":"","code":"library(lavaan)  ## A single-group, two-factor example mod1 <- '    # latent variables      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4    # regressions      dem60 ~ ind60 ' fit1 <- sem(model = mod1,           data  = PoliticalDemocracy) grand_standardized_solution(fit1) #> The grand standardized solution is equivalent to lavaan::standardizedSolution() for a model with a single group. #>     lhs op   rhs exo group block label est.std  se     z pvalue ci.lower #> 8 dem60  ~ ind60   0     1     1          0.46 0.1 4.593      0    0.264 #>   ci.upper #> 8    0.657  ## A single-group, three-factor example mod2 <- '     # latent variables       ind60 =~ x1 + x2 + x3       dem60 =~ y1 + y2 + y3 + y4       dem65 =~ y5 + y6 + y7 + y8     # regressions       dem60 ~ ind60       dem65 ~ ind60 + dem60 ' fit2 <- sem(model = mod2,             data  = PoliticalDemocracy) grand_standardized_solution(fit2) #> The grand standardized solution is equivalent to lavaan::standardizedSolution() for a model with a single group. #>      lhs op   rhs exo group block label est.std    se      z pvalue ci.lower #> 12 dem60  ~ ind60   0     1     1         0.448 0.102  4.393  0.000    0.248 #> 13 dem65  ~ ind60   0     1     1         0.146 0.070  2.071  0.038    0.008 #> 14 dem65  ~ dem60   0     1     1         0.913 0.048 19.120  0.000    0.819 #>    ci.upper #> 12    0.648 #> 13    0.283 #> 14    1.006  ## A multigroup, two-factor example mod3 <- '   # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9   # regressions     visual ~ c(b1, b1) * speed ' fit3 <- sem(mod3, data = HolzingerSwineford1939,             group = \"school\",             group.equal = c(\"loadings\", \"intercepts\")) grand_standardized_solution(fit3) #>       lhs op   rhs exo group block label est.std    se     z pvalue ci.lower #> 7  visual  ~ speed   0     1     1    b1   0.431 0.073 5.867      0    0.287 #> 30 visual  ~ speed   0     2     2    b1   0.431 0.073 5.867      0    0.287 #>    ci.upper #> 7     0.575 #> 30    0.575  ## A multigroup, three-factor example mod4 <- '   # latent variable definitions     visual =~ x1 + x2 + x3     textual =~ x4 + x5 + x6     speed =~ x7 + x8 + x9    # regressions     visual ~ c(b1, b1) * textual + c(b2, b2) * speed ' fit4 <- sem(mod4, data = HolzingerSwineford1939,             group = \"school\",             group.equal = c(\"loadings\", \"intercepts\")) grand_standardized_solution(fit4) #>       lhs op     rhs exo group block label est.std    se     z pvalue ci.lower #> 10 visual  ~ textual   0     1     1    b1   0.419 0.073 5.704      0    0.275 #> 11 visual  ~   speed   0     1     1    b2   0.324 0.078 4.145      0    0.171 #> 46 visual  ~ textual   0     2     2    b1   0.419 0.073 5.704      0    0.275 #> 47 visual  ~   speed   0     2     2    b2   0.324 0.078 4.145      0    0.171 #>    ci.upper #> 10    0.563 #> 11    0.477 #> 46    0.563 #> 47    0.477"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":null,"dir":"Reference","previous_headings":"","what":"Two-Stage Path Analysis — tspa","title":"Two-Stage Path Analysis — tspa","text":"Fit two-stage path analysis (2S-PA) model.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Two-Stage Path Analysis — tspa","text":"","code":"tspa(   model,   data,   reliability = NULL,   se = \"standard\",   se_fs = NULL,   fsT = NULL,   fsL = NULL,   fsb = NULL,   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Two-Stage Path Analysis — tspa","text":"model string variable describing structural path model, lavaan syntax. data data frame containing factor scores. reliability numeric vector representing reliability indexes latent factor. Currently tspa() support reliability argument. Please use se. se Deprecated avoid conflict argument name lavaan::lavaan(). se_fs numeric vector representing standard errors factor score variable single-group 2S-PA. list data frame storing standard errors group latent factor multigroup 2S-PA. fsT error variance-covariance matrix factor scores, can obtained output get_fs() using attr() argument = \"fsT\". fsL matrix loadings cross-loadings latent variables factor scores fs, can obtained output get_fs() using attr() argument = \"fsL\". details see multiple-factors vignette: vignette(\"multiple-factors\", package = \"R2spa\"). fsb vector intercepts factor scores fs, can obtained output get_fs() using attr() argument = \"fsb\". ... Additional arguments passed sem. See lavOptions complete list.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Two-Stage Path Analysis — tspa","text":"object class lavaan, attribute tspaModel contains model syntax.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Two-Stage Path Analysis — tspa","text":"","code":"library(lavaan)  # single-group, two-factor example, factor scores obtained separately # get factor scores fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                        model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                        model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60) # tspa model tspa(model = \"dem60 ~ ind60\", data = fs_dat,      se_fs = c(ind60 = fs_dat_ind60[1, \"fs_ind60_se\"],                dem60 = fs_dat_dem60[1, \"fs_dem60_se\"])) #> lavaan 0.6.17 ended normally after 17 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         3 #>  #>   Number of observations                            75 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0  # single-group, three-factor example mod2 <- \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     dem65 =~ y5 + y6 + y7 + y8 \" fs_dat2 <- get_fs(PoliticalDemocracy, model = mod2, std.lv = TRUE) tspa(model = \"dem60 ~ ind60               dem65 ~ ind60 + dem60\",      data = fs_dat2,      fsT = attr(fs_dat2, \"fsT\"),      fsL = attr(fs_dat2, \"fsL\")) #> lavaan 0.6.17 ended normally after 21 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         6 #>  #>   Number of observations                            75 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0  # multigroup, two-factor example mod3 <- \"   # latent variables     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9 \" fs_dat3 <- get_fs(HolzingerSwineford1939, model = mod3, std.lv = TRUE,                   group = \"school\") tspa(model = \"visual ~ speed\",      data = fs_dat3,      fsT = attr(fs_dat3, \"fsT\"),      fsL = attr(fs_dat3, \"fsL\"),      group = \"school\") #> lavaan 0.6.17 ended normally after 28 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0 #>   Test statistic for each group: #>     Pasteur                                      0.000 #>     Grant-White                                  0.000  # multigroup, three-factor example mod4 <- \"   # latent variables     visual =~ x1 + x2 + x3     textual =~ x4 + x5 + x6     speed =~ x7 + x8 + x9 \" fs_dat4 <- get_fs(HolzingerSwineford1939, model = mod4, std.lv = TRUE,                   group = \"school\") tspa(model = \"visual ~ speed               textual ~ visual + speed\",      data = fs_dat4,      fsT = attr(fs_dat4, \"fsT\"),      fsL = attr(fs_dat4, \"fsL\"),      group = \"school\") #> lavaan 0.6.17 ended normally after 39 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        18 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0 #>   Test statistic for each group: #>     Pasteur                                      0.000 #>     Grant-White                                  0.000  # get factor scores fs_dat_visual <- get_fs(data = HolzingerSwineford1939,                         model = \"visual =~ x1 + x2 + x3\",                         group = \"school\") fs_dat_speed <- get_fs(data = HolzingerSwineford1939,                        model = \"speed =~ x7 + x8 + x9\",                        group = \"school\") fs_hs <- cbind(do.call(rbind, fs_dat_visual),                do.call(rbind, fs_dat_speed))  # tspa model tspa(model = \"visual ~ speed\",      data = fs_hs,      se_fs = data.frame(visual = c(0.3391326, 0.311828),                         speed = c(0.2786875, 0.2740507)),      group = \"school\",      group.equal = \"regressions\") #> lavaan 0.6.17 ended normally after 19 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>   Number of equality constraints                     1 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.014 #>   Degrees of freedom                                 1 #>   P-value (Chi-square)                           0.907 #>   Test statistic for each group: #>     Pasteur                                      0.010 #>     Grant-White                                  0.003  # manually adding equality constraints on the regression coefficients tspa(model = \"visual ~ c(b1, b1) * speed\",      data = fs_hs,      se_fs = list(visual = c(0.3391326, 0.311828),                   speed = c(0.2786875, 0.2740507)),      group = \"school\") #> lavaan 0.6.17 ended normally after 19 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>   Number of equality constraints                     1 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.014 #>   Degrees of freedom                                 1 #>   P-value (Chi-square)                           0.907 #>   Test statistic for each group: #>     Pasteur                                      0.010 #>     Grant-White                                  0.003"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"Fit two-stage path analysis (2S-PA) model.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"","code":"tspa_mx_model(mx_model, data, mat_ld, mat_vc, ...)"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"mx_model model object class OpenMx::MxRAMModel, created OpenMx::mxModel() umx package. structural model part. data data frame containing factor scores. mat_ld \\(p \\times p\\) matrix indicating loadings factor scores latent variables. ith row indicate loadings ith factor score variable latent variables. one following: matrix created OpenMx::mxMatrix() loading values. named numeric matrix, rownames column names matching factor score latent variables. named character matrix, rownames column names matching factor score latent variables, character values indicating variable names data corresponding loadings. mat_vc Similar mat_ld error variance-covariance matrix factor scores. ... Additional arguments passed OpenMx::mxModel().","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"object class OpenMx::MxModel. Note model run.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"","code":"library(mirt) #> Loading required package: stats4 #> Loading required package: lattice library(umx) #> Loading required package: OpenMx #> To take full advantage of multiple cores, use: #>   mxOption(key='Number of Threads', value=parallel::detectCores()) #now #>   Sys.setenv(OMP_NUM_THREADS=parallel::detectCores()) #before library(OpenMx) #> For an overview type '?umx' #>  #> Attaching package: ‘umx’ #> The following object is masked from ‘package:stats’: #>  #>     loadings library(OpenMx) # Simulate data with mirt set.seed(1324) num_obs <- 100 # Simulate theta eta <- MASS::mvrnorm(num_obs, mu = c(0, 0), Sigma = diag(c(1, 1 - 0.5^2)),                      empirical = TRUE) th1 <- eta[, 1] th2 <- -1 + 0.5 * th1 + eta[, 2] # items and response data a1 <- matrix(1, 10) d1 <- matrix(rnorm(10)) a2 <- matrix(runif(10, min = 0.5, max = 1.5)) d2 <- matrix(rnorm(10)) dat1 <- mirt::simdata(a = a1, d = d1,                       N = num_obs, itemtype = \"2PL\", Theta = th1) dat2 <- mirt::simdata(a = a2, d = d2, N = num_obs,                       itemtype = \"2PL\", Theta = th2) # Factor scores mod1 <- mirt(dat1, model = 1, itemtype = \"Rasch\", verbose = FALSE) mod2 <- mirt(dat2, model = 1, itemtype = \"2PL\", verbose = FALSE) fs1 <- fscores(mod1, full.scores.SE = TRUE) fs2 <- fscores(mod2, full.scores.SE = TRUE) # Combine factor scores and standard errors into data set fs_dat <- as.data.frame(cbind(fs1, fs2)) names(fs_dat) <- c(\"fs1\", \"se_fs1\", \"fs2\", \"se_fs2\") # Compute reliability and error variances fs_dat <- within(fs_dat, expr = {   rel_fs1 <- 1 - se_fs1^2   rel_fs2 <- 1 - se_fs2^2   ev_fs1 <- se_fs1^2 * (1 - se_fs1^2)   ev_fs2 <- se_fs2^2 * (1 - se_fs2^2) }) # OpenMx model (from umx so that lavaan syntax can be used) fsreg_umx <- umxLav2RAM(   \"     fs2 ~ fs1     fs2 + fs1 ~ 1   \",   printTab = FALSE) #>  #> ?plot.MxModel options: std, means, digits, strip_zero, file, splines=T/F/ortho,..., min=, max =, same = , fixed, resid= 'circle|line|none' # Prepare loading and error covariance matrices cross_load <- matrix(c(\"rel_fs2\", NA, NA, \"rel_fs1\"), nrow = 2) |>   `dimnames<-`(rep(list(c(\"fs2\", \"fs1\")), 2)) err_cov <- matrix(c(\"ev_fs2\", NA, NA, \"ev_fs1\"), nrow = 2) |>   `dimnames<-`(rep(list(c(\"fs2\", \"fs1\")), 2)) # Create 2S-PA model (with definition variables) tspa_mx <-   tspa_mx_model(fsreg_umx,     data = fs_dat,     mat_ld = cross_load,     mat_vc = err_cov   ) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx) #> Running 2SPAD with 5 parameters # Summarize the results summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>           name matrix row col     Estimate Std.Error A #> 1   fs1_to_fs2   m1.A fs2 fs1  0.526025404 0.2351929   #> 2 fs2_with_fs2   m1.S fs2 fs2  0.857316415 0.2253150   #> 3 fs1_with_fs1   m1.S fs1 fs1  0.541308628 0.1472530   #> 4   one_to_fs2   m1.M   1 fs2 -0.003189929 0.1234776   #> 5   one_to_fs1   m1.M   1 fs1  0.003287301 0.1002648   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                    395              453.4113 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 100/400 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -336.5887               463.4113                 464.0496 #> BIC:     -1365.6310               476.4371                 460.6459 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-01-16 07:10:44  #> Wall clock time: 0.1383488 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.11  #> Need help?  See help(mxSummary)  #>"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Diagnostic plots of fitted 2S-PA model — tspa_plot","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"Diagnostic plots fitted 2S-PA model","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"","code":"tspa_plot(   tspa_fit,   title = NULL,   label_x = NULL,   label_y = NULL,   abbreviation = TRUE,   fscores_type = c(\"original\", \"lavaan\"),   ask = FALSE,   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"tspa_fit object class lavaan, representing output generated tspa() function. title Character. Set name scatter plot. default value \"Scatterplot\". label_x Character. Set name x-axis. default value \"fs_\" followed variable names. label_y Character. Set name y-axis. default value \"fs_\" followed variable names. abbreviation Logic input. FALSE indicated, group name shown full. default setting TRUE. fscores_type Character. Set type factor score input. default setting using factor score observed data (.e., output get_fs()). fscore_type = \"est\", use output lavaan::lavPredict(). ask Logic input. TRUE indicated, user asked plot generated. default setting 'False'. ... Additional arguments passed plot. See plot list.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"scatterplot factor scores, residual plot.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"","code":"library(lavaan) model <- \" # latent variable definitions ind60 =~ x1 + x2 + x3 dem60 =~ y1 + a*y2 + b*y3 + c*y4 # regressions dem60 ~ ind60 \" fs_dat_ind60 <- get_fs(   data = PoliticalDemocracy,   model = \"ind60 =~ x1 + x2 + x3\" ) fs_dat_dem60 <- get_fs(   data = PoliticalDemocracy,   model = \"dem60 =~ y1 + y2 + y3 + y4\" ) fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60)  tspa_fit <- tspa(   model = \"dem60 ~ ind60\",   data = fs_dat,   se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472) ) tspa_plot(tspa_fit)"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/vcov_corrected.html","id":null,"dir":"Reference","previous_headings":"","what":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","title":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","text":"First-order correction sampling covariance 2S-PA estimates","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/vcov_corrected.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","text":"","code":"vcov_corrected(tspa_fit, vfsLT, which_free = NULL, ...)"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/vcov_corrected.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","text":"tspa_fit fitted model tspa(). vfsLT sampling covariance matrix fsL fsT, can obtained get_fs() argument vfsLT = TRUE. which_free optional numeric vector indicating parameters fsL fsT free. parameters ordered fsL matrix lower-triangular part fsT, columns. example, two-factor model, fsL fsT 2 x 2 matrices, error covariance two factor scores (.e., [2, 1] element fsT) index 6. ... Currently used.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/vcov_corrected.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","text":"corrected covariance matrix dimension vcov(tspa_fit).","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"r2spa-003","dir":"Changelog","previous_headings":"","what":"R2spa 0.0.3","title":"R2spa 0.0.3","text":"Add function tspa_plot() bivariate residual plots (#23) get_fs() gains argument corrected_fsT computing corrected error estimates (#50) New function vcov_corrected() computing corrected SEs (#39) New function get_fs_lavaan() computing factor scores relevant matrices directly lavaan output (#61) Initial support 2S-PA OpenMx tspa_mx() Update naming relevant matrices computing factor scores: fsT: error covariance factor scores fsL: loading matrix factor scores fsb: intercepts factor scores scoring_matrix: weights computing factor scores items New vignettes : Corrected error variance factor scores (#50) Corrected standard errors incorporating uncertainty measurement parameters factor scores (#39) Using 2S-PA EFA scores Using 2S-PA OpenMx definition variables (PR #57) Latent interaction categorical indicators (#27) Growth modeling Better error messages tspa() (#53) Support mean structure growth model (#36, #19) Clean code lintr (#33)","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"r2spa-002","dir":"Changelog","previous_headings":"","what":"R2spa 0.0.2","title":"R2spa 0.0.2","text":"Use pkgdown create website, GitHub action (#22) get_fs() now returns list multi-group models (#29). New function grandStandardizedSolution() computes standardized solution based grand mean grand SD (#13). tspa() gains argument vc cross_loadings, useful factor scores obtained multi-factor models (#7). See vignette(\"multiple-factors\").","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"r2spa-001","dir":"Changelog","previous_headings":"","what":"R2spa 0.0.1","title":"R2spa 0.0.1","text":"Work--Progress! 0.0.1 version","code":""}]
