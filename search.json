[{"path":"https://gengrui-zhang.github.io/R2spa/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 R2spa authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"single-group","dir":"Articles","previous_headings":"","what":"Single Group","title":"Grand Standardized Coefficients","text":"single groups, standardized solution can obtained first obtaining latent variable covariance matrix: \\[   \\begin{aligned}     \\boldsymbol{\\mathbf{\\eta }}& = \\boldsymbol{\\mathbf{\\alpha }}+ \\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}} + \\boldsymbol{\\mathbf{B}} \\boldsymbol{\\mathbf{\\eta }}+ \\boldsymbol{\\mathbf{\\zeta }}\\\\     (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}}) \\boldsymbol{\\mathbf{\\eta }}& = \\boldsymbol{\\mathbf{\\alpha }}+ \\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}} + \\boldsymbol{\\mathbf{\\zeta }}\\\\     (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}}) \\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}) (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^\\top & = \\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}}) + \\boldsymbol{\\mathbf{\\Psi }}\\\\     \\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}) & = (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^{-1} [\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}}) + \\boldsymbol{\\mathbf{\\Psi}}] {(\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^\\top}^{-1}   \\end{aligned} \\] standardized estimates \\(\\boldsymbol{\\mathbf{B}}\\) matrix obtained \\[ \\boldsymbol{\\mathbf{B}}_s = \\boldsymbol{\\mathbf{S}}_\\eta^{-1} \\boldsymbol{\\mathbf{B}} \\boldsymbol{\\mathbf{S}}_\\eta^{1} \\] \\(\\boldsymbol{\\mathbf{S}}_\\eta\\) diagonal matrix containing square root diagonal elements \\(\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}})\\)","code":"myModel <- '    # latent variables      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4    # regressions      dem60 ~ ind60 ' fit <- sem(model = myModel,            data  = PoliticalDemocracy) # Latent variable covariances lavInspect(fit, \"cov.lv\") #>       ind60 dem60 #> ind60 0.449       #> dem60 0.646 4.382 # Using R2spa::veta()` fit_est <- lavInspect(fit, \"est\") R2spa:::veta(fit_est$beta, psi = fit_est$psi) #>           ind60     dem60 #> ind60 0.4485415 0.6455929 #> dem60 0.6455929 4.3824834 S_eta <- diag(   sqrt(diag(     R2spa:::veta(fit_est$beta, psi = fit_est$psi)   )) ) solve(S_eta) %*% fit_est$beta %*% S_eta #>           [,1] [,2] #> [1,] 0.0000000    0 #> [2,] 0.4604657    0 # Compare to lavaan standardizedSolution(fit)[8, ] #>     lhs op   rhs est.std  se     z pvalue ci.lower ci.upper #> 8 dem60  ~ ind60    0.46 0.1 4.593      0    0.264    0.657"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"multiple-groups","dir":"Articles","previous_headings":"","what":"Multiple Groups","title":"Grand Standardized Coefficients","text":"","code":"reg <- '   # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9    # regressions     visual ~ c(b1, b1) * speed ' reg_fit <- sem(reg, data = HolzingerSwineford1939,                group = \"school\",                group.equal = c(\"loadings\", \"intercepts\"))"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"separate-standardization-by-group","dir":"Articles","previous_headings":"Multiple Groups","what":"Separate standardization by group","title":"Grand Standardized Coefficients","text":"","code":"standardizedSolution(reg_fit, type = \"std.lv\") |>   subset(subset = label == \"b1\") #>       lhs op   rhs group label est.std    se     z pvalue ci.lower ci.upper #> 7  visual  ~ speed     1    b1   0.369 0.073 5.032      0    0.226    0.513 #> 30 visual  ~ speed     2    b1   0.496 0.086 5.768      0    0.327    0.664 # Compare to doing it by hand reg_fit_est <- lavInspect(reg_fit, what = \"est\") # Group 1: S_eta1 <- diag(   sqrt(diag(     R2spa:::veta(reg_fit_est[[1]]$beta, psi = reg_fit_est[[1]]$psi)   )) ) solve(S_eta1) %*% reg_fit_est[[1]]$beta %*% S_eta1 #>      [,1]      [,2] #> [1,]    0 0.3694845 #> [2,]    0 0.0000000 # Group 2: S_eta2 <- diag(   sqrt(diag(     R2spa:::veta(reg_fit_est[[2]]$beta, psi = reg_fit_est[[2]]$psi)   )) ) solve(S_eta2) %*% reg_fit_est[[2]]$beta %*% S_eta2 #>      [,1]      [,2] #> [1,]    0 0.4958876 #> [2,]    0 0.0000000"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"grand-standardization","dir":"Articles","previous_headings":"Multiple Groups","what":"Grand standardization","title":"Grand Standardized Coefficients","text":"group group-specific covariance matrix \\(\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}_g)\\). group-specific latent means \\[\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) = (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^{-1}[\\boldsymbol{\\mathbf{\\alpha }}+ \\Gamma \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{X}})]\\] grand mean \\(\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}) = \\sum_{g = 1}^G n_g \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) / N\\) grand covariance matrix : \\[\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}) = \\frac{1}{N} \\sum_{g = 1}^G n_g \\left\\{\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}_g) + [\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) - \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}})][\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) - \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}})]^\\top \\right\\}\\] need involve mean well function grandStandardizdSolution() automates computation grand standardized coefficients, asymptotic standard error obtained using delta method:","code":"# Latent means lavInspect(reg_fit, what = \"mean.lv\")  # lavaan #> $Pasteur #> visual  speed  #>      0      0  #>  #> $`Grant-White` #> visual  speed  #> -0.204 -0.167 R2spa:::eeta(reg_fit_est[[1]]$beta, alpha = reg_fit_est[[1]]$alpha) #>        intercept #> visual         0 #> speed          0 R2spa:::eeta(reg_fit_est[[2]]$beta, alpha = reg_fit_est[[2]]$alpha) #>         intercept #> visual -0.2036667 #> speed  -0.1666424 # Grand covariance ns <- lavInspect(reg_fit, what = \"nobs\") R2spa:::veta_grand(ns, beta_list = lapply(reg_fit_est, `[[`, \"beta\"),                    psi_list = lapply(reg_fit_est, `[[`, \"psi\"),                    alpha_list = lapply(reg_fit_est, `[[`, \"alpha\")) #>           visual     speed #> visual 0.5497595 0.2085522 #> speed  0.2085522 0.4059395 grandStandardizedSolution(reg_fit) #>       lhs op   rhs exo group block label est.std    se     z pvalue ci.lower #> 7  visual  ~ speed   0     1     1    b1   0.431 0.073 5.867      0    0.287 #> 30 visual  ~ speed   0     2     2    b1   0.431 0.073 5.867      0    0.287 #>    ci.upper #> 7     0.575 #> 30    0.575"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/multiple-factors.html","id":"factor-score","dir":"Articles","previous_headings":"","what":"Factor score","title":"Multi-Factor Measurement Model","text":"CFA model multiple latent factors, even indicator loads one factor, resulting factor scores generally weighted composites indicators. Consider regression score, form \\[\\tilde{\\boldsymbol \\eta} = \\mathbf{}(\\mathbf{y} - \\hat{\\boldsymbol \\mu}) + \\boldsymbol{\\alpha}\\] \\(\\mathbf{} = \\boldsymbol{\\Psi}\\boldsymbol{\\Lambda}^\\top \\hat{\\boldsymbol{\\Sigma}}^{-1}\\) \\(q\\) \\(\\times\\) \\(p\\) matrix, \\(\\hat{\\boldsymbol \\mu} = \\boldsymbol{\\nu} + \\boldsymbol{\\Lambda} \\boldsymbol{\\alpha}\\) \\(\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Psi}\\boldsymbol{\\Lambda}^\\top + \\boldsymbol{\\Theta}\\) model-implied means covariances indicators \\(\\mathbf{y}\\), \\(\\boldsymbol{\\alpha}\\) \\(\\boldsymbol{\\Psi}\\) latent means latent covariances. Therefore, assuming model correctly specified \\(\\mathbf{y} = \\boldsymbol{\\nu} + \\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\boldsymbol{\\varepsilon}\\), \\[   \\begin{aligned}   \\tilde{\\boldsymbol \\eta} & = \\mathbf{}(\\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\boldsymbol{\\varepsilon} - \\boldsymbol{\\Lambda} \\boldsymbol{\\alpha}) + \\boldsymbol{\\alpha} \\\\   & = (\\mathbf{} - \\mathbf{}\\boldsymbol{\\Lambda}) \\boldsymbol{\\alpha} +   \\mathbf{}\\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\mathbf{}\\boldsymbol{\\varepsilon}.   \\end{aligned} \\] consider \\(\\tilde{\\boldsymbol \\eta}\\) indicators \\(\\boldsymbol \\eta\\), can see \\(\\boldsymbol{\\nu}_\\tilde{\\boldsymbol{\\eta}} = (\\mathbf{} - \\mathbf{}\\boldsymbol{\\Lambda}) \\boldsymbol{\\alpha}\\) intercept, \\(\\boldsymbol{\\Lambda}_\\tilde{\\boldsymbol{\\eta}} = \\mathbf{}\\boldsymbol{\\Lambda}\\) \\(q\\) \\(\\times\\) \\(q\\) loading matrix, \\(\\boldsymbol{\\Theta}_\\tilde{\\boldsymbol{\\eta}} = \\mathbf{}\\boldsymbol{\\varepsilon}\\) error covariance matrix. can see \\(\\boldsymbol{\\Lambda}_\\tilde{\\boldsymbol{\\eta}}\\) generally diagonal, following shows: can also use R2spa::get_fs(): Therefore, need specify cross-loadings using 2S-PA. consistent SEM results.","code":"# CFA my_cfa <- \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4 \" cfa_fit <- cfa(model = my_cfa,                data  = PoliticalDemocracy,                std.lv = TRUE) # A matrix pars <- lavInspect(cfa_fit, what = \"est\") lambda_mat <- pars$lambda psi_mat <- pars$psi sigma_mat <- cfa_fit@implied$cov[[1]] ginvsigma <- MASS::ginv(sigma_mat) alambda <- psi_mat %*% crossprod(lambda_mat, ginvsigma %*% lambda_mat) alambda #>            ind60      dem60 #> ind60 0.95538579 0.01834111 #> dem60 0.05816994 0.86888887 (fs_dat <- get_fs(PoliticalDemocracy, model = my_cfa, std.lv = TRUE)) |> head() #>     fs_ind60   fs_dem60 fs_ind60_se fs_dem60_se ind60_by_fs_ind60 #> 1 -0.8101568 -1.3119114   0.1859987   0.3012901         0.9553858 #> 2  0.1888466 -1.3644831   0.1859987   0.3012901         0.9553858 #> 3  1.0960931  1.3107705   0.1859987   0.3012901         0.9553858 #> 4  1.8702043  1.4849083   0.1859987   0.3012901         0.9553858 #> 5  1.2446060  0.9193277   0.1859987   0.3012901         0.9553858 #> 6  0.3348621  0.4886331   0.1859987   0.3012901         0.9553858 #>   ind60_by_fs_dem60 dem60_by_fs_ind60 dem60_by_fs_dem60 evfs_ind60_ind60 #> 1        0.05816994        0.01834111         0.8688889       0.03459552 #> 2        0.05816994        0.01834111         0.8688889       0.03459552 #> 3        0.05816994        0.01834111         0.8688889       0.03459552 #> 4        0.05816994        0.01834111         0.8688889       0.03459552 #> 5        0.05816994        0.01834111         0.8688889       0.03459552 #> 6        0.05816994        0.01834111         0.8688889       0.03459552 #>   evfs_dem60_ind60 evfs_dem60_dem60 #> 1      0.004017388       0.09077571 #> 2      0.004017388       0.09077571 #> 3      0.004017388       0.09077571 #> 4      0.004017388       0.09077571 #> 5      0.004017388       0.09077571 #> 6      0.004017388       0.09077571 tspa_fit <- tspa(model = \"dem60 ~ ind60\", data = fs_dat,                  vc = attr(fs_dat, \"av_efs\"),                   cross_loadings = attr(fs_dat, \"fsA\")) cat(attr(tspa_fit, \"tspaModel\")) #> # latent variables (indicated by factor scores) #>  ind60 =~ 0.955385785456617 * fs_ind60 + 0.0581699407736298 * fs_dem60 #>  dem60 =~ 0.0183411131911884 * fs_ind60 + 0.868888868390615 * fs_dem60 #>  # constrain the errors #>  fs_ind60 ~~ 0.034595517927198 * fs_ind60 #>  fs_dem60 ~~ 0.00401738814566848 * fs_ind60 #>  fs_dem60 ~~ 0.0907757082269275 * fs_dem60 #>  # regressions #>  dem60 ~ ind60 parameterestimates(tspa_fit) #>         lhs op      rhs   est    se     z pvalue ci.lower ci.upper #> 1     ind60 =~ fs_ind60 0.955 0.000    NA     NA    0.955    0.955 #> 2     ind60 =~ fs_dem60 0.058 0.000    NA     NA    0.058    0.058 #> 3     dem60 =~ fs_ind60 0.018 0.000    NA     NA    0.018    0.018 #> 4     dem60 =~ fs_dem60 0.869 0.000    NA     NA    0.869    0.869 #> 5  fs_ind60 ~~ fs_ind60 0.035 0.000    NA     NA    0.035    0.035 #> 6  fs_ind60 ~~ fs_dem60 0.004 0.000    NA     NA    0.004    0.004 #> 7  fs_dem60 ~~ fs_dem60 0.091 0.000    NA     NA    0.091    0.091 #> 8     dem60  ~    ind60 0.460 0.113 4.089      0    0.240    0.681 #> 9     ind60 ~~    ind60 1.000 0.169 5.900      0    0.668    1.332 #> 10    dem60 ~~    dem60 0.788 0.150 5.267      0    0.495    1.081"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/multiple-factors.html","id":"three-factor-model-example","dir":"Articles","previous_headings":"","what":"Three-factor model example","title":"Multi-Factor Measurement Model","text":"can also use R2spa::get_fs(): Therefore, need specify cross-loadings using 2S-PA. Compare SEM:","code":"# CFA cfa_3fac <-  \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     dem65 =~ y5 + y6 + y7 + y8 \" cfa_3fac_fit <- cfa(model = cfa_3fac,                     data  = PoliticalDemocracy,                     std.lv = TRUE) # A matrix pars <- lavInspect(cfa_3fac_fit, what = \"est\") lambda_mat <- pars$lambda psi_mat <- pars$psi sigma_mat <- cfa_3fac_fit@implied$cov[[1]] ginvsigma <- MASS::ginv(sigma_mat) alambda <- psi_mat %*% crossprod(lambda_mat, ginvsigma %*% lambda_mat) alambda #>             ind60        dem60      dem65 #> ind60  0.95064774 -0.005967124 0.02951139 #> dem60 -0.02069724  0.533020047 0.41603787 #> dem65  0.09868528  0.401095944 0.49133377 (fs_dat_3fac <- get_fs(PoliticalDemocracy, model = cfa_3fac, std.lv = TRUE)) |>   head() #>     fs_ind60   fs_dem60    fs_dem65 fs_ind60_se fs_dem60_se fs_dem65_se #> 1 -0.7990475 -1.1571745 -1.13720655   0.1844193   0.2422541   0.2270976 #> 2  0.2152486 -1.0238236 -0.80871922   0.1844193   0.2422541   0.2270976 #> 3  1.1028297  1.3890842  1.46389950   0.1844193   0.2422541   0.2270976 #> 4  1.8585004  1.3163888  1.43045560   0.1844193   0.2422541   0.2270976 #> 5  1.2432985  0.9522026  1.03348589   0.1844193   0.2422541   0.2270976 #> 6  0.3067994  0.1109206  0.05023217   0.1844193   0.2422541   0.2270976 #>   ind60_by_fs_ind60 ind60_by_fs_dem60 ind60_by_fs_dem65 dem60_by_fs_ind60 #> 1         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 2         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 3         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 4         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 5         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 6         0.9506477       -0.02069724        0.09868528      -0.005967124 #>   dem60_by_fs_dem60 dem60_by_fs_dem65 dem65_by_fs_ind60 dem65_by_fs_dem60 #> 1           0.53302         0.4010959        0.02951139         0.4160379 #> 2           0.53302         0.4010959        0.02951139         0.4160379 #> 3           0.53302         0.4010959        0.02951139         0.4160379 #> 4           0.53302         0.4010959        0.02951139         0.4160379 #> 5           0.53302         0.4010959        0.02951139         0.4160379 #> 6           0.53302         0.4010959        0.02951139         0.4160379 #>   dem65_by_fs_dem65 evfs_ind60_ind60 evfs_dem60_ind60 evfs_dem60_dem60 #> 1         0.4913338        0.0340105     0.0003881641       0.05868703 #> 2         0.4913338        0.0340105     0.0003881641       0.05868703 #> 3         0.4913338        0.0340105     0.0003881641       0.05868703 #> 4         0.4913338        0.0340105     0.0003881641       0.05868703 #> 5         0.4913338        0.0340105     0.0003881641       0.05868703 #> 6         0.4913338        0.0340105     0.0003881641       0.05868703 #>   evfs_dem65_ind60 evfs_dem65_dem60 evfs_dem65_dem65 #> 1      0.005026024       0.05337525        0.0515733 #> 2      0.005026024       0.05337525        0.0515733 #> 3      0.005026024       0.05337525        0.0515733 #> 4      0.005026024       0.05337525        0.0515733 #> 5      0.005026024       0.05337525        0.0515733 #> 6      0.005026024       0.05337525        0.0515733 tspa_fit_3fac <- tspa(model = \"dem60 ~ ind60               dem65 ~ ind60 + dem60\",               data = fs_dat_3fac,               vc = attr(fs_dat_3fac, \"av_efs\"),               cross_loadings = attr(fs_dat_3fac, \"fsA\")) cat(attr(tspa_fit_3fac, \"tspaModel\")) #> # latent variables (indicated by factor scores) #>  ind60 =~ 0.950647742844845 * fs_ind60 + -0.0206972362902861 * fs_dem60 + 0.0986852834195757 * fs_dem65 #>  dem60 =~ -0.00596712444175369 * fs_ind60 + 0.533020047181512 * fs_dem60 + 0.401095943690545 * fs_dem65 #>  dem65 =~ 0.0295113941487127 * fs_ind60 + 0.416037872255944 * fs_dem60 + 0.491333767947424 * fs_dem65 #>  # constrain the errors #>  fs_ind60 ~~ 0.0340104951546674 * fs_ind60 #>  fs_dem60 ~~ 0.00038816407039874 * fs_ind60 #>  fs_dem65 ~~ 0.00502602420598859 * fs_ind60 #>  fs_dem60 ~~ 0.0586870313996517 * fs_dem60 #>  fs_dem65 ~~ 0.0533752457536214 * fs_dem60 #>  fs_dem65 ~~ 0.051573299369388 * fs_dem65 #>  # regressions #>  dem60 ~ ind60 #>               dem65 ~ ind60 + dem60 parameterestimates(tspa_fit_3fac) #>         lhs op      rhs    est    se      z pvalue ci.lower ci.upper #> 1     ind60 =~ fs_ind60  0.951 0.000     NA     NA    0.951    0.951 #> 2     ind60 =~ fs_dem60 -0.021 0.000     NA     NA   -0.021   -0.021 #> 3     ind60 =~ fs_dem65  0.099 0.000     NA     NA    0.099    0.099 #> 4     dem60 =~ fs_ind60 -0.006 0.000     NA     NA   -0.006   -0.006 #> 5     dem60 =~ fs_dem60  0.533 0.000     NA     NA    0.533    0.533 #> 6     dem60 =~ fs_dem65  0.401 0.000     NA     NA    0.401    0.401 #> 7     dem65 =~ fs_ind60  0.030 0.000     NA     NA    0.030    0.030 #> 8     dem65 =~ fs_dem60  0.416 0.000     NA     NA    0.416    0.416 #> 9     dem65 =~ fs_dem65  0.491 0.000     NA     NA    0.491    0.491 #> 10 fs_ind60 ~~ fs_ind60  0.034 0.000     NA     NA    0.034    0.034 #> 11 fs_ind60 ~~ fs_dem60  0.000 0.000     NA     NA    0.000    0.000 #> 12 fs_ind60 ~~ fs_dem65  0.005 0.000     NA     NA    0.005    0.005 #> 13 fs_dem60 ~~ fs_dem60  0.059 0.000     NA     NA    0.059    0.059 #> 14 fs_dem60 ~~ fs_dem65  0.053 0.000     NA     NA    0.053    0.053 #> 15 fs_dem65 ~~ fs_dem65  0.052 0.000     NA     NA    0.052    0.052 #> 16    dem60  ~    ind60  0.448 0.114  3.937  0.000    0.225    0.671 #> 17    dem65  ~    ind60  0.146 0.069  2.112  0.035    0.010    0.281 #> 18    dem65  ~    dem60  0.913 0.073 12.435  0.000    0.769    1.057 #> 19    ind60 ~~    ind60  1.000 0.169  5.902  0.000    0.668    1.332 #> 20    dem60 ~~    dem60  0.799 0.153  5.224  0.000    0.499    1.099 #> 21    dem65 ~~    dem65  0.026 0.043  0.620  0.535   -0.057    0.110 sem_3fac <- sem(\"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     dem65 =~ y5 + y6 + y7 + y8   # structural model     dem60 ~ ind60     dem65 ~ ind60 + dem60   \",   data = PoliticalDemocracy ) standardizedSolution(sem_3fac) #>      lhs op   rhs est.std    se      z pvalue ci.lower ci.upper #> 1  ind60 =~    x1   0.920 0.023 39.823  0.000    0.874    0.965 #> 2  ind60 =~    x2   0.973 0.017 58.858  0.000    0.941    1.006 #> 3  ind60 =~    x3   0.872 0.031 28.034  0.000    0.811    0.933 #> 4  dem60 =~    y1   0.845 0.039 21.698  0.000    0.769    0.921 #> 5  dem60 =~    y2   0.760 0.054 14.142  0.000    0.655    0.866 #> 6  dem60 =~    y3   0.705 0.063 11.225  0.000    0.582    0.828 #> 7  dem60 =~    y4   0.860 0.036 23.650  0.000    0.789    0.931 #> 8  dem65 =~    y5   0.803 0.046 17.602  0.000    0.714    0.893 #> 9  dem65 =~    y6   0.783 0.049 15.918  0.000    0.687    0.879 #> 10 dem65 =~    y7   0.819 0.043 19.122  0.000    0.735    0.903 #> 11 dem65 =~    y8   0.847 0.038 22.389  0.000    0.773    0.921 #> 12 dem60  ~ ind60   0.448 0.102  4.393  0.000    0.248    0.648 #> 13 dem65  ~ ind60   0.146 0.070  2.071  0.038    0.008    0.283 #> 14 dem65  ~ dem60   0.913 0.048 19.120  0.000    0.819    1.006 #> 15    x1 ~~    x1   0.154 0.042  3.636  0.000    0.071    0.238 #> 16    x2 ~~    x2   0.053 0.032  1.634  0.102   -0.010    0.116 #> 17    x3 ~~    x3   0.240 0.054  4.417  0.000    0.133    0.346 #> 18    y1 ~~    y1   0.286 0.066  4.348  0.000    0.157    0.415 #> 19    y2 ~~    y2   0.422 0.082  5.166  0.000    0.262    0.582 #> 20    y3 ~~    y3   0.503 0.089  5.676  0.000    0.329    0.676 #> 21    y4 ~~    y4   0.261 0.063  4.173  0.000    0.138    0.383 #> 22    y5 ~~    y5   0.355 0.073  4.842  0.000    0.211    0.499 #> 23    y6 ~~    y6   0.387 0.077  5.024  0.000    0.236    0.538 #> 24    y7 ~~    y7   0.329 0.070  4.696  0.000    0.192    0.467 #> 25    y8 ~~    y8   0.283 0.064  4.416  0.000    0.157    0.408 #> 26 ind60 ~~ ind60   1.000 0.000     NA     NA    1.000    1.000 #> 27 dem60 ~~ dem60   0.799 0.091  8.737  0.000    0.620    0.978 #> 28 dem65 ~~ dem65   0.026 0.046  0.579  0.562   -0.063    0.116"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette.html","id":"single-group-single-predictor","dir":"Articles","previous_headings":"","what":"Single group, single predictor1","title":"Two-Stage Path Analysis (2S-PA) Model Examples","text":"call tspa(), data frame factor scores needed latent variables. get data frame, apply get_fs() latent variables specify model parameter respective definitions. Combine factor scores latent variable using cbind() can used tspa() model building. build Two-Stage Path Analysis model, simply call tspa() function model = regressions, data = combined factor score data frame using get_fs(), specify standard error either list data frame. Values standard error can found column named fs_[variable name]_se. example, standard error latent variable ind60 can found column fs_ind60_se fs_dat data frame. view Two-Stage Path Analysis model, use attributes([model name])$tspaModel. Function cat() can help tidy model output. output, values error constraints computed squaring standard errors previous section.","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4    # regressions     dem60 ~ ind60 ' fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60) tspa_fit <- tspa(model = \"dem60 ~ ind60\",                   data = fs_dat,                   se = list(ind60 = 0.1213615, dem60 = 0.6756472)) cat(attributes(tspa_fit)$tspaModel) ## # latent variables (indicated by factor scores) ## ind60=~ c(1) * fs_ind60 ## dem60=~ c(1) * fs_dem60 ## # constrain the errors ## fs_ind60~~ c(0.01472861368225) * fs_ind60 ## fs_dem60~~ c(0.45649913886784) * fs_dem60 ## # regressions ## dem60 ~ ind60"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette.html","id":"single-group-multiple-predictors","dir":"Articles","previous_headings":"","what":"Single group, multiple predictors","title":"Two-Stage Path Analysis (2S-PA) Model Examples","text":"call tspa(), data frame factor scores needed latent variables. get data frame, apply get_fs() latent variables specify model parameters respective definitions. Combine factor scores latent variables using cbind() call tspa() model building. build Two-Stage Path Analysis model, simply call tspa() function model = regressions (predictors), data = factor score data frame created combining results get_fs(), specify standard errors either list data frame. Values standard errors can found column named fs_[variable name]_se.2 example, standard error latent variable ind60 can found column fs_ind60_se fs_dat data frame. output model, values error constraints computed squaring standard errors.","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # # residual correlations   #   y1 ~~ y5   #   y2 ~~ y4 + y6   #   y3 ~~ y7   #   y4 ~~ y8   #   y6 ~~ y8 ' fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat_dem65 <- get_fs(data = PoliticalDemocracy,                         model = \"dem65 =~ y5 + y6 + y7 + y8\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60, fs_dat_dem65) tspa_3var_fit <- tspa(model = \"dem60 ~ ind60                           dem65 ~ ind60 + dem60\",                        data = fs_dat,                        se = list(ind60 = 0.1213615, dem60 = 0.6756472,                                  dem65 = 0.5724405)) cat(attributes(tspa_3var_fit)$tspaModel) ## # latent variables (indicated by factor scores) ## ind60=~ c(1) * fs_ind60 ## dem60=~ c(1) * fs_dem60 ## dem65=~ c(1) * fs_dem65 ## # constrain the errors ## fs_ind60~~ c(0.01472861368225) * fs_ind60 ## fs_dem60~~ c(0.45649913886784) * fs_dem60 ## fs_dem65~~ c(0.32768812604025) * fs_dem65 ## # regressions ## dem60 ~ ind60 ##                           dem65 ~ ind60 + dem60"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette.html","id":"multigroup-single-predictor","dir":"Articles","previous_headings":"","what":"Multigroup, single predictor","title":"Two-Stage Path Analysis (2S-PA) Model Examples","text":"call tspa(), data frame factor scores needed multigroup variables. get data frame, apply get_fs() multigroup variables specify model parameter respective definitions. Combine factor scores multigroup variables using cbind() can fed tspa() model building. build Two-Stage Path Analysis model, simply call tspa() function model = regression relation. Specify standard error either list data frame. Values standard error can found column named fs_[variable name]_se. example, standard error multigroup variable visual can found column fs_visual_se fs_hs data frame. get standard errors group faster, unique() can called upon standard error column. example, case, unique(fs_hs$fs_visual_se) can called get standard errors multigroup variable visual. Function standardizedsolution() enables user view table standard error, z score, p-value, lower bound confidence interval multigroup regression relation. view Two-Stage Path Analysis model multigroup, use attributes([model name])$tspaModel. Function cat() can help tidy model output. output, values error constraints computed squaring standard errors previous section.","code":"model <- '    # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9    # regressions     visual ~ speed ' hs_mod <- ' visual =~ x1 + x2 + x3 speed =~ x7 + x8 + x9 '  # get factor scores fs_dat_visual <- get_fs(data = HolzingerSwineford1939,                          model = \"visual =~ x1 + x2 + x3\",                          group = \"school\") fs_dat_speed <- get_fs(data = HolzingerSwineford1939,                         model = \"speed =~ x7 + x8 + x9\",                         group = \"school\") fs_hs <- cbind(do.call(rbind, fs_dat_visual),                do.call(rbind, fs_dat_speed)) # tspa model tspa_fit <- tspa(model = \"visual ~ speed\",                  data = fs_hs,                  se = data.frame(visual = c(0.3391326, 0.311828),                                  speed = c(0.2786875, 0.2740507)),                  group = \"school\"                  # group.equal = \"regressions\"                  ) stdsol <- standardizedsolution(tspa_fit) subset(stdsol, subset = op == \"~\") ##       lhs op   rhs group est.std    se     z pvalue ci.lower ci.upper ## 5  visual  ~ speed     1   0.277 0.114 2.423  0.015    0.053    0.501 ## 16 visual  ~ speed     2   0.439 0.095 4.627  0.000    0.253    0.625 cat(attributes(tspa_fit)$tspaModel) ## # latent variables (indicated by factor scores) ## visual =~ fs_visual ## speed =~ fs_speed ## # constrain the errors ## fs_visual ~~ c(0.11501092038276, 0.097236701584) * fs_visual ## fs_speed ~~ c(0.07766672265625, 0.07510378617049) * fs_speed ## # latent variances ## visual ~ speed"},{"path":"https://gengrui-zhang.github.io/R2spa/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Mark Hok Chio Lai. Author, maintainer. Yixiao Li. Author. Winnie Wing-Yee Tse. Author. Gengrui Zhang Zhang. Author.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lai M, Li Y, Tse W, Zhang G (2023). R2spa: R package two-stage path analysis (2S-PA) adjust measurement errors. R package version 0.0.2, https://gengrui-zhang.github.io/R2spa/.","code":"@Manual{,   title = {R2spa: An R package for two-stage path analysis (2S-PA) to adjust for measurement errors},   author = {Mark Hok Chio Lai and Yixiao Li and Winnie Wing-Yee Tse and Gengrui Zhang Zhang},   year = {2023},   note = {R package version 0.0.2},   url = {https://gengrui-zhang.github.io/R2spa/}, }"},{"path":"https://gengrui-zhang.github.io/R2spa/index.html","id":"r2spa","dir":"","previous_headings":"","what":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","title":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","text":"R2spa free open-source R package performs two-stage path analysis (2S-PA). 2S-PA, researchers can perform path analysis first obtaining factor scores adjusting measurement errors using estimates observation-specific reliability standard error factor scores. viable alternative SEM, 2S-PA shown give equally-good estimates SEM relatively simple models large sample sizes, well give accurate parameter estimates, better control Type error rates, substantially less convergence problems complex models small sample sizes.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","text":"package still developmental stage can installed GitHub :","code":"# install.packages(\"remotes\") remotes::install_github(\"Gengrui-Zhang/R2spa\")"},{"path":"https://gengrui-zhang.github.io/R2spa/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","text":"","code":"library(lavaan) library(R2spa)  # Joint model model <- '   # latent variable definitions     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4    # regression     dem60 ~ ind60 ' # 2S-PA # Stage 1: Get factor scores and standard errors for each latent construct fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                        model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                        model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60)  # get_fs() gives a dataframe with factor scores and standard errors head(fs_dat) #>     fs_ind60 fs_ind60_se   fs_dem60 fs_dem60_se #> 1 -0.5261683   0.1213615 -2.7487224   0.6756472 #> 2  0.1436527   0.1213615 -3.0360803   0.6756472 #> 3  0.7143559   0.1213615  2.6718589   0.6756472 #> 4  1.2399257   0.1213615  2.9936997   0.6756472 #> 5  0.8319080   0.1213615  1.9242932   0.6756472 #> 6  0.2123845   0.1213615  0.9922798   0.6756472 # Stage 2: Perform 2S-PA tspa_fit <- tspa(   model = \"dem60 ~ ind60\",   data = fs_dat,   se = list(ind60 = 0.1213615, dem60 = 0.6756472) ) parameterestimates(tspa_fit) #>        lhs op      rhs label   est    se     z pvalue ci.lower ci.upper #> 1    ind60 =~ fs_ind60       1.000 0.000    NA     NA    1.000    1.000 #> 2    dem60 =~ fs_dem60       1.000 0.000    NA     NA    1.000    1.000 #> 3 fs_ind60 ~~ fs_ind60       0.015 0.000    NA     NA    0.015    0.015 #> 4 fs_dem60 ~~ fs_dem60       0.456 0.000    NA     NA    0.456    0.456 #> 5    ind60 ~~    ind60    v1 0.416 0.070 5.914      0    0.278    0.553 #> 6    dem60 ~~    dem60    v2 2.842 0.543 5.235      0    1.778    3.906 #> 7    dem60  ~    ind60       1.329 0.332 4.000      0    0.678    1.981"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute factor scores — compute_fscore","title":"Compute factor scores — compute_fscore","text":"Compute factor scores","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute factor scores — compute_fscore","text":"","code":"compute_fscore(   y,   lambda,   theta,   psi,   nu = NULL,   alpha = NULL,   method = c(\"regression\", \"Bartlett\"),   acov = FALSE,   fs_matrices = FALSE )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute factor scores — compute_fscore","text":"y N x p matrix row response vector. one observation, matrix one row. lambda p x q matrix factor loadings. theta p x p matrix unique variance-covariances. psi q x q matrix latent factor variance-covariances. nu vector length p measurement intercepts. alpha vector length q latent means. method character string indicating method computing factor scores. Currently, \"regression\" supported. acov Logical indicating whether asymptotic covariance matrix factor scores returned attribute. fs_matrices Logical indicating whether covariances error portion factor scores (av_efs), factor score loading matrix (\\(\\); fsA) intercept vector (\\(b\\); fsb) returned. loading intercept matrices implied loadings intercepts model using factor scores indicators latent variables. TRUE, matrices added attributes.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute factor scores — compute_fscore","text":"N x p matrix factor scores.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute factor scores — compute_fscore","text":"","code":"library(lavaan) #> This is lavaan 0.6-15 #> lavaan is FREE software! Please report any bugs. fit <- cfa(\" ind60 =~ x1 + x2 + x3              dem60 =~ y1 + y2 + y3 + y4 \",            data = PoliticalDemocracy) fs_lavaan <- lavPredict(fit, method = \"Bartlett\") # Using R2spa::compute_fscore() est <- lavInspect(fit, what = \"est\") fs_hand <- compute_fscore(lavInspect(fit, what = \"data\"),                           lambda = est$lambda,                           theta = est$theta,                           psi = est$psi,                           method = \"Bartlett\") fs_hand - fs_lavaan  # same scores #>       ind60 dem60 #>  [1,]     0     0 #>  [2,]     0     0 #>  [3,]     0     0 #>  [4,]     0     0 #>  [5,]     0     0 #>  [6,]     0     0 #>  [7,]     0     0 #>  [8,]     0     0 #>  [9,]     0     0 #> [10,]     0     0 #> [11,]     0     0 #> [12,]     0     0 #> [13,]     0     0 #> [14,]     0     0 #> [15,]     0     0 #> [16,]     0     0 #> [17,]     0     0 #> [18,]     0     0 #> [19,]     0     0 #> [20,]     0     0 #> [21,]     0     0 #> [22,]     0     0 #> [23,]     0     0 #> [24,]     0     0 #> [25,]     0     0 #> [26,]     0     0 #> [27,]     0     0 #> [28,]     0     0 #> [29,]     0     0 #> [30,]     0     0 #> [31,]     0     0 #> [32,]     0     0 #> [33,]     0     0 #> [34,]     0     0 #> [35,]     0     0 #> [36,]     0     0 #> [37,]     0     0 #> [38,]     0     0 #> [39,]     0     0 #> [40,]     0     0 #> [41,]     0     0 #> [42,]     0     0 #> [43,]     0     0 #> [44,]     0     0 #> [45,]     0     0 #> [46,]     0     0 #> [47,]     0     0 #> [48,]     0     0 #> [49,]     0     0 #> [50,]     0     0 #> [51,]     0     0 #> [52,]     0     0 #> [53,]     0     0 #> [54,]     0     0 #> [55,]     0     0 #> [56,]     0     0 #> [57,]     0     0 #> [58,]     0     0 #> [59,]     0     0 #> [60,]     0     0 #> [61,]     0     0 #> [62,]     0     0 #> [63,]     0     0 #> [64,]     0     0 #> [65,]     0     0 #> [66,]     0     0 #> [67,]     0     0 #> [68,]     0     0 #> [69,]     0     0 #> [70,]     0     0 #> [71,]     0     0 #> [72,]     0     0 #> [73,]     0     0 #> [74,]     0     0 #> [75,]     0     0"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"Get Factor Scores Corresponding Standard Error Measurement","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"","code":"get_fs(   data,   model = NULL,   group = NULL,   method = c(\"regression\", \"Bartlett\"),   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"data data frame containing indicators. model optional string specifying measurement model lavaan syntax. See model.syntax information. group Character. Name grouping variable multiple group analysis, passed cfa. method Character. Method computing factor scores (options \"regression\" \"Bartlett\"). Currently, default \"regression\" consistent lavPredict, Bartlett scores desirable properties may preferred 2S-PA. ... additional arguments passed cfa. See lavOptions complete list.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"data frame containing factor scores (prefix \"fs_\") standard errors (suffix \"_se\").","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"","code":"library(lavaan) get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\")]) #>          fs_f1  fs_f1_se f1_by_fs_f1 evfs_f1_f1 #> 1  -0.52616832 0.1213615   0.9657673 0.01472862 #> 2   0.14365274 0.1213615   0.9657673 0.01472862 #> 3   0.71435592 0.1213615   0.9657673 0.01472862 #> 4   1.23992565 0.1213615   0.9657673 0.01472862 #> 5   0.83190803 0.1213615   0.9657673 0.01472862 #> 6   0.21238453 0.1213615   0.9657673 0.01472862 #> 7   0.11880855 0.1213615   0.9657673 0.01472862 #> 8   0.11322703 0.1213615   0.9657673 0.01472862 #> 9   0.25617279 0.1213615   0.9657673 0.01472862 #> 10  0.37112496 0.1213615   0.9657673 0.01472862 #> 11  0.67281395 0.1213615   0.9657673 0.01472862 #> 12  0.56885577 0.1213615   0.9657673 0.01472862 #> 13  1.31369791 0.1213615   0.9657673 0.01472862 #> 14  0.22042629 0.1213615   0.9657673 0.01472862 #> 15  0.57849228 0.1213615   0.9657673 0.01472862 #> 16  0.37805983 0.1213615   0.9657673 0.01472862 #> 17  0.05734046 0.1213615   0.9657673 0.01472862 #> 18 -0.01609202 0.1213615   0.9657673 0.01472862 #> 19  0.88923616 0.1213615   0.9657673 0.01472862 #> 20  1.11445897 0.1213615   0.9657673 0.01472862 #> 21  0.94657339 0.1213615   0.9657673 0.01472862 #> 22  0.90122770 0.1213615   0.9657673 0.01472862 #> 23  0.58409450 0.1213615   0.9657673 0.01472862 #> 24  0.64089192 0.1213615   0.9657673 0.01472862 #> 25  0.91021968 0.1213615   0.9657673 0.01472862 #> 26 -0.89660969 0.1213615   0.9657673 0.01472862 #> 27 -0.13195991 0.1213615   0.9657673 0.01472862 #> 28 -0.52968769 0.1213615   0.9657673 0.01472862 #> 29 -0.81799629 0.1213615   0.9657673 0.01472862 #> 30 -1.27199371 0.1213615   0.9657673 0.01472862 #> 31 -0.32096024 0.1213615   0.9657673 0.01472862 #> 32 -1.16780103 0.1213615   0.9657673 0.01472862 #> 33 -0.12295473 0.1213615   0.9657673 0.01472862 #> 34 -0.04285945 0.1213615   0.9657673 0.01472862 #> 35 -0.34323505 0.1213615   0.9657673 0.01472862 #> 36 -0.60541633 0.1213615   0.9657673 0.01472862 #> 37  0.17688718 0.1213615   0.9657673 0.01472862 #> 38 -0.55066055 0.1213615   0.9657673 0.01472862 #> 39 -1.05988219 0.1213615   0.9657673 0.01472862 #> 40 -0.04138802 0.1213615   0.9657673 0.01472862 #> 41 -0.12611837 0.1213615   0.9657673 0.01472862 #> 42 -0.60322892 0.1213615   0.9657673 0.01472862 #> 43 -0.11057176 0.1213615   0.9657673 0.01472862 #> 44 -1.06423085 0.1213615   0.9657673 0.01472862 #> 45 -1.08354999 0.1213615   0.9657673 0.01472862 #> 46 -0.84009484 0.1213615   0.9657673 0.01472862 #> 47 -1.14678213 0.1213615   0.9657673 0.01472862 #> 48 -0.57578976 0.1213615   0.9657673 0.01472862 #> 49  0.07186692 0.1213615   0.9657673 0.01472862 #> 50  0.14682421 0.1213615   0.9657673 0.01472862 #> 51  0.35871830 0.1213615   0.9657673 0.01472862 #> 52 -0.43403195 0.1213615   0.9657673 0.01472862 #> 53  0.44603111 0.1213615   0.9657673 0.01472862 #> 54  0.26352000 0.1213615   0.9657673 0.01472862 #> 55  0.55051165 0.1213615   0.9657673 0.01472862 #> 56  0.23453122 0.1213615   0.9657673 0.01472862 #> 57  0.27968138 0.1213615   0.9657673 0.01472862 #> 58  0.70960640 0.1213615   0.9657673 0.01472862 #> 59  0.25227978 0.1213615   0.9657673 0.01472862 #> 60  1.18849297 0.1213615   0.9657673 0.01472862 #> 61  0.21104946 0.1213615   0.9657673 0.01472862 #> 62 -1.16516281 0.1213615   0.9657673 0.01472862 #> 63 -0.85560065 0.1213615   0.9657673 0.01472862 #> 64  0.13398476 0.1213615   0.9657673 0.01472862 #> 65 -0.07912189 0.1213615   0.9657673 0.01472862 #> 66 -0.27146711 0.1213615   0.9657673 0.01472862 #> 67 -0.04417217 0.1213615   0.9657673 0.01472862 #> 68 -1.33425662 0.1213615   0.9657673 0.01472862 #> 69 -0.38720750 0.1213615   0.9657673 0.01472862 #> 70 -0.55355511 0.1213615   0.9657673 0.01472862 #> 71 -0.72242623 0.1213615   0.9657673 0.01472862 #> 72  0.30607449 0.1213615   0.9657673 0.01472862 #> 73  0.77707950 0.1213615   0.9657673 0.01472862 #> 74  0.06847481 0.1213615   0.9657673 0.01472862 #> 75 -0.11052927 0.1213615   0.9657673 0.01472862  # Multiple factors get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\", \"y1\", \"y2\", \"y3\", \"y4\")],        model = \" ind60 =~ x1 + x2 + x3                  dem60 =~ y1 + y2 + y3 + y4 \") #>       fs_ind60    fs_dem60 fs_ind60_se fs_dem60_se ind60_by_fs_ind60 #> 1  -0.54258816 -2.74640573   0.1245694   0.6307323         0.9553858 #> 2   0.12647664 -2.85646114   0.1245694   0.6307323         0.9553858 #> 3   0.73408891  2.74401728   0.1245694   0.6307323         0.9553858 #> 4   1.25253604  3.10856431   0.1245694   0.6307323         0.9553858 #> 5   0.83355267  1.92455641   0.1245694   0.6307323         0.9553858 #> 6   0.22426801  1.02292332   0.1245694   0.6307323         0.9553858 #> 7   0.12517739  1.00406461   0.1245694   0.6307323         0.9553858 #> 8   0.11783867 -0.37216403   0.1245694   0.6307323         0.9553858 #> 9   0.25175134 -1.24897911   0.1245694   0.6307323         0.9553858 #> 10  0.39938631  2.85267059   0.1245694   0.6307323         0.9553858 #> 11  0.67497777  1.41959595   0.1245694   0.6307323         0.9553858 #> 12  0.56462020  1.08769844   0.1245694   0.6307323         0.9553858 #> 13  1.31236592  1.54090232   0.1245694   0.6307323         0.9553858 #> 14  0.23246021  1.77370863   0.1245694   0.6307323         0.9553858 #> 15  0.58638481  2.45676871   0.1245694   0.6307323         0.9553858 #> 16  0.38404785  2.35887573   0.1245694   0.6307323         0.9553858 #> 17  0.05076465  0.04034088   0.1245694   0.6307323         0.9553858 #> 18 -0.01747337 -1.86718064   0.1245694   0.6307323         0.9553858 #> 19  0.90920762  3.61477756   0.1245694   0.6307323         0.9553858 #> 20  1.12553557  0.88355273   0.1245694   0.6307323         0.9553858 #> 21  0.97202590  3.62673300   0.1245694   0.6307323         0.9553858 #> 22  0.87820036 -3.02428925   0.1245694   0.6307323         0.9553858 #> 23  0.57540754 -1.51695438   0.1245694   0.6307323         0.9553858 #> 24  0.66221224  2.76341635   0.1245694   0.6307323         0.9553858 #> 25  0.92358281  2.00507336   0.1245694   0.6307323         0.9553858 #> 26 -0.89353051 -0.92008050   0.1245694   0.6307323         0.9553858 #> 27 -0.13984744 -1.19025576   0.1245694   0.6307323         0.9553858 #> 28 -0.53828496 -1.01247764   0.1245694   0.6307323         0.9553858 #> 29 -0.80834865  0.10456709   0.1245694   0.6307323         0.9553858 #> 30 -1.25324343 -0.71847055   0.1245694   0.6307323         0.9553858 #> 31 -0.33373641 -1.61401581   0.1245694   0.6307323         0.9553858 #> 32 -1.17441075 -3.27250363   0.1245694   0.6307323         0.9553858 #> 33 -0.12409974 -1.17530231   0.1245694   0.6307323         0.9553858 #> 34 -0.04239173 -0.53796274   0.1245694   0.6307323         0.9553858 #> 35 -0.34010528  0.74552889   0.1245694   0.6307323         0.9553858 #> 36 -0.58953870  1.61018662   0.1245694   0.6307323         0.9553858 #> 37  0.17453657 -0.28144814   0.1245694   0.6307323         0.9553858 #> 38 -0.54457243  0.37694690   0.1245694   0.6307323         0.9553858 #> 39 -1.05196602 -0.62919501   0.1245694   0.6307323         0.9553858 #> 40 -0.05504697 -0.03346842   0.1245694   0.6307323         0.9553858 #> 41 -0.12364358 -0.38394102   0.1245694   0.6307323         0.9553858 #> 42 -0.59058710  1.35347275   0.1245694   0.6307323         0.9553858 #> 43 -0.11968796  0.89227782   0.1245694   0.6307323         0.9553858 #> 44 -1.07176064 -2.08481096   0.1245694   0.6307323         0.9553858 #> 45 -1.09139097 -2.07944291   0.1245694   0.6307323         0.9553858 #> 46 -0.83287255  1.59590721   0.1245694   0.6307323         0.9553858 #> 47 -1.14519896 -1.53352201   0.1245694   0.6307323         0.9553858 #> 48 -0.56115378  2.08138051   0.1245694   0.6307323         0.9553858 #> 49  0.06493340 -1.04044137   0.1245694   0.6307323         0.9553858 #> 50  0.15671638  1.72618633   0.1245694   0.6307323         0.9553858 #> 51  0.34626130 -1.24967043   0.1245694   0.6307323         0.9553858 #> 52 -0.45158373 -2.31742576   0.1245694   0.6307323         0.9553858 #> 53  0.43233465 -1.07533341   0.1245694   0.6307323         0.9553858 #> 54  0.25779725 -0.02904676   0.1245694   0.6307323         0.9553858 #> 55  0.51730650 -2.78207923   0.1245694   0.6307323         0.9553858 #> 56  0.20104991 -2.49001474   0.1245694   0.6307323         0.9553858 #> 57  0.25318620 -2.52145147   0.1245694   0.6307323         0.9553858 #> 58  0.72354623  1.86717109   0.1245694   0.6307323         0.9553858 #> 59  0.24619740 -0.93321102   0.1245694   0.6307323         0.9553858 #> 60  1.21681210  3.19853937   0.1245694   0.6307323         0.9553858 #> 61  0.18167599 -3.15685030   0.1245694   0.6307323         0.9553858 #> 62 -1.16605067 -3.41334680   0.1245694   0.6307323         0.9553858 #> 63 -0.86491026 -3.11864398   0.1245694   0.6307323         0.9553858 #> 64  0.10990059 -0.47238885   0.1245694   0.6307323         0.9553858 #> 65 -0.07376176  2.95292007   0.1245694   0.6307323         0.9553858 #> 66 -0.28782931 -1.96509718   0.1245694   0.6307323         0.9553858 #> 67 -0.02508160  2.96218478   0.1245694   0.6307323         0.9553858 #> 68 -1.31843215 -1.59567027   0.1245694   0.6307323         0.9553858 #> 69 -0.40462357 -1.79146161   0.1245694   0.6307323         0.9553858 #> 70 -0.55568363 -1.01578892   0.1245694   0.6307323         0.9553858 #> 71 -0.71308015  0.08818212   0.1245694   0.6307323         0.9553858 #> 72  0.31014319  1.70765911   0.1245694   0.6307323         0.9553858 #> 73  0.79092897  1.86102556   0.1245694   0.6307323         0.9553858 #> 74  0.08770237  3.12885767   0.1245694   0.6307323         0.9553858 #> 75 -0.14138149 -2.41398025   0.1245694   0.6307323         0.9553858 #>    ind60_by_fs_dem60 dem60_by_fs_ind60 dem60_by_fs_dem60 evfs_ind60_ind60 #> 1           0.181827       0.005867694         0.8688887       0.01551752 #> 2           0.181827       0.005867694         0.8688887       0.01551752 #> 3           0.181827       0.005867694         0.8688887       0.01551752 #> 4           0.181827       0.005867694         0.8688887       0.01551752 #> 5           0.181827       0.005867694         0.8688887       0.01551752 #> 6           0.181827       0.005867694         0.8688887       0.01551752 #> 7           0.181827       0.005867694         0.8688887       0.01551752 #> 8           0.181827       0.005867694         0.8688887       0.01551752 #> 9           0.181827       0.005867694         0.8688887       0.01551752 #> 10          0.181827       0.005867694         0.8688887       0.01551752 #> 11          0.181827       0.005867694         0.8688887       0.01551752 #> 12          0.181827       0.005867694         0.8688887       0.01551752 #> 13          0.181827       0.005867694         0.8688887       0.01551752 #> 14          0.181827       0.005867694         0.8688887       0.01551752 #> 15          0.181827       0.005867694         0.8688887       0.01551752 #> 16          0.181827       0.005867694         0.8688887       0.01551752 #> 17          0.181827       0.005867694         0.8688887       0.01551752 #> 18          0.181827       0.005867694         0.8688887       0.01551752 #> 19          0.181827       0.005867694         0.8688887       0.01551752 #> 20          0.181827       0.005867694         0.8688887       0.01551752 #> 21          0.181827       0.005867694         0.8688887       0.01551752 #> 22          0.181827       0.005867694         0.8688887       0.01551752 #> 23          0.181827       0.005867694         0.8688887       0.01551752 #> 24          0.181827       0.005867694         0.8688887       0.01551752 #> 25          0.181827       0.005867694         0.8688887       0.01551752 #> 26          0.181827       0.005867694         0.8688887       0.01551752 #> 27          0.181827       0.005867694         0.8688887       0.01551752 #> 28          0.181827       0.005867694         0.8688887       0.01551752 #> 29          0.181827       0.005867694         0.8688887       0.01551752 #> 30          0.181827       0.005867694         0.8688887       0.01551752 #> 31          0.181827       0.005867694         0.8688887       0.01551752 #> 32          0.181827       0.005867694         0.8688887       0.01551752 #> 33          0.181827       0.005867694         0.8688887       0.01551752 #> 34          0.181827       0.005867694         0.8688887       0.01551752 #> 35          0.181827       0.005867694         0.8688887       0.01551752 #> 36          0.181827       0.005867694         0.8688887       0.01551752 #> 37          0.181827       0.005867694         0.8688887       0.01551752 #> 38          0.181827       0.005867694         0.8688887       0.01551752 #> 39          0.181827       0.005867694         0.8688887       0.01551752 #> 40          0.181827       0.005867694         0.8688887       0.01551752 #> 41          0.181827       0.005867694         0.8688887       0.01551752 #> 42          0.181827       0.005867694         0.8688887       0.01551752 #> 43          0.181827       0.005867694         0.8688887       0.01551752 #> 44          0.181827       0.005867694         0.8688887       0.01551752 #> 45          0.181827       0.005867694         0.8688887       0.01551752 #> 46          0.181827       0.005867694         0.8688887       0.01551752 #> 47          0.181827       0.005867694         0.8688887       0.01551752 #> 48          0.181827       0.005867694         0.8688887       0.01551752 #> 49          0.181827       0.005867694         0.8688887       0.01551752 #> 50          0.181827       0.005867694         0.8688887       0.01551752 #> 51          0.181827       0.005867694         0.8688887       0.01551752 #> 52          0.181827       0.005867694         0.8688887       0.01551752 #> 53          0.181827       0.005867694         0.8688887       0.01551752 #> 54          0.181827       0.005867694         0.8688887       0.01551752 #> 55          0.181827       0.005867694         0.8688887       0.01551752 #> 56          0.181827       0.005867694         0.8688887       0.01551752 #> 57          0.181827       0.005867694         0.8688887       0.01551752 #> 58          0.181827       0.005867694         0.8688887       0.01551752 #> 59          0.181827       0.005867694         0.8688887       0.01551752 #> 60          0.181827       0.005867694         0.8688887       0.01551752 #> 61          0.181827       0.005867694         0.8688887       0.01551752 #> 62          0.181827       0.005867694         0.8688887       0.01551752 #> 63          0.181827       0.005867694         0.8688887       0.01551752 #> 64          0.181827       0.005867694         0.8688887       0.01551752 #> 65          0.181827       0.005867694         0.8688887       0.01551752 #> 66          0.181827       0.005867694         0.8688887       0.01551752 #> 67          0.181827       0.005867694         0.8688887       0.01551752 #> 68          0.181827       0.005867694         0.8688887       0.01551752 #> 69          0.181827       0.005867694         0.8688887       0.01551752 #> 70          0.181827       0.005867694         0.8688887       0.01551752 #> 71          0.181827       0.005867694         0.8688887       0.01551752 #> 72          0.181827       0.005867694         0.8688887       0.01551752 #> 73          0.181827       0.005867694         0.8688887       0.01551752 #> 74          0.181827       0.005867694         0.8688887       0.01551752 #> 75          0.181827       0.005867694         0.8688887       0.01551752 #>    evfs_dem60_ind60 evfs_dem60_dem60 #> 1       0.005632564        0.3978232 #> 2       0.005632564        0.3978232 #> 3       0.005632564        0.3978232 #> 4       0.005632564        0.3978232 #> 5       0.005632564        0.3978232 #> 6       0.005632564        0.3978232 #> 7       0.005632564        0.3978232 #> 8       0.005632564        0.3978232 #> 9       0.005632564        0.3978232 #> 10      0.005632564        0.3978232 #> 11      0.005632564        0.3978232 #> 12      0.005632564        0.3978232 #> 13      0.005632564        0.3978232 #> 14      0.005632564        0.3978232 #> 15      0.005632564        0.3978232 #> 16      0.005632564        0.3978232 #> 17      0.005632564        0.3978232 #> 18      0.005632564        0.3978232 #> 19      0.005632564        0.3978232 #> 20      0.005632564        0.3978232 #> 21      0.005632564        0.3978232 #> 22      0.005632564        0.3978232 #> 23      0.005632564        0.3978232 #> 24      0.005632564        0.3978232 #> 25      0.005632564        0.3978232 #> 26      0.005632564        0.3978232 #> 27      0.005632564        0.3978232 #> 28      0.005632564        0.3978232 #> 29      0.005632564        0.3978232 #> 30      0.005632564        0.3978232 #> 31      0.005632564        0.3978232 #> 32      0.005632564        0.3978232 #> 33      0.005632564        0.3978232 #> 34      0.005632564        0.3978232 #> 35      0.005632564        0.3978232 #> 36      0.005632564        0.3978232 #> 37      0.005632564        0.3978232 #> 38      0.005632564        0.3978232 #> 39      0.005632564        0.3978232 #> 40      0.005632564        0.3978232 #> 41      0.005632564        0.3978232 #> 42      0.005632564        0.3978232 #> 43      0.005632564        0.3978232 #> 44      0.005632564        0.3978232 #> 45      0.005632564        0.3978232 #> 46      0.005632564        0.3978232 #> 47      0.005632564        0.3978232 #> 48      0.005632564        0.3978232 #> 49      0.005632564        0.3978232 #> 50      0.005632564        0.3978232 #> 51      0.005632564        0.3978232 #> 52      0.005632564        0.3978232 #> 53      0.005632564        0.3978232 #> 54      0.005632564        0.3978232 #> 55      0.005632564        0.3978232 #> 56      0.005632564        0.3978232 #> 57      0.005632564        0.3978232 #> 58      0.005632564        0.3978232 #> 59      0.005632564        0.3978232 #> 60      0.005632564        0.3978232 #> 61      0.005632564        0.3978232 #> 62      0.005632564        0.3978232 #> 63      0.005632564        0.3978232 #> 64      0.005632564        0.3978232 #> 65      0.005632564        0.3978232 #> 66      0.005632564        0.3978232 #> 67      0.005632564        0.3978232 #> 68      0.005632564        0.3978232 #> 69      0.005632564        0.3978232 #> 70      0.005632564        0.3978232 #> 71      0.005632564        0.3978232 #> 72      0.005632564        0.3978232 #> 73      0.005632564        0.3978232 #> 74      0.005632564        0.3978232 #> 75      0.005632564        0.3978232  # Multiple-group hs_model <- ' visual  =~ x1 + x2 + x3 ' fit <- cfa(hs_model,            data = HolzingerSwineford1939,            group = \"school\") get_fs(HolzingerSwineford1939, hs_model, group = \"school\") #> $Pasteur #>        fs_visual fs_visual_se visual_by_fs_visual evfs_visual_visual  school #> 1   -0.821165191    0.3391326           0.6734826          0.1150109 Pasteur #> 2   -0.124009418    0.3391326           0.6734826          0.1150109 Pasteur #> 3   -0.370072089    0.3391326           0.6734826          0.1150109 Pasteur #> 4    0.440928618    0.3391326           0.6734826          0.1150109 Pasteur #> 5   -0.691389016    0.3391326           0.6734826          0.1150109 Pasteur #> 6   -0.110032619    0.3391326           0.6734826          0.1150109 Pasteur #> 7   -0.904127845    0.3391326           0.6734826          0.1150109 Pasteur #> 8   -0.031747573    0.3391326           0.6734826          0.1150109 Pasteur #> 9   -0.439478981    0.3391326           0.6734826          0.1150109 Pasteur #> 10  -0.938939050    0.3391326           0.6734826          0.1150109 Pasteur #> 11  -0.436821880    0.3391326           0.6734826          0.1150109 Pasteur #> 12   0.305033497    0.3391326           0.6734826          0.1150109 Pasteur #> 13   0.522076263    0.3391326           0.6734826          0.1150109 Pasteur #> 14  -0.090367931    0.3391326           0.6734826          0.1150109 Pasteur #> 15   0.526276771    0.3391326           0.6734826          0.1150109 Pasteur #> 16  -0.226580678    0.3391326           0.6734826          0.1150109 Pasteur #> 17  -0.582016192    0.3391326           0.6734826          0.1150109 Pasteur #> 18   0.017040431    0.3391326           0.6734826          0.1150109 Pasteur #> 19   0.563052459    0.3391326           0.6734826          0.1150109 Pasteur #> 20   0.746621910    0.3391326           0.6734826          0.1150109 Pasteur #> 21   0.234672405    0.3391326           0.6734826          0.1150109 Pasteur #> 22   1.157487518    0.3391326           0.6734826          0.1150109 Pasteur #> 23  -0.162272449    0.3391326           0.6734826          0.1150109 Pasteur #> 24  -0.556027059    0.3391326           0.6734826          0.1150109 Pasteur #> 25  -0.321443540    0.3391326           0.6734826          0.1150109 Pasteur #> 26   0.153141050    0.3391326           0.6734826          0.1150109 Pasteur #> 27   0.696234416    0.3391326           0.6734826          0.1150109 Pasteur #> 28  -0.020961039    0.3391326           0.6734826          0.1150109 Pasteur #> 29   0.532601236    0.3391326           0.6734826          0.1150109 Pasteur #> 30  -0.727687585    0.3391326           0.6734826          0.1150109 Pasteur #> 31  -0.676719580    0.3391326           0.6734826          0.1150109 Pasteur #> 32  -1.120216393    0.3391326           0.6734826          0.1150109 Pasteur #> 33  -0.313631732    0.3391326           0.6734826          0.1150109 Pasteur #> 34  -0.187091845    0.3391326           0.6734826          0.1150109 Pasteur #> 35  -0.887709484    0.3391326           0.6734826          0.1150109 Pasteur #> 36  -0.760795908    0.3391326           0.6734826          0.1150109 Pasteur #> 37   0.556943532    0.3391326           0.6734826          0.1150109 Pasteur #> 38  -0.458666570    0.3391326           0.6734826          0.1150109 Pasteur #> 39   0.514741536    0.3391326           0.6734826          0.1150109 Pasteur #> 40   0.373009089    0.3391326           0.6734826          0.1150109 Pasteur #> 41  -0.528550562    0.3391326           0.6734826          0.1150109 Pasteur #> 42  -0.865864795    0.3391326           0.6734826          0.1150109 Pasteur #> 43  -1.182344640    0.3391326           0.6734826          0.1150109 Pasteur #> 44  -0.435334517    0.3391326           0.6734826          0.1150109 Pasteur #> 45   0.306520860    0.3391326           0.6734826          0.1150109 Pasteur #> 46   0.821604565    0.3391326           0.6734826          0.1150109 Pasteur #> 47   1.213927875    0.3391326           0.6734826          0.1150109 Pasteur #> 48  -0.851887996    0.3391326           0.6734826          0.1150109 Pasteur #> 49  -0.085053749    0.3391326           0.6734826          0.1150109 Pasteur #> 50  -0.508885873    0.3391326           0.6734826          0.1150109 Pasteur #> 51   0.502467638    0.3391326           0.6734826          0.1150109 Pasteur #> 52   0.284732253    0.3391326           0.6734826          0.1150109 Pasteur #> 53   0.202677755    0.3391326           0.6734826          0.1150109 Pasteur #> 54  -0.335953502    0.3391326           0.6734826          0.1150109 Pasteur #> 55   0.556410369    0.3391326           0.6734826          0.1150109 Pasteur #> 56  -0.058746970    0.3391326           0.6734826          0.1150109 Pasteur #> 57  -0.066932487    0.3391326           0.6734826          0.1150109 Pasteur #> 58   0.554230368    0.3391326           0.6734826          0.1150109 Pasteur #> 59  -0.321761185    0.3391326           0.6734826          0.1150109 Pasteur #> 60  -0.421834819    0.3391326           0.6734826          0.1150109 Pasteur #> 61   0.345476529    0.3391326           0.6734826          0.1150109 Pasteur #> 62   0.194809883    0.3391326           0.6734826          0.1150109 Pasteur #> 63  -0.207870208    0.3391326           0.6734826          0.1150109 Pasteur #> 64  -0.441658981    0.3391326           0.6734826          0.1150109 Pasteur #> 65   0.102070958    0.3391326           0.6734826          0.1150109 Pasteur #> 66   0.311198487    0.3391326           0.6734826          0.1150109 Pasteur #> 67   0.676364229    0.3391326           0.6734826          0.1150109 Pasteur #> 68   0.297858262    0.3391326           0.6734826          0.1150109 Pasteur #> 69  -1.055487128    0.3391326           0.6734826          0.1150109 Pasteur #> 70  -0.737997019    0.3391326           0.6734826          0.1150109 Pasteur #> 71  -1.576099236    0.3391326           0.6734826          0.1150109 Pasteur #> 72   0.534360181    0.3391326           0.6734826          0.1150109 Pasteur #> 73  -0.105888156    0.3391326           0.6734826          0.1150109 Pasteur #> 74   0.266237302    0.3391326           0.6734826          0.1150109 Pasteur #> 75  -0.352427927    0.3391326           0.6734826          0.1150109 Pasteur #> 76  -0.334783784    0.3391326           0.6734826          0.1150109 Pasteur #> 77   0.133588508    0.3391326           0.6734826          0.1150109 Pasteur #> 78  -1.035662965    0.3391326           0.6734826          0.1150109 Pasteur #> 79   0.762507108    0.3391326           0.6734826          0.1150109 Pasteur #> 80  -0.260699265    0.3391326           0.6734826          0.1150109 Pasteur #> 81  -0.329095893    0.3391326           0.6734826          0.1150109 Pasteur #> 82   0.752413211    0.3391326           0.6734826          0.1150109 Pasteur #> 83   0.149268188    0.3391326           0.6734826          0.1150109 Pasteur #> 84  -0.208880471    0.3391326           0.6734826          0.1150109 Pasteur #> 85  -1.078285998    0.3391326           0.6734826          0.1150109 Pasteur #> 86   0.306043760    0.3391326           0.6734826          0.1150109 Pasteur #> 87   0.349677056    0.3391326           0.6734826          0.1150109 Pasteur #> 88   0.165686549    0.3391326           0.6734826          0.1150109 Pasteur #> 89   0.077307606    0.3391326           0.6734826          0.1150109 Pasteur #> 90  -0.077401396    0.3391326           0.6734826          0.1150109 Pasteur #> 91  -0.081863485    0.3391326           0.6734826          0.1150109 Pasteur #> 92   0.106748566    0.3391326           0.6734826          0.1150109 Pasteur #> 93  -0.211593616    0.3391326           0.6734826          0.1150109 Pasteur #> 94  -0.926665153    0.3391326           0.6734826          0.1150109 Pasteur #> 95  -0.739484382    0.3391326           0.6734826          0.1150109 Pasteur #> 96   0.570387167    0.3391326           0.6734826          0.1150109 Pasteur #> 97  -0.913642554    0.3391326           0.6734826          0.1150109 Pasteur #> 98   0.547484887    0.3391326           0.6734826          0.1150109 Pasteur #> 99  -0.602850599    0.3391326           0.6734826          0.1150109 Pasteur #> 100  0.225794270    0.3391326           0.6734826          0.1150109 Pasteur #> 101  0.620447015    0.3391326           0.6734826          0.1150109 Pasteur #> 102  0.158885005    0.3391326           0.6734826          0.1150109 Pasteur #> 103 -0.127938344    0.3391326           0.6734826          0.1150109 Pasteur #> 104 -0.420347455    0.3391326           0.6734826          0.1150109 Pasteur #> 105  1.327978307    0.3391326           0.6734826          0.1150109 Pasteur #> 106  0.181843348    0.3391326           0.6734826          0.1150109 Pasteur #> 107 -0.148932224    0.3391326           0.6734826          0.1150109 Pasteur #> 108  0.612373626    0.3391326           0.6734826          0.1150109 Pasteur #> 109 -0.066558798    0.3391326           0.6734826          0.1150109 Pasteur #> 110 -0.420880619    0.3391326           0.6734826          0.1150109 Pasteur #> 111  1.127036295    0.3391326           0.6734826          0.1150109 Pasteur #> 112  0.237591068    0.3391326           0.6734826          0.1150109 Pasteur #> 113  0.853758689    0.3391326           0.6734826          0.1150109 Pasteur #> 114 -0.143618023    0.3391326           0.6734826          0.1150109 Pasteur #> 115  0.475206679    0.3391326           0.6734826          0.1150109 Pasteur #> 116 -0.670554590    0.3391326           0.6734826          0.1150109 Pasteur #> 117  0.022672257    0.3391326           0.6734826          0.1150109 Pasteur #> 118  0.302002707    0.3391326           0.6734826          0.1150109 Pasteur #> 119  0.151392125    0.3391326           0.6734826          0.1150109 Pasteur #> 120 -0.475300449    0.3391326           0.6734826          0.1150109 Pasteur #> 121 -0.346740056    0.3391326           0.6734826          0.1150109 Pasteur #> 122 -0.078888759    0.3391326           0.6734826          0.1150109 Pasteur #> 123  1.197237913    0.3391326           0.6734826          0.1150109 Pasteur #> 124  0.539243306    0.3391326           0.6734826          0.1150109 Pasteur #> 125  0.867258388    0.3391326           0.6734826          0.1150109 Pasteur #> 126  0.592287901    0.3391326           0.6734826          0.1150109 Pasteur #> 127 -0.500540901    0.3391326           0.6734826          0.1150109 Pasteur #> 128 -0.361193954    0.3391326           0.6734826          0.1150109 Pasteur #> 129  0.626883588    0.3391326           0.6734826          0.1150109 Pasteur #> 130 -0.437514518    0.3391326           0.6734826          0.1150109 Pasteur #> 131  0.695972854    0.3391326           0.6734826          0.1150109 Pasteur #> 132  0.424715775    0.3391326           0.6734826          0.1150109 Pasteur #> 133 -0.203725744    0.3391326           0.6734826          0.1150109 Pasteur #> 134 -0.441499507    0.3391326           0.6734826          0.1150109 Pasteur #> 135  0.735619838    0.3391326           0.6734826          0.1150109 Pasteur #> 136  0.783874697    0.3391326           0.6734826          0.1150109 Pasteur #> 137  0.565709540    0.3391326           0.6734826          0.1150109 Pasteur #> 138  0.258425494    0.3391326           0.6734826          0.1150109 Pasteur #> 139  0.861093397    0.3391326           0.6734826          0.1150109 Pasteur #> 140 -0.059757233    0.3391326           0.6734826          0.1150109 Pasteur #> 141 -0.920340689    0.3391326           0.6734826          0.1150109 Pasteur #> 142  0.845629236    0.3391326           0.6734826          0.1150109 Pasteur #> 143  1.227427574    0.3391326           0.6734826          0.1150109 Pasteur #> 144  1.054223601    0.3391326           0.6734826          0.1150109 Pasteur #> 145 -1.246596805    0.3391326           0.6734826          0.1150109 Pasteur #> 146 -0.473120468    0.3391326           0.6734826          0.1150109 Pasteur #> 147 -0.560171503    0.3391326           0.6734826          0.1150109 Pasteur #> 148 -0.365394462    0.3391326           0.6734826          0.1150109 Pasteur #> 149  0.084744422    0.3391326           0.6734826          0.1150109 Pasteur #> 150  0.910676146    0.3391326           0.6734826          0.1150109 Pasteur #> 151  1.094189533    0.3391326           0.6734826          0.1150109 Pasteur #> 152 -0.013149231    0.3391326           0.6734826          0.1150109 Pasteur #> 153 -0.166472976    0.3391326           0.6734826          0.1150109 Pasteur #> 154  0.008695459    0.3391326           0.6734826          0.1150109 Pasteur #> 155 -0.094989494    0.3391326           0.6734826          0.1150109 Pasteur #> 156 -0.457123143    0.3391326           0.6734826          0.1150109 Pasteur #>  #> $`Grant-White` #>        fs_visual fs_visual_se visual_by_fs_visual evfs_visual_visual #> 1   -0.915287109     0.311828           0.6990509         0.09723667 #> 2    0.035963597     0.311828           0.6990509         0.09723667 #> 3    0.355636604     0.311828           0.6990509         0.09723667 #> 4   -0.387353871     0.311828           0.6990509         0.09723667 #> 5   -0.622393942     0.311828           0.6990509         0.09723667 #> 6    0.195944561     0.311828           0.6990509         0.09723667 #> 7    1.353023831     0.311828           0.6990509         0.09723667 #> 8   -0.341506254     0.311828           0.6990509         0.09723667 #> 9   -0.199493575     0.311828           0.6990509         0.09723667 #> 10  -0.689869149     0.311828           0.6990509         0.09723667 #> 11  -0.463929554     0.311828           0.6990509         0.09723667 #> 12  -0.423001505     0.311828           0.6990509         0.09723667 #> 13   0.279743296     0.311828           0.6990509         0.09723667 #> 14  -0.916908219     0.311828           0.6990509         0.09723667 #> 15   0.589344501     0.311828           0.6990509         0.09723667 #> 16   0.191474701     0.311828           0.6990509         0.09723667 #> 17   0.935275715     0.311828           0.6990509         0.09723667 #> 18   0.393715904     0.311828           0.6990509         0.09723667 #> 19   0.086569994     0.311828           0.6990509         0.09723667 #> 20   0.555606898     0.311828           0.6990509         0.09723667 #> 21  -0.558217193     0.311828           0.6990509         0.09723667 #> 22   0.766715894     0.311828           0.6990509         0.09723667 #> 23   0.115548801     0.311828           0.6990509         0.09723667 #> 24   0.901249191     0.311828           0.6990509         0.09723667 #> 25   0.174316971     0.311828           0.6990509         0.09723667 #> 26  -0.078980322     0.311828           0.6990509         0.09723667 #> 27  -0.581882977     0.311828           0.6990509         0.09723667 #> 28  -0.661179262     0.311828           0.6990509         0.09723667 #> 29  -0.245341176     0.311828           0.6990509         0.09723667 #> 30  -0.195801662     0.311828           0.6990509         0.09723667 #> 31  -0.281221524     0.311828           0.6990509         0.09723667 #> 32  -0.293909378     0.311828           0.6990509         0.09723667 #> 33  -0.604192958     0.311828           0.6990509         0.09723667 #> 34  -0.738437335     0.311828           0.6990509         0.09723667 #> 35  -0.304109345     0.311828           0.6990509         0.09723667 #> 36   0.104931733     0.311828           0.6990509         0.09723667 #> 37  -0.025781487     0.311828           0.6990509         0.09723667 #> 38  -0.897318824     0.311828           0.6990509         0.09723667 #> 39  -0.892560027     0.311828           0.6990509         0.09723667 #> 40  -0.078402465     0.311828           0.6990509         0.09723667 #> 41  -0.379063934     0.311828           0.6990509         0.09723667 #> 42  -0.324926380     0.311828           0.6990509         0.09723667 #> 43  -0.684299797     0.311828           0.6990509         0.09723667 #> 44  -0.304109345     0.311828           0.6990509         0.09723667 #> 45   0.622793169     0.311828           0.6990509         0.09723667 #> 46  -0.152835419     0.311828           0.6990509         0.09723667 #> 47  -0.421902013     0.311828           0.6990509         0.09723667 #> 48  -0.060883872     0.311828           0.6990509         0.09723667 #> 49  -0.303298790     0.311828           0.6990509         0.09723667 #> 50   0.425021826     0.311828           0.6990509         0.09723667 #> 51   0.131478875     0.311828           0.6990509         0.09723667 #> 52  -0.914998172     0.311828           0.6990509         0.09723667 #> 53   0.324226132     0.311828           0.6990509         0.09723667 #> 54  -0.086170767     0.311828           0.6990509         0.09723667 #> 55   0.428424818     0.311828           0.6990509         0.09723667 #> 56   0.188465179     0.311828           0.6990509         0.09723667 #> 57  -0.306958111     0.311828           0.6990509         0.09723667 #> 58   0.581736955     0.311828           0.6990509         0.09723667 #> 59  -0.743485051     0.311828           0.6990509         0.09723667 #> 60   0.263580524     0.311828           0.6990509         0.09723667 #> 61   0.178786831     0.311828           0.6990509         0.09723667 #> 62  -0.064832113     0.311828           0.6990509         0.09723667 #> 63   0.499976414     0.311828           0.6990509         0.09723667 #> 64  -0.092839593     0.311828           0.6990509         0.09723667 #> 65  -0.263702931     0.311828           0.6990509         0.09723667 #> 66  -0.983966325     0.311828           0.6990509         0.09723667 #> 67   1.434912536     0.311828           0.6990509         0.09723667 #> 68  -1.037582228     0.311828           0.6990509         0.09723667 #> 69  -0.047569849     0.311828           0.6990509         0.09723667 #> 70   1.084767775     0.311828           0.6990509         0.09723667 #> 71   0.092011181     0.311828           0.6990509         0.09723667 #> 72  -0.562687052     0.311828           0.6990509         0.09723667 #> 73  -0.304919900     0.311828           0.6990509         0.09723667 #> 74   1.038920175     0.311828           0.6990509         0.09723667 #> 75  -0.789854287     0.311828           0.6990509         0.09723667 #> 76  -0.602282928     0.311828           0.6990509         0.09723667 #> 77  -0.894630830     0.311828           0.6990509         0.09723667 #> 78   0.532614527     0.311828           0.6990509         0.09723667 #> 79  -0.548955945     0.311828           0.6990509         0.09723667 #> 80  -0.221514635     0.311828           0.6990509         0.09723667 #> 81  -0.095849115     0.311828           0.6990509         0.09723667 #> 82  -0.122235502     0.311828           0.6990509         0.09723667 #> 83   0.892276864     0.311828           0.6990509         0.09723667 #> 84   0.328439663     0.311828           0.6990509         0.09723667 #> 85   1.217391042     0.311828           0.6990509         0.09723667 #> 86   0.574513902     0.311828           0.6990509         0.09723667 #> 87   0.160168762     0.311828           0.6990509         0.09723667 #> 88   0.654909662     0.311828           0.6990509         0.09723667 #> 89  -0.509777155     0.311828           0.6990509         0.09723667 #> 90   1.201493560     0.311828           0.6990509         0.09723667 #> 91   0.584874625     0.311828           0.6990509         0.09723667 #> 92   0.075142371     0.311828           0.6990509         0.09723667 #> 93   0.550976266     0.311828           0.6990509         0.09723667 #> 94  -0.886308302     0.311828           0.6990509         0.09723667 #> 95   0.552075757     0.311828           0.6990509         0.09723667 #> 96   1.415972940     0.311828           0.6990509         0.09723667 #> 97   0.298650301     0.311828           0.6990509         0.09723667 #> 98  -0.143028906     0.311828           0.6990509         0.09723667 #> 99   0.245195154     0.311828           0.6990509         0.09723667 #> 100  0.247072593     0.311828           0.6990509         0.09723667 #> 101  0.817322291     0.311828           0.6990509         0.09723667 #> 102  0.651771976     0.311828           0.6990509         0.09723667 #> 103  1.338875623     0.311828           0.6990509         0.09723667 #> 104 -1.160005528     0.311828           0.6990509         0.09723667 #> 105  0.163306449     0.311828           0.6990509         0.09723667 #> 106 -0.387353871     0.311828           0.6990509         0.09723667 #> 107 -0.517128372     0.311828           0.6990509         0.09723667 #> 108  0.065103160     0.311828           0.6990509         0.09723667 #> 109 -0.115438510     0.311828           0.6990509         0.09723667 #> 110  0.094049376     0.311828           0.6990509         0.09723667 #> 111  0.396725409     0.311828           0.6990509         0.09723667 #> 112  0.672356312     0.311828           0.6990509         0.09723667 #> 113  1.165974090     0.311828           0.6990509         0.09723667 #> 114 -0.483518949     0.311828           0.6990509         0.09723667 #> 115  0.035024877     0.311828           0.6990509         0.09723667 #> 116  0.741974248     0.311828           0.6990509         0.09723667 #> 117 -0.170386603     0.311828           0.6990509         0.09723667 #> 118 -0.205873481     0.311828           0.6990509         0.09723667 #> 119  0.714777307     0.311828           0.6990509         0.09723667 #> 120 -0.620772831     0.311828           0.6990509         0.09723667 #> 121 -0.313626938     0.311828           0.6990509         0.09723667 #> 122 -0.157466035     0.311828           0.6990509         0.09723667 #> 123  0.118269386     0.311828           0.6990509         0.09723667 #> 124  0.101111673     0.311828           0.6990509         0.09723667 #> 125 -0.625403463     0.311828           0.6990509         0.09723667 #> 126  0.486638761     0.311828           0.6990509         0.09723667 #> 127 -0.178676540     0.311828           0.6990509         0.09723667 #> 128  0.274013189     0.311828           0.6990509         0.09723667 #> 129 -0.316347523     0.311828           0.6990509         0.09723667 #> 130 -0.026752814     0.311828           0.6990509         0.09723667 #> 131  0.245323318     0.311828           0.6990509         0.09723667 #> 132 -0.356336853     0.311828           0.6990509         0.09723667 #> 133 -0.581594057     0.311828           0.6990509         0.09723667 #> 134  0.263002667     0.311828           0.6990509         0.09723667 #> 135 -0.864680712     0.311828           0.6990509         0.09723667 #> 136 -0.377964443     0.311828           0.6990509         0.09723667 #> 137 -0.112717909     0.311828           0.6990509         0.09723667 #> 138  0.114449326     0.311828           0.6990509         0.09723667 #> 139  0.001287274     0.311828           0.6990509         0.09723667 #> 140  0.597634438     0.311828           0.6990509         0.09723667 #> 141 -0.252531637     0.311828           0.6990509         0.09723667 #> 142 -0.472901881     0.311828           0.6990509         0.09723667 #> 143 -0.187255397     0.311828           0.6990509         0.09723667 #> 144 -0.542415283     0.311828           0.6990509         0.09723667 #> 145  0.358774274     0.311828           0.6990509         0.09723667 #>          school #> 1   Grant-White #> 2   Grant-White #> 3   Grant-White #> 4   Grant-White #> 5   Grant-White #> 6   Grant-White #> 7   Grant-White #> 8   Grant-White #> 9   Grant-White #> 10  Grant-White #> 11  Grant-White #> 12  Grant-White #> 13  Grant-White #> 14  Grant-White #> 15  Grant-White #> 16  Grant-White #> 17  Grant-White #> 18  Grant-White #> 19  Grant-White #> 20  Grant-White #> 21  Grant-White #> 22  Grant-White #> 23  Grant-White #> 24  Grant-White #> 25  Grant-White #> 26  Grant-White #> 27  Grant-White #> 28  Grant-White #> 29  Grant-White #> 30  Grant-White #> 31  Grant-White #> 32  Grant-White #> 33  Grant-White #> 34  Grant-White #> 35  Grant-White #> 36  Grant-White #> 37  Grant-White #> 38  Grant-White #> 39  Grant-White #> 40  Grant-White #> 41  Grant-White #> 42  Grant-White #> 43  Grant-White #> 44  Grant-White #> 45  Grant-White #> 46  Grant-White #> 47  Grant-White #> 48  Grant-White #> 49  Grant-White #> 50  Grant-White #> 51  Grant-White #> 52  Grant-White #> 53  Grant-White #> 54  Grant-White #> 55  Grant-White #> 56  Grant-White #> 57  Grant-White #> 58  Grant-White #> 59  Grant-White #> 60  Grant-White #> 61  Grant-White #> 62  Grant-White #> 63  Grant-White #> 64  Grant-White #> 65  Grant-White #> 66  Grant-White #> 67  Grant-White #> 68  Grant-White #> 69  Grant-White #> 70  Grant-White #> 71  Grant-White #> 72  Grant-White #> 73  Grant-White #> 74  Grant-White #> 75  Grant-White #> 76  Grant-White #> 77  Grant-White #> 78  Grant-White #> 79  Grant-White #> 80  Grant-White #> 81  Grant-White #> 82  Grant-White #> 83  Grant-White #> 84  Grant-White #> 85  Grant-White #> 86  Grant-White #> 87  Grant-White #> 88  Grant-White #> 89  Grant-White #> 90  Grant-White #> 91  Grant-White #> 92  Grant-White #> 93  Grant-White #> 94  Grant-White #> 95  Grant-White #> 96  Grant-White #> 97  Grant-White #> 98  Grant-White #> 99  Grant-White #> 100 Grant-White #> 101 Grant-White #> 102 Grant-White #> 103 Grant-White #> 104 Grant-White #> 105 Grant-White #> 106 Grant-White #> 107 Grant-White #> 108 Grant-White #> 109 Grant-White #> 110 Grant-White #> 111 Grant-White #> 112 Grant-White #> 113 Grant-White #> 114 Grant-White #> 115 Grant-White #> 116 Grant-White #> 117 Grant-White #> 118 Grant-White #> 119 Grant-White #> 120 Grant-White #> 121 Grant-White #> 122 Grant-White #> 123 Grant-White #> 124 Grant-White #> 125 Grant-White #> 126 Grant-White #> 127 Grant-White #> 128 Grant-White #> 129 Grant-White #> 130 Grant-White #> 131 Grant-White #> 132 Grant-White #> 133 Grant-White #> 134 Grant-White #> 135 Grant-White #> 136 Grant-White #> 137 Grant-White #> 138 Grant-White #> 139 Grant-White #> 140 Grant-White #> 141 Grant-White #> 142 Grant-White #> 143 Grant-White #> 144 Grant-White #> 145 Grant-White #>  #> attr(,\"av_efs\") #> attr(,\"av_efs\")$Pasteur #>           visual #> visual 0.1150109 #>  #> attr(,\"av_efs\")$`Grant-White` #>            visual #> visual 0.09723667 #>  #> attr(,\"fsA\") #> attr(,\"fsA\")$Pasteur #>           visual #> visual 0.6734826 #>  #> attr(,\"fsA\")$`Grant-White` #>           visual #> visual 0.6990509 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #>        intrcp #> visual      0 #>  #> attr(,\"fsb\")$`Grant-White` #>        intrcp #> visual      0 #>  # Or without the model get_fs(HolzingerSwineford1939[c(\"school\", \"x4\", \"x5\", \"x6\")],        group = \"school\") #> $Pasteur #>             fs_f1  fs_f1_se f1_by_fs_f1 evfs_f1_f1  school #> 1    0.3074500370 0.2999315   0.8833584 0.08995892 Pasteur #> 2   -0.7746062892 0.2999315   0.8833584 0.08995892 Pasteur #> 3   -1.5843019574 0.2999315   0.8833584 0.08995892 Pasteur #> 4    0.2739579120 0.2999315   0.8833584 0.08995892 Pasteur #> 5    0.1440153923 0.2999315   0.8833584 0.08995892 Pasteur #> 6   -1.0440895948 0.2999315   0.8833584 0.08995892 Pasteur #> 7    1.0507357396 0.2999315   0.8833584 0.08995892 Pasteur #> 8    0.1041882698 0.2999315   0.8833584 0.08995892 Pasteur #> 9    0.7750146375 0.2999315   0.8833584 0.08995892 Pasteur #> 10   0.4822117444 0.2999315   0.8833584 0.08995892 Pasteur #> 11  -0.4511886490 0.2999315   0.8833584 0.08995892 Pasteur #> 12   0.3522691973 0.2999315   0.8833584 0.08995892 Pasteur #> 13   0.0657041070 0.2999315   0.8833584 0.08995892 Pasteur #> 14   0.3259750264 0.2999315   0.8833584 0.08995892 Pasteur #> 15   1.3008341323 0.2999315   0.8833584 0.08995892 Pasteur #> 16  -0.2804588573 0.2999315   0.8833584 0.08995892 Pasteur #> 17  -0.3604017581 0.2999315   0.8833584 0.08995892 Pasteur #> 18  -0.7502293722 0.2999315   0.8833584 0.08995892 Pasteur #> 19   1.2600468631 0.2999315   0.8833584 0.08995892 Pasteur #> 20   0.2874908636 0.2999315   0.8833584 0.08995892 Pasteur #> 21  -1.1394826729 0.2999315   0.8833584 0.08995892 Pasteur #> 22  -0.3791151940 0.2999315   0.8833584 0.08995892 Pasteur #> 23   1.0444979094 0.2999315   0.8833584 0.08995892 Pasteur #> 24  -0.6248930150 0.2999315   0.8833584 0.08995892 Pasteur #> 25  -0.3689426673 0.2999315   0.8833584 0.08995892 Pasteur #> 26  -0.5663524842 0.2999315   0.8833584 0.08995892 Pasteur #> 27  -0.9568515617 0.2999315   0.8833584 0.08995892 Pasteur #> 28  -0.8137619455 0.2999315   0.8833584 0.08995892 Pasteur #> 29  -0.5028199109 0.2999315   0.8833584 0.08995892 Pasteur #> 30  -0.3054100713 0.2999315   0.8833584 0.08995892 Pasteur #> 31  -0.3728773637 0.2999315   0.8833584 0.08995892 Pasteur #> 32  -0.8529175744 0.2999315   0.8833584 0.08995892 Pasteur #> 33   0.4101382620 0.2999315   0.8833584 0.08995892 Pasteur #> 34   0.1848026386 0.2999315   0.8833584 0.08995892 Pasteur #> 35  -0.9680814159 0.2999315   0.8833584 0.08995892 Pasteur #> 36  -0.1436070439 0.2999315   0.8833584 0.08995892 Pasteur #> 37  -0.0126071782 0.2999315   0.8833584 0.08995892 Pasteur #> 38  -0.3531066368 0.2999315   0.8833584 0.08995892 Pasteur #> 39   1.0665717701 0.2999315   0.8833584 0.08995892 Pasteur #> 40   0.7993915544 0.2999315   0.8833584 0.08995892 Pasteur #> 41   0.1110975387 0.2999315   0.8833584 0.08995892 Pasteur #> 42  -0.2735495637 0.2999315   0.8833584 0.08995892 Pasteur #> 43  -0.7167372327 0.2999315   0.8833584 0.08995892 Pasteur #> 44  -0.6594424814 0.2999315   0.8833584 0.08995892 Pasteur #> 45   0.9507364688 0.2999315   0.8833584 0.08995892 Pasteur #> 46  -0.0478281107 0.2999315   0.8833584 0.08995892 Pasteur #> 47  -1.7573348450 0.2999315   0.8833584 0.08995892 Pasteur #> 48  -0.4620326417 0.2999315   0.8833584 0.08995892 Pasteur #> 49  -1.0305566597 0.2999315   0.8833584 0.08995892 Pasteur #> 50   0.7618675383 0.2999315   0.8833584 0.08995892 Pasteur #> 51  -1.3414986833 0.2999315   0.8833584 0.08995892 Pasteur #> 52  -0.0761397743 0.2999315   0.8833584 0.08995892 Pasteur #> 53  -1.8231705136 0.2999315   0.8833584 0.08995892 Pasteur #> 54   0.0094666323 0.2999315   0.8833584 0.08995892 Pasteur #> 55   0.0436302463 0.2999315   0.8833584 0.08995892 Pasteur #> 56  -0.0001315726 0.2999315   0.8833584 0.08995892 Pasteur #> 57  -0.6571393750 0.2999315   0.8833584 0.08995892 Pasteur #> 58   0.6121542642 0.2999315   0.8833584 0.08995892 Pasteur #> 59  -1.0957208458 0.2999315   0.8833584 0.08995892 Pasteur #> 60  -0.9289257761 0.2999315   0.8833584 0.08995892 Pasteur #> 61   0.9553426588 0.2999315   0.8833584 0.08995892 Pasteur #> 62   0.3647448029 0.2999315   0.8833584 0.08995892 Pasteur #> 63  -1.1003270330 0.2999315   0.8833584 0.08995892 Pasteur #> 64   0.4259742697 0.2999315   0.8833584 0.08995892 Pasteur #> 65   0.1689666035 0.2999315   0.8833584 0.08995892 Pasteur #> 66   0.6586050419 0.2999315   0.8833584 0.08995892 Pasteur #> 67  -0.2952375720 0.2999315   0.8833584 0.08995892 Pasteur #> 68  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 69  -0.5613604418 0.2999315   0.8833584 0.08995892 Pasteur #> 70   0.5632119383 0.2999315   0.8833584 0.08995892 Pasteur #> 71  -0.7769093955 0.2999315   0.8833584 0.08995892 Pasteur #> 72  -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 73  -0.7808440920 0.2999315   0.8833584 0.08995892 Pasteur #> 74   0.9270309952 0.2999315   0.8833584 0.08995892 Pasteur #> 75  -0.5426470306 0.2999315   0.8833584 0.08995892 Pasteur #> 76  -1.1065648385 0.2999315   0.8833584 0.08995892 Pasteur #> 77   0.6233841094 0.2999315   0.8833584 0.08995892 Pasteur #> 78  -1.7010974136 0.2999315   0.8833584 0.08995892 Pasteur #> 79  -0.0013773330 0.2999315   0.8833584 0.08995892 Pasteur #> 80  -1.2772946275 0.2999315   0.8833584 0.08995892 Pasteur #> 81  -1.0344913561 0.2999315   0.8833584 0.08995892 Pasteur #> 82   0.4345151789 0.2999315   0.8833584 0.08995892 Pasteur #> 83  -1.5280645151 0.2999315   0.8833584 0.08995892 Pasteur #> 84  -0.6101143231 0.2999315   0.8833584 0.08995892 Pasteur #> 85  -1.5122284832 0.2999315   0.8833584 0.08995892 Pasteur #> 86   1.2011204798 0.2999315   0.8833584 0.08995892 Pasteur #> 87  -0.3258523027 0.2999315   0.8833584 0.08995892 Pasteur #> 88   0.6687775411 0.2999315   0.8833584 0.08995892 Pasteur #> 89  -1.6382363147 0.2999315   0.8833584 0.08995892 Pasteur #> 90   0.3062042720 0.2999315   0.8833584 0.08995892 Pasteur #> 91  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 92  -0.6798846937 0.2999315   0.8833584 0.08995892 Pasteur #> 93  -1.1865077366 0.2999315   0.8833584 0.08995892 Pasteur #> 94  -0.5011882981 0.2999315   0.8833584 0.08995892 Pasteur #> 95   0.7362448336 0.2999315   0.8833584 0.08995892 Pasteur #> 96   0.4934415622 0.2999315   0.8833584 0.08995892 Pasteur #> 97   0.9661866515 0.2999315   0.8833584 0.08995892 Pasteur #> 98   1.7212764661 0.2999315   0.8833584 0.08995892 Pasteur #> 99  -0.8199997483 0.2999315   0.8833584 0.08995892 Pasteur #> 100  0.7369163271 0.2999315   0.8833584 0.08995892 Pasteur #> 101 -0.5403439270 0.2999315   0.8833584 0.08995892 Pasteur #> 102  0.0525570079 0.2999315   0.8833584 0.08995892 Pasteur #> 103  0.4973762860 0.2999315   0.8833584 0.08995892 Pasteur #> 104  0.5434412113 0.2999315   0.8833584 0.08995892 Pasteur #> 105  1.4015048920 0.2999315   0.8833584 0.08995892 Pasteur #> 106  0.5338429790 0.2999315   0.8833584 0.08995892 Pasteur #> 107  1.5005470281 0.2999315   0.8833584 0.08995892 Pasteur #> 108 -0.4353526184 0.2999315   0.8833584 0.08995892 Pasteur #> 109  1.7269399974 0.2999315   0.8833584 0.08995892 Pasteur #> 110 -0.1863115671 0.2999315   0.8833584 0.08995892 Pasteur #> 111  0.7431541299 0.2999315   0.8833584 0.08995892 Pasteur #> 112  0.3345159128 0.2999315   0.8833584 0.08995892 Pasteur #> 113  0.3111963144 0.2999315   0.8833584 0.08995892 Pasteur #> 114  0.6750153713 0.2999315   0.8833584 0.08995892 Pasteur #> 115 -1.3822859470 0.2999315   0.8833584 0.08995892 Pasteur #> 116  0.4299090164 0.2999315   0.8833584 0.08995892 Pasteur #> 117  0.4368182853 0.2999315   0.8833584 0.08995892 Pasteur #> 118 -0.6334339242 0.2999315   0.8833584 0.08995892 Pasteur #> 119 -0.9153928519 0.2999315   0.8833584 0.08995892 Pasteur #> 120 -0.2662544424 0.2999315   0.8833584 0.08995892 Pasteur #> 121 -0.1238362896 0.2999315   0.8833584 0.08995892 Pasteur #> 122 -0.1987871727 0.2999315   0.8833584 0.08995892 Pasteur #> 123  1.5676284956 0.2999315   0.8833584 0.08995892 Pasteur #> 124 -0.2906313821 0.2999315   0.8833584 0.08995892 Pasteur #> 125  0.7125393874 0.2999315   0.8833584 0.08995892 Pasteur #> 126  0.1324998831 0.2999315   0.8833584 0.08995892 Pasteur #> 127  1.1488177518 0.2999315   0.8833584 0.08995892 Pasteur #> 128  0.5559168169 0.2999315   0.8833584 0.08995892 Pasteur #> 129  0.8572606191 0.2999315   0.8833584 0.08995892 Pasteur #> 130 -0.9789254060 0.2999315   0.8833584 0.08995892 Pasteur #> 131  1.4416206448 0.2999315   0.8833584 0.08995892 Pasteur #> 132  0.4542859333 0.2999315   0.8833584 0.08995892 Pasteur #> 133 -1.3845890506 0.2999315   0.8833584 0.08995892 Pasteur #> 134 -0.2883282757 0.2999315   0.8833584 0.08995892 Pasteur #> 135  0.4430560881 0.2999315   0.8833584 0.08995892 Pasteur #> 136  1.2089899229 0.2999315   0.8833584 0.08995892 Pasteur #> 137  1.1942112109 0.2999315   0.8833584 0.08995892 Pasteur #> 138  0.6013102486 0.2999315   0.8833584 0.08995892 Pasteur #> 139  0.2371053667 0.2999315   0.8833584 0.08995892 Pasteur #> 140  1.0053422804 0.2999315   0.8833584 0.08995892 Pasteur #> 141  0.8095640810 0.2999315   0.8833584 0.08995892 Pasteur #> 142 -1.4408264861 0.2999315   0.8833584 0.08995892 Pasteur #> 143  1.3821199900 0.2999315   0.8833584 0.08995892 Pasteur #> 144  2.7284791267 0.2999315   0.8833584 0.08995892 Pasteur #> 145  0.0123440494 0.2999315   0.8833584 0.08995892 Pasteur #> 146  0.8277032180 0.2999315   0.8833584 0.08995892 Pasteur #> 147  0.7862444827 0.2999315   0.8833584 0.08995892 Pasteur #> 148 -1.1325734026 0.2999315   0.8833584 0.08995892 Pasteur #> 149  1.7660956264 0.2999315   0.8833584 0.08995892 Pasteur #> 150 -0.3712457509 0.2999315   0.8833584 0.08995892 Pasteur #> 151  1.8944065607 0.2999315   0.8833584 0.08995892 Pasteur #> 152  0.6098511578 0.2999315   0.8833584 0.08995892 Pasteur #> 153  0.2654170028 0.2999315   0.8833584 0.08995892 Pasteur #> 154 -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 155 -0.6005160981 0.2999315   0.8833584 0.08995892 Pasteur #> 156  0.3562038937 0.2999315   0.8833584 0.08995892 Pasteur #>  #> $`Grant-White` #>           fs_f1  fs_f1_se f1_by_fs_f1 evfs_f1_f1      school #> 1   -0.39525603 0.3152173   0.8801489 0.09936192 Grant-White #> 2   -0.63397248 0.3152173   0.8801489 0.09936192 Grant-White #> 3    0.20062403 0.3152173   0.8801489 0.09936192 Grant-White #> 4   -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 5    0.43519197 0.3152173   0.8801489 0.09936192 Grant-White #> 6    0.31151246 0.3152173   0.8801489 0.09936192 Grant-White #> 7    2.15612913 0.3152173   0.8801489 0.09936192 Grant-White #> 8   -0.29014192 0.3152173   0.8801489 0.09936192 Grant-White #> 9   -0.08369303 0.3152173   0.8801489 0.09936192 Grant-White #> 10  -0.01807396 0.3152173   0.8801489 0.09936192 Grant-White #> 11  -0.34820234 0.3152173   0.8801489 0.09936192 Grant-White #> 12  -2.10215974 0.3152173   0.8801489 0.09936192 Grant-White #> 13  -0.64552119 0.3152173   0.8801489 0.09936192 Grant-White #> 14  -1.46155228 0.3152173   0.8801489 0.09936192 Grant-White #> 15   1.02620880 0.3152173   0.8801489 0.09936192 Grant-White #> 16  -1.05064956 0.3152173   0.8801489 0.09936192 Grant-White #> 17   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 18   0.95822548 0.3152173   0.8801489 0.09936192 Grant-White #> 19  -0.25355303 0.3152173   0.8801489 0.09936192 Grant-White #> 20   1.42141427 0.3152173   0.8801489 0.09936192 Grant-White #> 21  -0.95400523 0.3152173   0.8801489 0.09936192 Grant-White #> 22   1.00295292 0.3152173   0.8801489 0.09936192 Grant-White #> 23   1.21841354 0.3152173   0.8801489 0.09936192 Grant-White #> 24  -1.24987101 0.3152173   0.8801489 0.09936192 Grant-White #> 25  -0.51984663 0.3152173   0.8801489 0.09936192 Grant-White #> 26  -0.04710419 0.3152173   0.8801489 0.09936192 Grant-White #> 27   0.43934048 0.3152173   0.8801489 0.09936192 Grant-White #> 28  -1.03117301 0.3152173   0.8801489 0.09936192 Grant-White #> 29  -0.95022593 0.3152173   0.8801489 0.09936192 Grant-White #> 30  -0.11417634 0.3152173   0.8801489 0.09936192 Grant-White #> 31  -0.40048841 0.3152173   0.8801489 0.09936192 Grant-White #> 32   0.10506359 0.3152173   0.8801489 0.09936192 Grant-White #> 33  -0.33541128 0.3152173   0.8801489 0.09936192 Grant-White #> 34  -1.55565961 0.3152173   0.8801489 0.09936192 Grant-White #> 35  -0.84420068 0.3152173   0.8801489 0.09936192 Grant-White #> 36   0.07802839 0.3152173   0.8801489 0.09936192 Grant-White #> 37   0.20116597 0.3152173   0.8801489 0.09936192 Grant-White #> 38  -2.52639546 0.3152173   0.8801489 0.09936192 Grant-White #> 39  -0.69149096 0.3152173   0.8801489 0.09936192 Grant-White #> 40   2.02343787 0.3152173   0.8801489 0.09936192 Grant-White #> 41   0.97338078 0.3152173   0.8801489 0.09936192 Grant-White #> 42   0.42040588 0.3152173   0.8801489 0.09936192 Grant-White #> 43  -0.92605892 0.3152173   0.8801489 0.09936192 Grant-White #> 44  -0.56690032 0.3152173   0.8801489 0.09936192 Grant-White #> 45   0.12021889 0.3152173   0.8801489 0.09936192 Grant-White #> 46   0.62108043 0.3152173   0.8801489 0.09936192 Grant-White #> 47  -1.34219405 0.3152173   0.8801489 0.09936192 Grant-White #> 48   0.16258210 0.3152173   0.8801489 0.09936192 Grant-White #> 49  -0.03231810 0.3152173   0.8801489 0.09936192 Grant-White #> 50   0.24444031 0.3152173   0.8801489 0.09936192 Grant-White #> 51  -0.74431900 0.3152173   0.8801489 0.09936192 Grant-White #> 52  -0.43798842 0.3152173   0.8801489 0.09936192 Grant-White #> 53  -1.85297845 0.3152173   0.8801489 0.09936192 Grant-White #> 54  -0.82563527 0.3152173   0.8801489 0.09936192 Grant-White #> 55   1.20039010 0.3152173   0.8801489 0.09936192 Grant-White #> 56  -0.33287433 0.3152173   0.8801489 0.09936192 Grant-White #> 57   0.01996797 0.3152173   0.8801489 0.09936192 Grant-White #> 58   1.67582800 0.3152173   0.8801489 0.09936192 Grant-White #> 59  -0.71906808 0.3152173   0.8801489 0.09936192 Grant-White #> 60  -0.20504632 0.3152173   0.8801489 0.09936192 Grant-White #> 61   1.96214007 0.3152173   0.8801489 0.09936192 Grant-White #> 62  -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 63  -0.35343474 0.3152173   0.8801489 0.09936192 Grant-White #> 64  -1.95809255 0.3152173   0.8801489 0.09936192 Grant-White #> 65  -1.36021750 0.3152173   0.8801489 0.09936192 Grant-White #> 66   0.08595623 0.3152173   0.8801489 0.09936192 Grant-White #> 67  -0.23407652 0.3152173   0.8801489 0.09936192 Grant-White #> 68   0.67805696 0.3152173   0.8801489 0.09936192 Grant-White #> 69  -0.42951863 0.3152173   0.8801489 0.09936192 Grant-White #> 70  -0.69203290 0.3152173   0.8801489 0.09936192 Grant-White #> 71  -0.71583072 0.3152173   0.8801489 0.09936192 Grant-White #> 72  -0.19603459 0.3152173   0.8801489 0.09936192 Grant-White #> 73  -0.36767888 0.3152173   0.8801489 0.09936192 Grant-White #> 74   1.77425660 0.3152173   0.8801489 0.09936192 Grant-White #> 75  -0.67924187 0.3152173   0.8801489 0.09936192 Grant-White #> 76   0.07603337 0.3152173   0.8801489 0.09936192 Grant-White #> 77   1.49895127 0.3152173   0.8801489 0.09936192 Grant-White #> 78  -0.78813525 0.3152173   0.8801489 0.09936192 Grant-White #> 79  -1.35643816 0.3152173   0.8801489 0.09936192 Grant-White #> 80  -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 81   1.10806701 0.3152173   0.8801489 0.09936192 Grant-White #> 82   1.04768034 0.3152173   0.8801489 0.09936192 Grant-White #> 83   1.02242947 0.3152173   0.8801489 0.09936192 Grant-White #> 84   0.38236395 0.3152173   0.8801489 0.09936192 Grant-White #> 85   0.56447306 0.3152173   0.8801489 0.09936192 Grant-White #> 86   0.78803426 0.3152173   0.8801489 0.09936192 Grant-White #> 87   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 88   0.40383552 0.3152173   0.8801489 0.09936192 Grant-White #> 89  -0.44376277 0.3152173   0.8801489 0.09936192 Grant-White #> 90   0.08126577 0.3152173   0.8801489 0.09936192 Grant-White #> 91   0.14833793 0.3152173   0.8801489 0.09936192 Grant-White #> 92   0.53922219 0.3152173   0.8801489 0.09936192 Grant-White #> 93   0.53598481 0.3152173   0.8801489 0.09936192 Grant-White #> 94  -0.17024174 0.3152173   0.8801489 0.09936192 Grant-White #> 95   0.43610310 0.3152173   0.8801489 0.09936192 Grant-White #> 96   2.22843366 0.3152173   0.8801489 0.09936192 Grant-White #> 97   1.36480696 0.3152173   0.8801489 0.09936192 Grant-White #> 98   0.65135298 0.3152173   0.8801489 0.09936192 Grant-White #> 99   1.69439338 0.3152173   0.8801489 0.09936192 Grant-White #> 100 -0.15745068 0.3152173   0.8801489 0.09936192 Grant-White #> 101 -0.27680894 0.3152173   0.8801489 0.09936192 Grant-White #> 102  1.80473991 0.3152173   0.8801489 0.09936192 Grant-White #> 103 -0.03286005 0.3152173   0.8801489 0.09936192 Grant-White #> 104  0.21541008 0.3152173   0.8801489 0.09936192 Grant-White #> 105  0.63586648 0.3152173   0.8801489 0.09936192 Grant-White #> 106 -0.44122580 0.3152173   0.8801489 0.09936192 Grant-White #> 107  0.24389836 0.3152173   0.8801489 0.09936192 Grant-White #> 108  0.97392270 0.3152173   0.8801489 0.09936192 Grant-White #> 109  1.00619031 0.3152173   0.8801489 0.09936192 Grant-White #> 110  0.95967859 0.3152173   0.8801489 0.09936192 Grant-White #> 111  2.10529611 0.3152173   0.8801489 0.09936192 Grant-White #> 112  0.95012491 0.3152173   0.8801489 0.09936192 Grant-White #> 113  1.14033462 0.3152173   0.8801489 0.09936192 Grant-White #> 114 -0.41527450 0.3152173   0.8801489 0.09936192 Grant-White #> 115  0.50263334 0.3152173   0.8801489 0.09936192 Grant-White #> 116  0.00518188 0.3152173   0.8801489 0.09936192 Grant-White #> 117 -0.30961846 0.3152173   0.8801489 0.09936192 Grant-White #> 118 -0.38570232 0.3152173   0.8801489 0.09936192 Grant-White #> 119 -0.88747505 0.3152173   0.8801489 0.09936192 Grant-White #> 120 -1.11340047 0.3152173   0.8801489 0.09936192 Grant-White #> 121 -0.22830216 0.3152173   0.8801489 0.09936192 Grant-White #> 122  0.16781447 0.3152173   0.8801489 0.09936192 Grant-White #> 123 -1.16622845 0.3152173   0.8801489 0.09936192 Grant-White #> 124 -0.45277447 0.3152173   0.8801489 0.09936192 Grant-White #> 125 -0.69527031 0.3152173   0.8801489 0.09936192 Grant-White #> 126  1.16558552 0.3152173   0.8801489 0.09936192 Grant-White #> 127 -0.49081643 0.3152173   0.8801489 0.09936192 Grant-White #> 128  0.45412656 0.3152173   0.8801489 0.09936192 Grant-White #> 129 -0.75910506 0.3152173   0.8801489 0.09936192 Grant-White #> 130 -0.46232819 0.3152173   0.8801489 0.09936192 Grant-White #> 131  1.33631868 0.3152173   0.8801489 0.09936192 Grant-White #> 132 -0.78236094 0.3152173   0.8801489 0.09936192 Grant-White #> 133  0.11407532 0.3152173   0.8801489 0.09936192 Grant-White #> 134 -0.26111172 0.3152173   0.8801489 0.09936192 Grant-White #> 135 -0.58492377 0.3152173   0.8801489 0.09936192 Grant-White #> 136 -1.40872427 0.3152173   0.8801489 0.09936192 Grant-White #> 137 -0.24308825 0.3152173   0.8801489 0.09936192 Grant-White #> 138 -0.17601610 0.3152173   0.8801489 0.09936192 Grant-White #> 139 -0.74486092 0.3152173   0.8801489 0.09936192 Grant-White #> 140 -0.13419481 0.3152173   0.8801489 0.09936192 Grant-White #> 141 -0.74809830 0.3152173   0.8801489 0.09936192 Grant-White #> 142 -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 143  0.88737403 0.3152173   0.8801489 0.09936192 Grant-White #> 144 -0.05665784 0.3152173   0.8801489 0.09936192 Grant-White #> 145  0.58303847 0.3152173   0.8801489 0.09936192 Grant-White #>  #> attr(,\"av_efs\") #> attr(,\"av_efs\")$Pasteur #>            f1 #> f1 0.08995892 #>  #> attr(,\"av_efs\")$`Grant-White` #>            f1 #> f1 0.09936192 #>  #> attr(,\"fsA\") #> attr(,\"fsA\")$Pasteur #>           f1 #> f1 0.8833584 #>  #> attr(,\"fsA\")$`Grant-White` #>           f1 #> f1 0.8801489 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #>    intrcp #> f1      0 #>  #> attr(,\"fsb\")$`Grant-White` #>    intrcp #> f1      0 #>"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grandStandardizedSolution.html","id":null,"dir":"Reference","previous_headings":"","what":"Grand Standardized Solution — grandStandardizedSolution","title":"Grand Standardized Solution — grandStandardizedSolution","text":"Grand standardized solution two-stage path analysis model.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grandStandardizedSolution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grand Standardized Solution — grandStandardizedSolution","text":"","code":"grandStandardizedSolution(   object,   model_list = NULL,   se = TRUE,   acov_par = NULL,   free_list = NULL,   level = 0.95 )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grandStandardizedSolution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grand Standardized Solution — grandStandardizedSolution","text":"object object class lavaan. model_list list string variable describing structural path model, lavaan syntax. se Boolean variable. TRUE, standard errors grand standardized parameters computed. acov_par asymptotic variance-covariance matrix fitted model object. free_list list model matrices indicate position free parameters parameter vector. level confidence level required.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grandStandardizedSolution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Grand Standardized Solution — grandStandardizedSolution","text":"matrix standardized model parameters standard errors.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grandStandardizedSolution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Grand Standardized Solution — grandStandardizedSolution","text":"","code":"library(lavaan)  ## A single-group, two-factor example mod1 <- '    # latent variables      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4    # regressions      dem60 ~ ind60 ' fit1 <- sem(model = mod1,           data  = PoliticalDemocracy) grandStandardizedSolution(fit1) #> The grand standardized solution is equivalent to lavaan::standardizedSolution() for a model with a single group. #>     lhs op   rhs exo group block label est.std  se     z pvalue ci.lower #> 8 dem60  ~ ind60   0     1     1          0.46 0.1 4.593      0    0.264 #>   ci.upper #> 8    0.657  ## A single-group, three-factor example mod2 <- '     # latent variables       ind60 =~ x1 + x2 + x3       dem60 =~ y1 + y2 + y3 + y4       dem65 =~ y5 + y6 + y7 + y8     # regressions       dem60 ~ ind60       dem65 ~ ind60 + dem60 ' fit2 <- sem(model = mod2,             data  = PoliticalDemocracy) grandStandardizedSolution(fit2) #> The grand standardized solution is equivalent to lavaan::standardizedSolution() for a model with a single group. #>      lhs op   rhs exo group block label est.std    se      z pvalue ci.lower #> 12 dem60  ~ ind60   0     1     1         0.448 0.102  4.393  0.000    0.248 #> 13 dem65  ~ ind60   0     1     1         0.146 0.070  2.071  0.038    0.008 #> 14 dem65  ~ dem60   0     1     1         0.913 0.048 19.120  0.000    0.819 #>    ci.upper #> 12    0.648 #> 13    0.283 #> 14    1.006  ## A multigroup, two-factor example mod3 <- '   # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9   # regressions     visual ~ c(b1, b1) * speed ' fit3 <- sem(mod3, data = HolzingerSwineford1939,             group = \"school\",             group.equal = c(\"loadings\", \"intercepts\")) grandStandardizedSolution(fit3) #>       lhs op   rhs exo group block label est.std    se     z pvalue ci.lower #> 7  visual  ~ speed   0     1     1    b1   0.431 0.073 5.867      0    0.287 #> 30 visual  ~ speed   0     2     2    b1   0.431 0.073 5.867      0    0.287 #>    ci.upper #> 7     0.575 #> 30    0.575  ## A multigroup, three-factor example mod4 <- '   # latent variable definitions     visual =~ x1 + x2 + x3     textual =~ x4 + x5 + x6     speed =~ x7 + x8 + x9    # regressions     visual ~ c(b1, b1) * textual + c(b2, b2) * speed ' fit4 <- sem(mod4, data = HolzingerSwineford1939,             group = \"school\",             group.equal = c(\"loadings\", \"intercepts\")) grandStandardizedSolution(fit4) #>       lhs op     rhs exo group block label est.std    se     z pvalue ci.lower #> 10 visual  ~ textual   0     1     1    b1   0.419 0.073 5.704      0    0.275 #> 11 visual  ~   speed   0     1     1    b2   0.324 0.078 4.145      0    0.171 #> 46 visual  ~ textual   0     2     2    b1   0.419 0.073 5.704      0    0.275 #> 47 visual  ~   speed   0     2     2    b2   0.324 0.078 4.145      0    0.171 #>    ci.upper #> 10    0.563 #> 11    0.477 #> 46    0.563 #> 47    0.477"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":null,"dir":"Reference","previous_headings":"","what":"Two-Stage Path Analysis — tspa","title":"Two-Stage Path Analysis — tspa","text":"Fit two-stage path analysis (2S-PA) model.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Two-Stage Path Analysis — tspa","text":"","code":"tspa(   model,   data,   reliability = NULL,   se = NULL,   vc = NULL,   cross_loadings = NULL,   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Two-Stage Path Analysis — tspa","text":"model string variable describing structural path model, lavaan syntax. data data frame containing factor scores. reliability numeric vector representing reliability indexes latent factor. Currently tspa() support reliability argument. Please use se. se numeric vector representing standard errors factor score variable single-group 2S-PA. list data frame storing standard errors group latent factor multigroup 2S-PA. vc error variance-covariance matrix factor scores, can obtained output get_fs() using attr() argument = \"av_efs\". cross_loadings matrix loadings cross-loadings latent variables factor scores fs, can obtained output get_fs() using attr() argument = \"fsA\". details see multiple-factors vignette: vignette(\"multiple-factors\", package = \"R2spa\"). ... Additional arguments passed sem. See lavOptions complete list.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Two-Stage Path Analysis — tspa","text":"object class lavaan, attribute tspaModel contains model syntax.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Two-Stage Path Analysis — tspa","text":"","code":"library(lavaan)  # single-group, two-factor example, factor scores obtained separately # get factor scores fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                        model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                        model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60) # tspa model tspa(model = \"dem60 ~ ind60\", data = fs_dat,      se = c(ind60 = fs_dat_ind60[1, \"fs_ind60_se\"],             dem60 = fs_dat_dem60[1, \"fs_dem60_se\"])) #> lavaan 0.6.15 ended normally after 17 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         3 #>  #>   Number of observations                            75 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0  # single-group, three-factor example mod2 <- \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     dem65 =~ y5 + y6 + y7 + y8 \" fs_dat2 <- get_fs(PoliticalDemocracy, model = mod2, std.lv = TRUE) tspa(model = \"dem60 ~ ind60               dem65 ~ ind60 + dem60\",      data = fs_dat2,      vc = attr(fs_dat2, \"av_efs\"),      cross_loadings = attr(fs_dat2, \"fsA\")) #> lavaan 0.6.15 ended normally after 21 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         6 #>  #>   Number of observations                            75 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0  # multigroup, two-factor example mod3 <- \"   # latent variables     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9 \" fs_dat3 <- get_fs(HolzingerSwineford1939, model = mod3, std.lv = TRUE,                   group = \"school\") tspa(model = \"visual ~ speed\",      data = fs_dat3,      vc = attr(fs_dat3, \"av_efs\"),      cross_loadings = attr(fs_dat3, \"fsA\"),      group = \"school\") #> lavaan 0.6.15 ended normally after 28 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0 #>   Test statistic for each group: #>     Pasteur                                      0.000 #>     Grant-White                                  0.000  # multigroup, three-factor example mod4 <- \"   # latent variables     visual =~ x1 + x2 + x3     textual =~ x4 + x5 + x6     speed =~ x7 + x8 + x9  \" fs_dat4 <- get_fs(HolzingerSwineford1939, model = mod4, std.lv = TRUE,                   group = \"school\") tspa(model = \"visual ~ speed               textual ~ visual + speed\",      data = fs_dat4,      vc = attr(fs_dat4, \"av_efs\"),      cross_loadings = attr(fs_dat4, \"fsA\"),      group = \"school\") #> lavaan 0.6.15 ended normally after 39 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        18 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0 #>   Test statistic for each group: #>     Pasteur                                      0.000 #>     Grant-White                                  0.000  # get factor scores fs_dat_visual <- get_fs(data = HolzingerSwineford1939,                         model = \"visual =~ x1 + x2 + x3\",                         group = \"school\") fs_dat_speed <- get_fs(data = HolzingerSwineford1939,                        model = \"speed =~ x7 + x8 + x9\",                        group = \"school\") fs_hs <- cbind(do.call(rbind, fs_dat_visual),                do.call(rbind, fs_dat_speed))  # tspa model tspa(model = \"visual ~ speed\",      data = fs_hs,      se = data.frame(visual = c(0.3391326, 0.311828),                      speed = c(0.2786875, 0.2740507)),      group = \"school\",      group.equal = \"regressions\") #> lavaan 0.6.15 ended normally after 20 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>   Number of equality constraints                     1 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.014 #>   Degrees of freedom                                 1 #>   P-value (Chi-square)                           0.907 #>   Test statistic for each group: #>     Pasteur                                      0.010 #>     Grant-White                                  0.003  # manually adding equality constraints on the regression coefficients tspa(model = \"visual ~ c(b1, b1) * speed\",      data = fs_hs,      se = list(visual = c(0.3391326, 0.311828),                speed = c(0.2786875, 0.2740507)),      group = \"school\") #> lavaan 0.6.15 ended normally after 20 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>   Number of equality constraints                     1 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.014 #>   Degrees of freedom                                 1 #>   P-value (Chi-square)                           0.907 #>   Test statistic for each group: #>     Pasteur                                      0.010 #>     Grant-White                                  0.003"},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"r2spa-002","dir":"Changelog","previous_headings":"","what":"R2spa 0.0.2","title":"R2spa 0.0.2","text":"Use pkgdown create website, GitHub action (#22) get_fs() now returns list multi-group models (#29). New function grandStandardizedSolution() computes standardized solution based grand mean grand SD (#13). tspa() gains argument vc cross_loadings, useful factor scores obtained multi-factor models (#7). See vignette(\"multiple-factors\").","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"r2spa-001","dir":"Changelog","previous_headings":"","what":"R2spa 0.0.1","title":"R2spa 0.0.1","text":"Work--Progress! 0.0.1 version","code":""}]
