[{"path":"https://gengrui-zhang.github.io/R2spa/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 R2spa authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/R2spa.html","id":"single-group-single-predictor","dir":"Articles","previous_headings":"","what":"Single group, single predictor1","title":"Two-Stage Path Analysis (2S-PA) Model Examples","text":"call tspa(), data frame factor scores needed latent variables. get data frame, apply get_fs() latent variables specify model parameter respective definitions. Combine factor scores latent variable using cbind() can used tspa() model building. build Two-Stage Path Analysis model, simply call tspa() function model = regressions, data = combined factor score data frame using get_fs(), specify standard error either list data frame. Values standard error can found column named fs_[variable name]_se. example, standard error latent variable ind60 can found column fs_ind60_se fs_dat data frame. view Two-Stage Path Analysis model, use attributes([model name])$tspaModel. Function cat() can help tidy model output. output, values error constraints computed squaring standard errors previous section.","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4    # regressions     dem60 ~ ind60 ' fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60) tspa_fit <- tspa(model = \"dem60 ~ ind60\",                   data = fs_dat,                   se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472)) cat(attributes(tspa_fit)$tspaModel) ## # latent variables (indicated by factor scores) ## ind60=~ c(1) * fs_ind60 ## dem60=~ c(1) * fs_dem60 ##  ## # constrain the errors ## fs_ind60~~ c(0.01472861368225) * fs_ind60 ## fs_dem60~~ c(0.45649913886784) * fs_dem60 ##  ## # structural model ## dem60 ~ ind60"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/R2spa.html","id":"single-group-multiple-predictors","dir":"Articles","previous_headings":"","what":"Single group, multiple predictors","title":"Two-Stage Path Analysis (2S-PA) Model Examples","text":"call tspa(), data frame factor scores needed latent variables. get data frame, apply get_fs() latent variables specify model parameters respective definitions. Combine factor scores latent variables using cbind() call tspa() model building. build Two-Stage Path Analysis model, simply call tspa() function model = regressions (predictors), data = factor score data frame created combining results get_fs(), specify standard errors either list data frame. Values standard errors can found column named fs_[variable name]_se.2 example, standard error latent variable ind60 can found column fs_ind60_se fs_dat data frame. output model, values error constraints computed squaring standard errors.","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # # residual correlations   #   y1 ~~ y5   #   y2 ~~ y4 + y6   #   y3 ~~ y7   #   y4 ~~ y8   #   y6 ~~ y8 ' fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat_dem65 <- get_fs(data = PoliticalDemocracy,                         model = \"dem65 =~ y5 + y6 + y7 + y8\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60, fs_dat_dem65) tspa_3var_fit <- tspa(model = \"dem60 ~ ind60                           dem65 ~ ind60 + dem60\",                        data = fs_dat,                        se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472,                                     dem65 = 0.5724405)) cat(attributes(tspa_3var_fit)$tspaModel) ## # latent variables (indicated by factor scores) ## ind60=~ c(1) * fs_ind60 ## dem60=~ c(1) * fs_dem60 ## dem65=~ c(1) * fs_dem65 ##  ## # constrain the errors ## fs_ind60~~ c(0.01472861368225) * fs_ind60 ## fs_dem60~~ c(0.45649913886784) * fs_dem60 ## fs_dem65~~ c(0.32768812604025) * fs_dem65 ##  ## # structural model ## dem60 ~ ind60 ##                           dem65 ~ ind60 + dem60"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/R2spa.html","id":"multigroup-single-predictor","dir":"Articles","previous_headings":"","what":"Multigroup, single predictor","title":"Two-Stage Path Analysis (2S-PA) Model Examples","text":"call tspa(), data frame factor scores needed multigroup variables. get data frame, apply get_fs() multigroup variables specify model parameter respective definitions. Combine factor scores multigroup variables using cbind() can fed tspa() model building. build Two-Stage Path Analysis model, simply call tspa() function model = regression relation. Specify standard error either list data frame. Values standard error can found column named fs_[variable name]_se. example, standard error multigroup variable visual can found column fs_visual_se fs_hs data frame. get standard errors group faster, unique() can called upon standard error column. example, case, unique(fs_hs$fs_visual_se) can called get standard errors multigroup variable visual. Function standardizedsolution() enables user view table standard error, z score, p-value, lower bound confidence interval multigroup regression relation. view Two-Stage Path Analysis model multigroup, use attributes([model name])$tspaModel. Function cat() can help tidy model output. output, values error constraints computed squaring standard errors previous section.","code":"model <- '    # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9    # regressions     visual ~ speed ' hs_mod <- ' visual =~ x1 + x2 + x3 speed =~ x7 + x8 + x9 '  # get factor scores fs_dat_visual <- get_fs(data = HolzingerSwineford1939,                          model = \"visual =~ x1 + x2 + x3\",                          group = \"school\") fs_dat_speed <- get_fs(data = HolzingerSwineford1939,                         model = \"speed =~ x7 + x8 + x9\",                         group = \"school\") fs_hs <- cbind(do.call(rbind, fs_dat_visual),                do.call(rbind, fs_dat_speed)) # tspa model tspa_fit <- tspa(model = \"visual ~ speed\",                  data = fs_hs,                  se_fs = data.frame(visual = c(0.3391326, 0.311828),                                     speed = c(0.2786875, 0.2740507)),                  group = \"school\"                  # group.equal = \"regressions\"                  ) stdsol <- standardizedsolution(tspa_fit) subset(stdsol, subset = op == \"~\") ##       lhs op   rhs group est.std    se     z pvalue ci.lower ci.upper ## 5  visual  ~ speed     1   0.277 0.114 2.423  0.015    0.053    0.501 ## 16 visual  ~ speed     2   0.439 0.095 4.627  0.000    0.253    0.625 cat(attributes(tspa_fit)$tspaModel) ## # latent variables (indicated by factor scores) ## visual=~ c(1, 1) * fs_visual ## speed=~ c(1, 1) * fs_speed ##  ## # constrain the errors ## fs_visual~~ c(0.11501092038276, 0.097236701584) * fs_visual ## fs_speed~~ c(0.07766672265625, 0.07510378617049) * fs_speed ##  ## # structural model ## visual ~ speed"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"simulate-data","dir":"Articles","previous_headings":"","what":"Simulate Data","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"","code":"#' Population parameters num_obs <- 5000  # relatively large sample cov_xm <- .45 betas <- c(.2, .3, -.4) # Results seem more accurate with more items lambdax <- 1.7 * c(.6, .7, .8, .5, .5, .3, .9, .7, .6)  # on normal ogive metric lambdam <- 1.7 * c(.8, .7, .7, .6, .5)  # on normal ogive metric lambday <- c(.5, .7, .9) thresx <- matrix(c(1, 1.5, 1.3, 0.5, 2, -0.5, -1, 0.5, 0.8), nrow = 1)  # binary thresm <- matrix(c(-2, -2, -0.4, -0.5, 0,                    -1.5, -0.5, 0, 0, 0.5,                    -1, 1, 1.5, 0.8, 1), nrow = 3,                  byrow = TRUE)  # ordinal with 4 categories inty <- 1.5  # intercept of y # error variance of y r2y <- crossprod(     betas,     matrix(c(1, cov_xm, 0, cov_xm, 1, 0, 0, 0, 1 + cov_xm^2), nrow = 3)     ) %*% betas evar <- as.numeric(1 - r2y)  #' Simulate latent variables X and M with variances of 1, and the disturbance of #' Y cov_xm_ey <- matrix(c(1, cov_xm, 0,                       cov_xm, 1, 0,                       0, 0, evar), nrow = 3) eta <- MASS::mvrnorm(num_obs, mu = rep(0, 3), Sigma = cov_xm_ey,                      empirical = TRUE) # Add product term eta <- cbind(eta, eta[, 1] * eta[, 2])  #' Compute latent y (standardized) etay <- inty + eta[, -3] %*% betas + eta[, 3] # Verify the structural coefficients with eta known lm(etay ~ eta[, -3]) #>  #> Call: #> lm(formula = etay ~ eta[, -3]) #>  #> Coefficients: #> (Intercept)   eta[, -3]1   eta[, -3]2   eta[, -3]3   #>      1.4938       0.2002       0.2999      -0.3862  #' Simulate y (continuous) y <- t(     lambday %*% t(etay) +         rnorm(num_obs * length(lambday), sd = sqrt(1 - lambday^2)) )  #' Simulate latent continuous responses for X and M xstar <- eta[, 1] %*% t(lambdax) + rnorm(num_obs * length(lambdax)) mstar <- eta[, 2] %*% t(lambdam) + rnorm(num_obs * length(lambdam)) #' Obtain categorical items x <- vapply(     seq_along(lambdax),     FUN = \\(i) {         findInterval(xstar[, i], thresx[, i])     },     FUN.VALUE = numeric(num_obs)) m <- vapply(     seq_along(lambdam),     FUN = \\(i) {         findInterval(mstar[, i], thresm[, i])     },     FUN.VALUE = numeric(num_obs))"},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"y","dir":"Articles","previous_headings":"Compute Factor Scores","what":"Y","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"","code":"fsy <- get_fs(data = y, std.lv = TRUE)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"x","dir":"Articles","previous_headings":"Compute Factor Scores","what":"X","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"example, use EAP scores, conceptually similar regression scores continuous data.","code":"irtx <- mirt(data.frame(x), itemtype = \"2PL\",              verbose = FALSE)  # IRT (2-PL) for x marginal_rxx(irtx)  # marginal reliability #> [1] 0.7606752 fsx <- fscores(irtx, full.scores.SE = TRUE) empirical_rxx(fsx)  # empirical reliability #>        F1  #> 0.7732511"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"m","dir":"Articles","previous_headings":"Compute Factor Scores","what":"M","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"","code":"irtm <- mirt(data.frame(m), itemtype = \"graded\",              verbose = FALSE)  # IRT (GRM) for m marginal_rxx(irtm)  # marginal reliability #> [1] 0.7931549 fsm <- fscores(irtm, full.scores.SE = TRUE) empirical_rxx(fsm)  # empirical reliability #>        F1  #> 0.7942367"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"loading-and-error-variances-for-product-of-factor-scores","dir":"Articles","previous_headings":"","what":"Loading and Error Variances for Product of Factor scores","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"Let η̃X\\tilde \\eta_X η̃M\\tilde \\eta_M factor scores predictor moderator. η̃Xη̃M=(λXηX+eX̃)(λMηM+eM̃)=(λXλMηXηM)+λXηXeM+λMηMeX+eXeM\\tilde \\eta_X \\tilde \\eta_M = (\\lambda_X \\eta_X + e_{\\tilde X}) (\\lambda_M \\eta_M + e_{\\tilde M}) = (\\lambda_X \\lambda_M \\eta_X \\eta_M) + \\lambda_X \\eta_X e_M + \\lambda_M \\eta_M e_X + e_X e_M loading λXλM\\lambda_X \\lambda_M, error variance product indicator λX2V(ηX)V(eM)+λM2V(ηM)V(eX)+V(eX)V(eM)\\lambda_X^2 V(\\eta_X) V(e_M) + \\lambda_M^2 V(\\eta_M) V(e_X) + V(e_X) V(e_M)","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"directly-using-factor-scores","dir":"Articles","previous_headings":"","what":"Directly Using Factor Scores","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"","code":"# Assemble data fs_dat <- data.frame(fsy[c(1, 3, 4)], fsx, fsm) |>     setNames(c(         \"fsy\", \"rel_fsy\", \"ev_fsy\",         \"fsx\", \"se_fsx\", \"fsm\", \"se_fsm\"     )) |>     # Compute reliability; only needed for 2S-PA     within(expr = {         rel_fsx <- 1 - se_fsx^2         rel_fsm <- 1 - se_fsm^2         ev_fsx <- se_fsx^2 * (1 - se_fsx^2)         ev_fsm <- se_fsm^2 * (1 - se_fsm^2)     }) |>     # Add interaction     within(expr = {         fsxm <- fsx * fsm         ld_fsxm <- rel_fsx * rel_fsm         ev_fsxm <- rel_fsx^2 * ev_fsm + rel_fsm^2 * ev_fsx + ev_fsm * ev_fsx     }) lm(fsy ~ fsx * fsm, data = fs_dat)  # some bias #>  #> Call: #> lm(formula = fsy ~ fsx * fsm, data = fs_dat) #>  #> Coefficients: #> (Intercept)          fsx          fsm      fsx:fsm   #>     0.09867      0.19539      0.25989     -0.35788"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/categorical-interaction.html","id":"two-stage-path-analysis","dir":"Articles","previous_headings":"","what":"Two-Stage Path Analysis","title":"Using Two-Stage Path Analysis with Definition Variables for Latent Interactions with Categorical Indicators","text":"Create OpenMx model without latent variables Create loading error covariance matrices Combine (1) (2), run 2S-PA","code":"fsreg_mx <- mxModel(\"str\",     type = \"RAM\",     manifestVars = c(\"fsy\", \"fsx\", \"fsm\", \"fsxm\"),     # Path coefficients     mxPath(         from = c(\"fsx\", \"fsm\", \"fsxm\"), to = \"fsy\",         values = 0     ),     # Variances     mxPath(         from = c(\"fsy\", \"fsx\", \"fsm\", \"fsxm\"),         to = c(\"fsy\", \"fsx\", \"fsm\", \"fsxm\"),         arrows = 2,         values = c(0.6, 1, 1, 1)     ),     # Covariances     mxPath(         from = c(\"fsx\", \"fsm\", \"fsxm\"),         to = c(\"fsx\", \"fsm\", \"fsxm\"),         arrows = 2, connect = \"unique.pairs\",         values = 0     ),     mxPath(         from = \"one\", to = c(\"fsy\", \"fsx\", \"fsm\", \"fsxm\"),         values = 0     ) ) fsreg_umx <- umxLav2RAM(     \"       fsy ~ b1 * fsx + b2 * fsm + b3 * fsxm       fsy + fsx + fsm + fsxm ~ 1       fsx ~~ vx * fsx + fsm + fsxm       fsm ~~ vm * fsm + fsxm     \",     printTab = FALSE) #>  #> ?plot.MxModel options: std, means, digits, strip_zero, file, splines=T/F/ortho,..., min=, max =, same = , fixed, resid= 'circle|line|none' DiagrammeR::grViz(omxGraphviz(fsreg_mx)) #> digraph \"str\" { #>   node [style=filled, fontname=\"Arial\", fontsize=16]; #>       /* Manifest Variables */ #>       { rank = max; fsy; fsx; fsm; fsxm } #>   fsy [shape=square, fillcolor=\"#a9fab1\", height=0.5, width=0.5]; #>   fsx [shape=square, fillcolor=\"#a9fab1\", height=0.5, width=0.5]; #>   fsm [shape=square, fillcolor=\"#a9fab1\", height=0.5, width=0.5]; #>   fsxm [shape=square, fillcolor=\"#a9fab1\", height=0.5, width=0.5]; #> /* Means */ #>   one [shape=triangle]; #> /* Paths */ #>   fsx -> fsy[dir=forward]; #>   fsm -> fsy[dir=forward]; #>   fsxm -> fsy[dir=forward]; #>   fsy -> fsy[dir=both, headport=s, tailport=s]; #>   fsx -> fsx[dir=both, headport=s, tailport=s]; #>   fsx -> fsm[dir=both]; #>   fsx -> fsxm[dir=both]; #>   fsm -> fsm[dir=both, headport=s, tailport=s]; #>   fsm -> fsxm[dir=both]; #>   fsxm -> fsxm[dir=both, headport=s, tailport=s]; #>   one -> fsy[dir=forward]; #>   one -> fsx[dir=forward]; #>   one -> fsm[dir=forward]; #>   one -> fsxm[dir=forward]; #> } # Loading matL <- mxMatrix(     type = \"Diag\", nrow = 4, ncol = 4,     free = FALSE,     labels = c(\"data.rel_fsy\", \"data.rel_fsx\", \"data.rel_fsm\", \"data.ld_fsxm\"),     name = \"L\" ) # Error matE <- mxMatrix(     type = \"Diag\", nrow = 4, ncol = 4,     free = FALSE,     labels = c(\"data.ev_fsy\", \"data.ev_fsx\", \"data.ev_fsm\", \"data.ev_fsxm\"),     name = \"E\" ) tspa_mx <-     tspa_mx_model(         mxModel(fsreg_umx,                 mxAlgebra(b1 * sqrt(vx), name = \"stdx_b1\"),                 mxAlgebra(b2 * sqrt(vm), name = \"stdx_b2\"),                 mxAlgebra(b3 * sqrt(vx) * sqrt(vm), name = \"stdx_b3\"),                 mxCI(c(\"stdx_b1\", \"stdx_b2\", \"stdx_b3\"))),         data = fs_dat,         mat_ld = matL, mat_ev = matE) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx, intervals = TRUE) #> Running 2SPAD with 14 parameters # Summarize the results (takes a minute or so) # Also include the X-Standardized coefficients summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>              name matrix  row  col      Estimate  Std.Error A #> 1              b1   m1.A  fsy  fsx  2.064624e-01 0.01897723   #> 2              b2   m1.A  fsy  fsm  2.989998e-01 0.01870619   #> 3              b3   m1.A  fsy fsxm -4.152434e-01 0.01869847   #> 4    fsy_with_fsy   m1.S  fsy  fsy  6.398847e-01 0.01956885   #> 5              vx   m1.S  fsx  fsx  9.991969e-01 0.02776582   #> 6    fsm_with_fsx   m1.S  fsx  fsm  4.455208e-01 0.01927455   #> 7              vm   m1.S  fsm  fsm  9.996933e-01 0.02685778   #> 8   fsx_with_fsxm   m1.S  fsx fsxm  2.425816e-02 0.02230417   #> 9   fsm_with_fsxm   m1.S  fsm fsxm  4.883996e-03 0.02200107   #> 10 fsxm_with_fsxm   m1.S fsxm fsxm  1.035316e+00 0.03835927   #> 11     one_to_fsy   m1.M    1  fsy  1.871730e-01 0.01612887   #> 12     one_to_fsx   m1.M    1  fsx -1.368772e-04 0.01615666   #> 13     one_to_fsm   m1.M    1  fsm -6.265561e-05 0.01587288   #> 14    one_to_fsxm   m1.M    1 fsxm  4.506424e-01 0.01832363   #>  #> confidence intervals: #>                     lbound   estimate     ubound note #> m1.stdx_b1[1,1]  0.1693079  0.2063795  0.2436209      #> m1.stdx_b2[1,1]  0.2622028  0.2989540  0.3356675      #> m1.stdx_b3[1,1] -0.4563615 -0.4150129 -0.3757701      #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:             14                  39986              50641.35 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 5000/40000 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -29330.65               50669.35                 50669.43 #> BIC:     -289927.14               50760.59                 50716.10 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-09-06 04:18:38  #> Wall clock time: 170.3362 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.12  #> Need help?  See help(mxSummary) # Standard errors for X-Standardized coefficients mxSE(m1.stdx_b1, tspa_mx_fit) #> Treating first argument as an expression #>            [,1] #> [1,] 0.01891419 mxSE(m1.stdx_b2, tspa_mx_fit) #> Treating first argument as an expression #>            [,1] #> [1,] 0.01868014 mxSE(m1.stdx_b3, tspa_mx_fit) #> Treating first argument as an expression #>            [,1] #> [1,] 0.02048697"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"first-order-correction-for-se","dir":"Articles","previous_headings":"","what":"First-order correction for SE","title":"Corrected Standard Errors","text":"V̂γ,c=V̂γ+𝐉𝛄(𝛉̂)V̂θ𝐉𝛄(𝛉̂)⊤,\\hat V_{\\gamma, c} = \\hat V_{\\gamma} + \\boldsymbol{J}_\\boldsymbol{\\gamma}(\\hat{\\boldsymbol{\\theta}}) \\hat V_{\\theta} \\boldsymbol{J}_\\boldsymbol{\\gamma}(\\hat{\\boldsymbol{\\theta}})^\\top, 𝐉𝛄\\boldsymbol{J}_\\boldsymbol{\\gamma} Jacobian matrix 𝛄̂\\hat{\\boldsymbol{\\gamma}} respect 𝛉\\boldsymbol{\\theta}, V̂γ,c=V̂γ+(𝐇γ)−1(∂2ℓ∂θ∂γ⊤)V̂θ(∂2ℓ∂θ∂γ⊤)⊤(𝐇γ)−1,\\hat V_{\\gamma, c} = \\hat V_{\\gamma} + (\\boldsymbol{H}_\\gamma)^{-1} \\left(\\frac{\\partial^2 \\ell}{\\partial \\theta \\partial \\gamma^\\top}\\right) \\hat V_{\\theta} \\left(\\frac{\\partial^2 \\ell}{\\partial \\theta \\partial \\gamma^\\top}\\right)^\\top (\\boldsymbol{H}_\\gamma)^{-1}, VγV_{\\gamma} naive covariance matrix structural parameter estimates 𝛄̂\\hat{\\boldsymbol{\\gamma}} assuming measurement error variance parameter, 𝛉\\boldsymbol{\\theta}, known, 𝐇γ\\boldsymbol{H}_\\gamma Hessian matrix log-likelihood ℓ\\ell respect 𝛄̂\\hat{\\boldsymbol{\\gamma}}, VθV_{\\theta} can obtained first-stage measurement model analysis. example based Political Democracy data used https://lavaan.ugent./tutorial/sem.html example Bollen (1989)’s book.","code":"library(lavaan) library(R2spa) library(numDeriv) library(boot)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"separate-measurement-models","dir":"Articles","previous_headings":"","what":"Separate Measurement Models","title":"Corrected Standard Errors","text":"Standardized solution Compare joint structural measurement model Bootstrap Standard Errors standard errors seem diverge slightly among methods. SE(dem60~ind60) bootstrap lowest. SE(ind60~~ind60) joint model particularly higher two. SE(dem60~~dem60) lowest corrected 2S-PA. pointed joint model 2S-PA different estimators.","code":"model <- '   # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4    # regressions     dem60 ~ ind60 ' cfa_ind60 <- cfa(\"ind60 =~ x1 + x2 + x3\", data = PoliticalDemocracy) # Regression factor scores fs1 <- get_fs_lavaan(cfa_ind60, vfsLT = TRUE) # Delta method variance of (loading, intercept) vldev1 <- attr(fs1, which = \"vfsLT\") cfa_dem60 <- cfa(\"dem60 =~ y1 + y2 + y3 + y4\",                  data = PoliticalDemocracy) # Regression factor scores fs2 <- get_fs_lavaan(cfa_dem60, vfsLT = TRUE) # Delta method variance of (loading, intercept) vldev2 <- attr(fs2, which = \"vfsLT\") fs_dat <- data.frame(   fs_ind60 = fs1$fs_ind60,   fs_dem60 = fs2$fs_dem60 ) # Combine sampling variance of loading and error variance # Note: loadings first, then error variance vldev <- block_diag(vldev1, vldev2)[c(1, 3, 2, 4), c(1, 3, 2, 4)] # 2S-PA # Assemble loadings ld <- block_diag(attr(fs1, which = \"fsL\"),                  attr(fs2, which = \"fsL\")) ev <- block_diag(attr(fs1, which = \"fsT\"),                  attr(fs2, which = \"fsT\")) tspa_fit <- tspa(model = \"dem60 ~ ind60\",                  data = fs_dat,                  fsL = ld,                  fsT = ev) # Unadjusted covariance vcov(tspa_fit) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.131               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.006  0.000  0.477 # Adjusted covariance matrix (vc_cor <- vcov_corrected(tspa_fit, vfsLT = vldev, which_free = c(1, 4, 5, 7))) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.133               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60  0.001  0.000  0.518 # Corrected standard errors sqrt(diag(vc_cor)) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.36415174   0.07585532   0.71996886 tspa_est_std <- function(ld_ev) {   ld1 <- ld   ev1 <- ev   ld1[c(1, 4)] <- ld_ev[1:2]   ev1[c(1, 4)] <- ld_ev[3:4]   tfit <- tspa(model = \"dem60 ~ ind60\",                data = fs_dat,                fsL = ld1,                fsT = ev1)   standardizedSolution(tfit)$est.std[8] } tspa_est_std(c(ld[c(1, 4)], ev[c(1, 4)])) #> [1] 0.4531693 # Jacobian jac_std <- numDeriv::jacobian(tspa_est_std,                               x = c(ld[c(1, 4)], ev[c(1, 4)])) # Corrected standard error sqrt(   standardizedSolution(tspa_fit)[8, \"se\"]^2 +     jac_std %*% vldev %*% t(jac_std) ) #>           [,1] #> [1,] 0.1013887 sem_fit <- sem(model, data = PoliticalDemocracy) # Larger standard error (vc_j <-    vcov(sem_fit)[c(\"dem60~ind60\", \"ind60~~ind60\", \"dem60~~dem60\"),                 c(\"dem60~ind60\", \"ind60~~ind60\", \"dem60~~dem60\")]) #>               dem60~ind60  ind60~~ind60  dem60~~dem60 #> dem60~ind60   0.143355743 -0.0033833569  0.0648673299 #> ind60~~ind60 -0.003383357  0.0075268147 -0.0001098306 #> dem60~~dem60  0.064867330 -0.0001098306  0.7655699561 sqrt(diag(vc_j)) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.37862348   0.08675722   0.87496855 run_tspa <- function(df, inds) {   fs_ind60 <- get_fs(data = df[inds, ], model = \"ind60 =~ x1 + x2 + x3\",                      se = \"none\", test = \"none\")   fs_dem60 <- get_fs(data = df[inds, ], model = \"dem60 =~ y1 + y2 + y3 + y4\",                      se = \"none\", test = \"none\")   fs_dat <- cbind(fs_ind60, fs_dem60)   # Assemble loadings   ld <- block_diag(attr(fs_ind60, which = \"fsL\"),                    attr(fs_dem60, which = \"fsL\"))   ev <- block_diag(attr(fs_ind60, which = \"fsT\"),                    attr(fs_dem60, which = \"fsT\"))   tspa_fit <- tspa(model = \"dem60 ~ ind60\",                    data = fs_dat,                    fsL = ld,                    fsT = ev,                    test = \"none\")   coef(tspa_fit) } boo <- boot(PoliticalDemocracy, statistic = run_tspa, R = 1999) # Use MAD to downweigh outlying replications boo$t |>     apply(MARGIN = 2, FUN = mad) |>     setNames(c(\"dem60~ind60\", \"ind60~~ind60\", \"dem60~~dem60\")) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.32521386   0.07271009   0.87676911 run_sem <- function(df, inds) {   sem_fit <- sem(model, data = df[inds, ], se = \"none\", test = \"none\")   coef(sem_fit) } boo <- boot(PoliticalDemocracy, statistic = run_sem, R = 999)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"joint-measurement-model","dir":"Articles","previous_headings":"","what":"Joint Measurement Model","title":"Corrected Standard Errors","text":"Bootstrap Standard Errors","code":"cfa_joint <- cfa(\"ind60 =~ x1 + x2 + x3                   dem60 =~ y1 + y2 + y3 + y4\",                  data = PoliticalDemocracy) # Factor score fs_joint <- get_fs_lavaan(cfa_joint, vfsLT = TRUE) # Delta method variance vldev_joint <- attr(fs_joint, which = \"vfsLT\") tspa_fit2 <- tspa(model = \"dem60 ~ ind60\",                   data = fs_joint,                   fsT = attr(fs_joint, \"fsT\"),                   fsL = attr(fs_joint, \"fsL\")) # Unadjusted covariance vcov(tspa_fit2) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.124               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.006  0.000  0.430 # Adjusted covariance (vc2_cor <- vcov_corrected(tspa_fit2, vfsLT = vldev_joint)) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.130               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.009  0.000  0.493 # Corrected standard errors sqrt(diag(vc2_cor)) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.36049260   0.07654824   0.70229676 run_tspa2 <- function(df, inds) {   fs_joint <- get_fs(df[inds, ],                      model = \"ind60 =~ x1 + x2 + x3                               dem60 =~ y1 + y2 + y3 + y4\",                      se = \"none\", test = \"none\")   tspa_fit2 <- tspa(model = \"dem60 ~ ind60\",                     data = fs_joint,                     fsT = attr(fs_joint, \"fsT\"),                     fsL = attr(fs_joint, \"fsL\"),                     test = \"none\")   coef(tspa_fit2) } boo2 <- boot(PoliticalDemocracy, statistic = run_tspa2, R = 1999) # run_tspa2b <- function(df, inds) { #   fsb_joint <- get_fs(df[inds, ], #                       model = \"ind60 =~ x1 + x2 + x3 #                               dem60 =~ y1 + y2 + y3 + y4\", #                       se = \"none\", test = \"none\", method = \"Bartlett\") #   tspa_fit2b <- tspa(model = \"dem60 ~ ind60\", #                      data = fsb_joint, #                      fsT = attr(fsb_joint, \"fsT\"), #                      fsL = diag(2), #                      test = \"none\") #   coef(tspa_fit2b) # } # boo2b <- boot(PoliticalDemocracy, statistic = run_tspa2b, R = 4999) # run_sem2 <- function(df, inds) { #   sem_fit <- sem(model, data = df[inds, ]) #   coef(sem_fit)[c(6, 14, 15)] # } # boo2j <- boot(PoliticalDemocracy, statistic = run_sem2, R = 4999) # Use MAD to downweigh outlying replications boo2$t |>     apply(MARGIN = 2, FUN = mad) |>     setNames(c(\"dem60~ind60\", \"ind60~~ind60\", \"dem60~~dem60\")) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.34364943   0.07851415   0.88168143"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"with-bartletts-method","dir":"Articles","previous_headings":"Joint Measurement Model","what":"With Bartlett’s Method","title":"Corrected Standard Errors","text":"","code":"# Factor score fsb_joint <- get_fs(PoliticalDemocracy,                     model = \"ind60 =~ x1 + x2 + x3                              dem60 =~ y1 + y2 + y3 + y4\",                     method = \"Bartlett\",                     vfsLT = TRUE) vldevb_joint <- attr(fsb_joint, which = \"vfsLT\") tspa_fit2b <- tspa(model = \"dem60 ~ ind60\",                    data = fsb_joint,                    fsT = attr(fsb_joint, \"fsT\"),                    fsL = diag(2) |>                        `dimnames<-`(list(c(\"fs_ind60\", \"fs_dem60\"),                                          c(\"ind60\", \"dem60\")))) # Unadjusted covariance matrix vcov(tspa_fit2b) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.124               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.006  0.000  0.430 # Adjusted covariance matrix (vc2b_cor <- vcov_corrected(     tspa_fit2b,     # Exclude fixed loadings and error variance     vfsLT = vldevb_joint[c(5, 7), c(5, 7)],     # Specify which elements are free (error variances only)     which_free = c(5, 7))) #>              d60~60 i60~~6 d60~~6 #> dem60~ind60   0.124               #> ind60~~ind60 -0.001  0.006        #> dem60~~dem60 -0.006  0.000  0.448 # Corrected standard errors sqrt(diag(vc2b_cor)) #>  dem60~ind60 ind60~~ind60 dem60~~dem60  #>   0.35271752   0.07634634   0.66959806"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/corrected-se.html","id":"multiple-groups","dir":"Articles","previous_headings":"","what":"Multiple Groups","title":"Corrected Standard Errors","text":"","code":"# Multigroup, three-factor example mod <- \"   # latent variables     visual =~ x1 + x2 + x3     textual =~ x4 + x5 + x6     speed =~ x7 + x8 + x9 \" # Factor scores based on partial invariance fs_dat <- get_fs(HolzingerSwineford1939, model = mod, std.lv = TRUE,                  group = \"school\",                  group.equal = c(\"loadings\", \"intercepts\"),                  group.partial = c(\"visual=~x2\", \"x7~1\")) fit <- cfa(mod,            data = HolzingerSwineford1939,            std.lv = TRUE,            group = \"school\",            group.equal = c(\"loadings\", \"intercepts\"),            group.partial = c(\"visual=~x2\", \"x7~1\")) fs_dat <- get_fs_lavaan(fit) vldev <- R2spa:::vcov_ld_evfs(fit) tspa_fit <- tspa(model = \"visual ~~ textual + speed                           textual ~~ speed\",                  data = fs_dat,                  group = \"school\",                  fsL = attr(fs_dat, which = \"fsL\"),                  fsT = attr(fs_dat, which = \"fsT\")) # Unadjusted covariance vcov(tspa_fit)[c(1:6, 10:15), c(1:6, 10:15)] #>                     visual~~textual visual~~speed textual~~speed visual~~visual #> visual~~textual         0.012032756   0.004143016    0.003562702    0.009059744 #> visual~~speed           0.004143016   0.015289623    0.005749677    0.006286186 #> textual~~speed          0.003562702   0.005749677    0.012308431    0.002169626 #> visual~~visual          0.009059744   0.006286186    0.002169626    0.026249333 #> textual~~textual        0.007226793   0.002111028    0.004878946    0.003126897 #> speed~~speed            0.001464756   0.006962597    0.006774549    0.001505415 #> visual~~textual.g2      0.000000000   0.000000000    0.000000000    0.000000000 #> visual~~speed.g2        0.000000000   0.000000000    0.000000000    0.000000000 #> textual~~speed.g2       0.000000000   0.000000000    0.000000000    0.000000000 #> visual~~visual.g2       0.000000000   0.000000000    0.000000000    0.000000000 #> textual~~textual.g2     0.000000000   0.000000000    0.000000000    0.000000000 #> speed~~speed.g2         0.000000000   0.000000000    0.000000000    0.000000000 #>                     textual~~textual speed~~speed visual~~textual.g2 #> visual~~textual          0.007226793  0.001464756        0.000000000 #> visual~~speed            0.002111028  0.006962597        0.000000000 #> textual~~speed           0.004878946  0.006774549        0.000000000 #> visual~~visual           0.003126897  0.001505415        0.000000000 #> textual~~textual         0.016702352  0.001425195        0.000000000 #> speed~~speed             0.001425195  0.032202255        0.000000000 #> visual~~textual.g2       0.000000000  0.000000000        0.011424159 #> visual~~speed.g2         0.000000000  0.000000000        0.005834597 #> textual~~speed.g2        0.000000000  0.000000000        0.006283615 #> visual~~visual.g2        0.000000000  0.000000000        0.008537899 #> textual~~textual.g2      0.000000000  0.000000000        0.007689097 #> speed~~speed.g2          0.000000000  0.000000000        0.003681750 #>                     visual~~speed.g2 textual~~speed.g2 visual~~visual.g2 #> visual~~textual          0.000000000       0.000000000       0.000000000 #> visual~~speed            0.000000000       0.000000000       0.000000000 #> textual~~speed           0.000000000       0.000000000       0.000000000 #> visual~~visual           0.000000000       0.000000000       0.000000000 #> textual~~textual         0.000000000       0.000000000       0.000000000 #> speed~~speed             0.000000000       0.000000000       0.000000000 #> visual~~textual.g2       0.005834597       0.006283615       0.008537899 #> visual~~speed.g2         0.020016361       0.008699815       0.010689107 #> textual~~speed.g2        0.008699815       0.016930572       0.004219633 #> visual~~visual.g2        0.010689107       0.004219633       0.021628068 #> textual~~textual.g2      0.002940790       0.006708957       0.003370422 #> speed~~speed.g2          0.017174234       0.011969242       0.005282811 #>                     textual~~textual.g2 speed~~speed.g2 #> visual~~textual             0.000000000     0.000000000 #> visual~~speed               0.000000000     0.000000000 #> textual~~speed              0.000000000     0.000000000 #> visual~~visual              0.000000000     0.000000000 #> textual~~textual            0.000000000     0.000000000 #> speed~~speed                0.000000000     0.000000000 #> visual~~textual.g2          0.007689097     0.003681750 #> visual~~speed.g2            0.002940790     0.017174234 #> textual~~speed.g2           0.006708957     0.011969242 #> visual~~visual.g2           0.003370422     0.005282811 #> textual~~textual.g2         0.017541486     0.002565923 #> speed~~speed.g2             0.002565923     0.055832832 # Adjusted covariance vcov_corrected(tspa_fit, vfsLT = vldev)[c(1:6, 10:15), c(1:6, 10:15)] #>                     visual~~textual visual~~speed textual~~speed visual~~visual #> visual~~textual        1.766770e-02  3.948995e-03   1.680095e-03   0.0032658891 #> visual~~speed          3.948995e-03  2.625930e-02   6.180231e-03  -0.0002941259 #> textual~~speed         1.680095e-03  6.180231e-03   2.088330e-02   0.0034032016 #> visual~~visual         3.265889e-03 -2.941259e-04   3.403202e-03   0.0552337485 #> textual~~textual       6.343722e-03  3.471197e-03   4.310592e-03   0.0022325897 #> speed~~speed           2.319057e-03 -2.462920e-04   2.977648e-03   0.0039800365 #> visual~~textual.g2     2.894521e-04 -1.727969e-05  -1.158344e-04  -0.0036750998 #> visual~~speed.g2      -7.758231e-05  4.463544e-04  -3.159286e-04  -0.0017771299 #> textual~~speed.g2     -1.395251e-04 -2.939691e-05   1.074132e-04   0.0021179697 #> visual~~visual.g2      6.773687e-04  9.416669e-05  -5.286419e-04  -0.0139631267 #> textual~~textual.g2   -8.773851e-05  1.870183e-04  -5.236929e-05   0.0001038840 #> speed~~speed.g2       -3.614222e-04  2.154682e-04   1.739917e-04   0.0030238026 #>                     textual~~textual  speed~~speed visual~~textual.g2 #> visual~~textual         6.343722e-03  2.319057e-03       2.894521e-04 #> visual~~speed           3.471197e-03 -2.462920e-04      -1.727969e-05 #> textual~~speed          4.310592e-03  2.977648e-03      -1.158344e-04 #> visual~~visual          2.232590e-03  3.980036e-03      -3.675100e-03 #> textual~~textual        1.943788e-02 -2.260608e-05       3.559274e-04 #> speed~~speed           -2.260608e-05  5.904050e-02       1.853678e-03 #> visual~~textual.g2      3.559274e-04  1.853678e-03       2.079770e-02 #> visual~~speed.g2       -8.758319e-04  3.799116e-03       7.128677e-03 #> textual~~speed.g2      -2.295514e-05 -2.136909e-03       2.631124e-03 #> visual~~visual.g2       1.246137e-03  2.827516e-03       1.425193e-02 #> textual~~textual.g2    -2.028541e-04 -1.070262e-03       5.734502e-03 #> speed~~speed.g2         7.282334e-04 -1.578688e-02       3.266464e-04 #>                     visual~~speed.g2 textual~~speed.g2 visual~~visual.g2 #> visual~~textual        -7.758231e-05     -1.395251e-04      6.773687e-04 #> visual~~speed           4.463544e-04     -2.939691e-05      9.416669e-05 #> textual~~speed         -3.159286e-04      1.074132e-04     -5.286419e-04 #> visual~~visual         -1.777130e-03      2.117970e-03     -1.396313e-02 #> textual~~textual       -8.758319e-04     -2.295514e-05      1.246137e-03 #> speed~~speed            3.799116e-03     -2.136909e-03      2.827516e-03 #> visual~~textual.g2      7.128677e-03      2.631124e-03      1.425193e-02 #> visual~~speed.g2        3.246016e-02      6.793932e-03      8.478633e-03 #> textual~~speed.g2       6.793932e-03      2.230005e-02     -7.599378e-04 #> visual~~visual.g2       8.478633e-03     -7.599378e-04      1.079909e-01 #> textual~~textual.g2     4.641147e-03      6.987265e-03      4.758042e-04 #> speed~~speed.g2         6.636008e-03      1.596428e-02     -3.656654e-03 #>                     textual~~textual.g2 speed~~speed.g2 #> visual~~textual           -8.773851e-05   -0.0003614222 #> visual~~speed              1.870183e-04    0.0002154682 #> textual~~speed            -5.236929e-05    0.0001739917 #> visual~~visual             1.038840e-04    0.0030238026 #> textual~~textual          -2.028541e-04    0.0007282334 #> speed~~speed              -1.070262e-03   -0.0157868803 #> visual~~textual.g2         5.734502e-03    0.0003266464 #> visual~~speed.g2           4.641147e-03    0.0066360081 #> textual~~speed.g2          6.987265e-03    0.0159642788 #> visual~~visual.g2          4.758042e-04   -0.0036566537 #> textual~~textual.g2        2.062776e-02    0.0018416930 #> speed~~speed.g2            1.841693e-03    0.1076812729"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/correction-error.html","id":"unidimensional-case","dir":"Articles","previous_headings":"","what":"Unidimensional Case","title":"Correction to Measurement Error","text":"Consider conditional variance factor scores, Var(η̃∣η)Var(\\tilde \\eta \\mid \\eta), η̃=𝐚⊤𝐲\\tilde \\eta = \\boldsymbol{}^\\top \\boldsymbol{y}. factor model, Var(η̃∣η)=Var(𝐚⊤𝛌η+𝐚⊤𝛆∣η)=η2Var(𝐚⊤𝛌)+Var(𝐚⊤𝛆).Var(\\tilde \\eta \\mid \\eta) = Var(\\boldsymbol{}^\\top \\boldsymbol{\\lambda} \\eta + \\boldsymbol{}^\\top \\boldsymbol{\\varepsilon} \\mid \\eta) = \\eta^2 Var(\\boldsymbol{}^\\top \\boldsymbol{\\lambda}) + Var(\\boldsymbol{}^\\top \\boldsymbol{\\varepsilon}). second term error component. small simulation: standard errors larger using sample estimates weights. 𝐚\\boldsymbol{} known, error variance simply 𝐚⊤𝚯𝐚\\boldsymbol{}^\\top \\boldsymbol{\\Theta} \\boldsymbol{}. However, 𝐚\\boldsymbol{} unknown estimated 𝐚̃\\tilde{\\boldsymbol{}} data, Var(𝐚̃⊤𝛆)=E[Var(𝐚̃⊤𝛆∣𝐚̃)]+Var[E(𝐚̃⊤𝛆∣𝐚̃)].Var(\\tilde{\\boldsymbol{}}^\\top \\boldsymbol{\\varepsilon}) = E[Var(\\tilde{\\boldsymbol{}}^\\top \\boldsymbol{\\varepsilon} \\mid \\tilde{\\boldsymbol{}})] + Var[E(\\tilde{\\boldsymbol{}}^\\top \\boldsymbol{\\varepsilon} \\mid \\tilde{\\boldsymbol{}})]. assumption E(𝛆)E(\\boldsymbol{\\varepsilon}) = 𝟎\\boldsymbol{0}, second term 0. first term expectation quadratic form, E(𝐚̃⊤𝚯𝐚̃)=E(𝐚̃⊤)𝚯E(𝐚̃)+tr(𝚯𝐕𝐚̃)E(\\tilde{\\boldsymbol{}}^\\top \\boldsymbol{\\Theta} \\tilde{\\boldsymbol{}}) = E(\\tilde{\\boldsymbol{}}^\\top) \\boldsymbol{\\Theta} E(\\tilde{\\boldsymbol{}}) + \\mathrm{tr}(\\boldsymbol{\\Theta} \\boldsymbol{V}_{\\tilde{\\boldsymbol{}}}) 𝐕𝐚̃\\boldsymbol{V}_{\\tilde{\\boldsymbol{}}} covariance matrix 𝐚̃\\tilde{\\boldsymbol{}}. can see using simulated data","code":"#' Load package and set seed library(lavaan) #> This is lavaan 0.6-18 #> lavaan is FREE software! Please report any bugs. library(R2spa) set.seed(191254)  #' Fixed conditions num_obs <- 100 lambda <- c(.3, .5, .7, .9) theta <- c(.5, .4, .5, .7)  #' Simulate true score, and standardized eta <- rnorm(num_obs) eta <- (eta - mean(eta)) eta <- eta / sqrt(mean(eta^2))  #' Function for simulating y simy <- function(eta, lambda) {     t(tcrossprod(lambda, eta) +           rnorm(length(lambda) * length(eta), sd = sqrt(theta))) } #' Simulation nsim <- 2500 tfsy_sim <- fsy_sim <- matrix(NA, nrow = num_obs, ncol = nsim) # Also save the scoring matrix a_sim <- matrix(NA, nrow = length(lambda), ncol = nsim) for (i in seq_len(nsim)) {     y <- simy(eta = eta, lambda = lambda)     tfsy_sim[, i] <- R2spa::compute_fscore(       y, lambda = lambda, theta = diag(theta), psi = matrix(1)     )     fsy <- R2spa::get_fs(y, std.lv = TRUE)     fsy_sim[, i] <- fsy[, 1]     a_sim[, i] <- attr(fsy, which = \"scoring_matrix\") } #' Average conditional variance # a known apply(tfsy_sim, 1, var) |> mean() #> [1] 0.1871828 # compare to theoretical value true_a <- R2spa:::compute_a_reg(lambda, psi = matrix(1), theta = diag(theta)) true_a %*% diag(theta) %*% t(true_a) #>           [,1] #> [1,] 0.1893211 # a estimated apply(fsy_sim, 1, var) |> mean()  #> [1] 0.2078126 correct_fac <- sum(diag(diag(theta) %*% cov(t(a_sim)))) true_a %*% diag(theta) %*% t(true_a) + correct_fac #>           [,1] #> [1,] 0.2100938"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/correction-error.html","id":"correction-factor","dir":"Articles","previous_headings":"Unidimensional Case","what":"Correction Factor","title":"Correction to Measurement Error","text":"can approximate 𝐕𝐚̃\\boldsymbol{V}_{\\tilde{\\boldsymbol{}}} using delta method, terms using sample estimates 𝐚̃⊤𝚯̂𝐚̃+tr(𝚯̂𝐕̂𝐚̃).\\tilde{\\boldsymbol{}}^\\top \\hat{\\boldsymbol{\\Theta}} \\tilde{\\boldsymbol{}} + \\mathrm{tr}(\\hat{\\boldsymbol{\\Theta}} \\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{}}}). delta method estimate, 𝐕̂𝐚̃\\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{}}}, 𝐉𝐚̃(𝛉)⊤𝐕̂𝛉̃𝐉𝐚̃(𝛉),\\boldsymbol{J}_{\\tilde{\\boldsymbol{}}}(\\boldsymbol{\\theta})^\\top \\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{\\theta}}} \\boldsymbol{J}_{\\tilde{\\boldsymbol{}}}(\\boldsymbol{\\theta}), 𝐉𝐚̃(𝛉)\\boldsymbol{J}_{\\tilde{\\boldsymbol{}}}(\\boldsymbol{\\theta}) Jacobian matrix, 𝐕̂𝛉̃\\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{\\theta}}} sampling variance sample estimator 𝛉\\boldsymbol{\\theta}. Jacobian can approximated using finite difference lavaan::lav_func_jacobian_complex().","code":"y <- simy(eta = eta, lambda = lambda) cfa_fit <- cfa(\"f =~ y1 + y2 + y3 + y4\",                data = setNames(data.frame(y), paste0(\"y\", 1:4)),                std.lv = TRUE) # Jacobian R2spa:::compute_a(coef(cfa_fit), lavobj = cfa_fit) #> [[1]] #>         [,1]      [,2]     [,3]      [,4] #> f 0.09890619 0.3246767 0.450034 0.3134454 jac_a <- lavaan::lav_func_jacobian_complex(     \\(x) R2spa:::compute_a(x, lavobj = cfa_fit)[[1]],     coef(cfa_fit) ) sum(diag(lavInspect(cfa_fit, what = \"est\")$theta %*%              jac_a %*% vcov(cfa_fit) %*% t(jac_a))) #> [1] 0.02152978 fsy <- R2spa::get_fs(y, std.lv = TRUE) attr(fsy, which = \"fsT\") #>           fs_f1 #> fs_f1 0.2014365 # Using `get_fs(..., corrected_fsT = TRUE) fsy2 <- R2spa::get_fs(y, std.lv = TRUE, corrected_fsT = TRUE) attr(fsy2, which = \"fsT\") #>           fs_f1 #> fs_f1 0.2229663"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/correction-error.html","id":"multidimensional-case","dir":"Articles","previous_headings":"","what":"Multidimensional Case","title":"Correction to Measurement Error","text":"ii, jj element correction matrix can approximated tr(𝚯̂𝐕̂𝐚̃i𝐚̃j)\\mathrm{tr}(\\hat{\\boldsymbol{\\Theta}} \\hat{\\boldsymbol{V}}_{{\\tilde{\\boldsymbol{}}_i}{\\tilde{\\boldsymbol{}}_j}}), 𝐕̂𝐚̃i𝐚̃j)\\hat{\\boldsymbol{V}}_{{\\tilde{\\boldsymbol{}}_i}{\\tilde{\\boldsymbol{}}_j}}) obtained 𝐉𝐚̃(𝛉)⊤𝐕̂𝛉̃𝐉𝐚̃j(𝛉),\\boldsymbol{J}_{\\tilde{\\boldsymbol{}}_i}(\\boldsymbol{\\theta})^\\top \\hat{\\boldsymbol{V}}_{\\tilde{\\boldsymbol{\\theta}}} \\boldsymbol{J}_{\\tilde{\\boldsymbol{}}_j}(\\boldsymbol{\\theta}),","code":"#' Set seed set.seed(251329)  #' Fixed conditions num_obs <- 100 lambda <- Matrix::bdiag(list(c(.3, .5, .7, .9), c(.7, .6, .7))) |>     as.matrix() theta <- c(.5, .4, .5, .7, .7, .8, .5) psi <- matrix(c(1, -.4, -.4, 1), nrow = 2)  #' Simulate true score (exact means and covariances) eta <- MASS::mvrnorm(num_obs, mu = rep(0, 2), Sigma = psi, empirical = TRUE)  #' Function for simulating y simy <- function(eta, lambda) {     tcrossprod(eta, lambda) +         MASS::mvrnorm(num_obs, mu = rep(0, length(theta)), Sigma = diag(theta)) } #' Simulation nsim <- 2500 tfsy_sim <- fsy_sim <- array(NA, dim = c(num_obs, ncol(lambda), nsim)) # Also save the scoring matrix a_sim <- array(NA, dim = c(ncol(lambda), nrow(lambda), nsim)) for (i in seq_len(nsim)) {     y <- simy(eta = eta, lambda = lambda)     tfsy_sim[, , i] <- R2spa::compute_fscore(       y, lambda = lambda, theta = diag(theta), psi = psi     )     fsy <- R2spa::get_fs(         data.frame(y) |> setNames(paste0(\"y\", 1:7)),         model = \"f1 =~ y1 + y2 + y3 + y4\\nf2 =~ y5 + y6 + y7\",         std.lv = TRUE)     fsy_sim[, , i] <- as.matrix(fsy[, 1:2])     a_sim[, , i] <- attr(fsy, which = \"scoring_matrix\") } #' Average conditional variance # a known apply(tfsy_sim, MARGIN = 1, FUN = \\(x) cov(t(x))) |>      rowMeans() |> matrix(ncol = 2, nrow = 2) #>            [,1]       [,2] #> [1,]  0.1789934 -0.0483855 #> [2,] -0.0483855  0.2022312 # compare to theoretical value true_a <- R2spa:::compute_a_reg(lambda, psi = psi, theta = diag(theta)) true_a %*% diag(theta) %*% t(true_a) #>             [,1]        [,2] #> [1,]  0.18076106 -0.04855749 #> [2,] -0.04855749  0.20339709 # a estimated apply(fsy_sim, MARGIN = 1, FUN = \\(x) cov(t(x))) |>      rowMeans() |> matrix(ncol = 2, nrow = 2) #>             [,1]        [,2] #> [1,]  0.20175239 -0.05254471 #> [2,] -0.05254471  0.23427145 correct_fac11 <- sum(diag(diag(theta) %*% cov(t(a_sim[1, , ])))) correct_fac22 <- sum(diag(diag(theta) %*% cov(t(a_sim[2, , ])))) correct_fac21 <- sum(diag(diag(theta) %*%                               cov(t(a_sim[2, , ]), t(a_sim[1, , ])))) correct_fac <- matrix(c(correct_fac11, correct_fac21,                         correct_fac21, correct_fac22), nrow = 2) true_a %*% diag(theta) %*% t(true_a) + correct_fac #>             [,1]        [,2] #> [1,]  0.20114155 -0.05375924 #> [2,] -0.05375924  0.23429418 y <- simy(eta = eta, lambda = lambda) cfa_fit <- cfa(\"f1 =~ y1 + y2 + y3 + y4\\nf2 =~ y5 + y6 + y7\",                data = setNames(data.frame(y), paste0(\"y\", 1:7)),                std.lv = TRUE) # Jacobian R2spa:::compute_a(coef(cfa_fit), lavobj = cfa_fit) #> [[1]] #>           [,1]        [,2]        [,3]        [,4]        [,5]        [,6] #> f1  0.06546889  0.22696068  0.38977223  0.33101521 -0.05012587 -0.02226577 #> f2 -0.01227313 -0.04254719 -0.07306865 -0.06205377  0.37397828  0.16612010 #>           [,7] #> f1 -0.05314801 #> f2  0.39652577 jac_a1 <- lavaan::lav_func_jacobian_complex(     \\(x) R2spa:::compute_a(x, lavobj = cfa_fit)[[1]][1, ],     coef(cfa_fit) ) jac_a2 <- lavaan::lav_func_jacobian_complex(     \\(x) R2spa:::compute_a(x, lavobj = cfa_fit)[[1]][2, ],     coef(cfa_fit) ) # Correction[1, 1] sum(diag(lavInspect(cfa_fit, what = \"est\")$theta %*%              jac_a1 %*% vcov(cfa_fit) %*% t(jac_a1))) #> [1] 0.01551285 # Correction[2, 2] sum(diag(lavInspect(cfa_fit, what = \"est\")$theta %*%              jac_a2 %*% vcov(cfa_fit) %*% t(jac_a2))) #> [1] 0.02207038 # Correction[2, 1] sum(diag(lavInspect(cfa_fit, what = \"est\")$theta %*%              jac_a2 %*% vcov(cfa_fit) %*% t(jac_a1))) #> [1] -0.004873356 fsy <- R2spa::get_fs(     data.frame(y) |> setNames(paste0(\"y\", 1:7)),     model = \"f1 =~ y1 + y2 + y3 + y4\\nf2 =~ y5 + y6 + y7\",     std.lv = TRUE) attr(fsy, which = \"fsT\") #>             fs_f1       fs_f2 #> fs_f1  0.16691097 -0.05653148 #> fs_f2 -0.05653148  0.19891917 # Using `get_fs(..., corrected_fsT = TRUE) fsy2 <- R2spa::get_fs(     data.frame(y) |> setNames(paste0(\"y\", 1:7)),     model = \"f1 =~ y1 + y2 + y3 + y4\\nf2 =~ y5 + y6 + y7\",     std.lv = TRUE,     corrected_fsT = TRUE) attr(fsy2, which = \"fsT\") #>             fs_f1       fs_f2 #> fs_f1  0.18242382 -0.06140484 #> fs_f2 -0.06140484  0.22098955"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/efa-score.html","id":"example-big-five-inventory","dir":"Articles","previous_headings":"","what":"Example: Big Five Inventory","title":"EFA Scores","text":"EFA Factor Correlation Sum scores Hand calculate Bartlett scores using weights Bartlett scores","code":"# Covariance (with FIML) corr_bfi <- lavCor(bfi[1:25], missing = \"fiml\") # EFA (Target rotation) target_mat_bfi <- matrix(0, nrow = 25, ncol = 5) target_mat_bfi[1:5, 1] <- NA target_mat_bfi[6:10, 2] <- NA target_mat_bfi[11:15, 3] <- NA target_mat_bfi[16:20, 4] <- NA target_mat_bfi[21:25, 5] <- NA fa_target_bfi <- psych::fa(     corr_bfi, n.obs = 2436,     nfactors = 5,     rotate = \"targetQ\", Target = target_mat_bfi,     scores = \"Bartlett\") #> Loading required namespace: GPArotation # Factor correlations fa_target_bfi$Phi |>     (`[`)(paste0(\"MR\", 1:5), paste0(\"MR\", 1:5)) |>     knitr::kable(digits = 2, caption = \"EFA Factor Correlation\") # Correlation with sum scores bfi |>     transform(A = (7 - A1) + A2 + A3 + A4 + A5,            C = C1 + C2 + C3 + (7 - C4) + (7 - C5),            E = (7 - E1) + (7 - E2) + E3 + E4 + E5,            N = N1 + N2 + N3 + N4 + N5,            O = O1 + (7 - O2) + O3 + O4 + (7 - O5)) |>     subset(select = A:O) |>     cor(use = \"complete\") |>     knitr::kable(digits = 2, caption = \"Sum scores\") # Bartlett score for first person bscores <-     psych::factor.scores(bfi[, 1:25], f = fa_target_bfi,                          method = \"Bartlett\") fa_target_bfi$weights #>             MR4          MR3          MR2          MR1           MR5 #> A1  0.019379345  0.088570322  0.037984352 -0.192609241 -0.0446095141 #> A2  0.039471717 -0.058827607  0.016480405  0.387253874  0.0174635927 #> A3  0.038138482 -0.002491405 -0.018448555  0.443385489 -0.0008791950 #> A4  0.008949806 -0.011419400  0.074609038  0.196183077 -0.1058123616 #> A5 -0.007115326  0.064563350 -0.028894457  0.297710866 -0.0033648030 #> C1  0.025793549 -0.013945609  0.252251588 -0.025663593  0.0878936560 #> C2  0.067216606 -0.054276500  0.383692956  0.040736916  0.0208035616 #> C3  0.023175743 -0.039728147  0.266210191  0.042308787 -0.0532943534 #> C4  0.045093233  0.017707633 -0.349869186  0.042230966 -0.0202260355 #> C5  0.055608096 -0.047779140 -0.303202360  0.040826161  0.1037402597 #> E1  0.002492943 -0.234998207  0.069439907  0.033510757 -0.0074443287 #> E2  0.069816249 -0.398147198  0.017193600  0.096706957  0.0534356364 #> E3  0.025889788  0.192977816 -0.031642142  0.069479646  0.1645920884 #> E4 -0.004903761  0.315619052 -0.010031608  0.099826497 -0.1699081214 #> E5  0.034244648  0.197071361  0.128843720 -0.049763160  0.0903700702 #> N1  0.376489849  0.174004405  0.036365710 -0.153681565 -0.0900410369 #> N2  0.315648329  0.098517700  0.033517347 -0.103972896  0.0008553925 #> N3  0.274596160 -0.019282873 -0.009840675  0.065937424  0.0331731810 #> N4  0.182196509 -0.186091836 -0.069870228  0.123634306  0.1272479199 #> N5  0.144850775 -0.078929736  0.008603568  0.133251027 -0.0834902117 #> O1  0.007965717  0.045782965  0.014054766 -0.025087007  0.3135869739 #> O2  0.045459925  0.014231220 -0.021761255  0.082163004 -0.2754285178 #> O3  0.016818868  0.080919045 -0.024446602 -0.001049249  0.4783313815 #> O4  0.052204814 -0.121040967 -0.019643988  0.103037548  0.2512799168 #> O5  0.024318925  0.034957969  0.005514044  0.026713299 -0.3498465254 # Calculation by hand y1 <- scale(bfi[, 1:25])[1, ]  # z-score crossprod(fa_target_bfi$weights, as.matrix(y1)) #>             [,1] #> MR4 -0.379596163 #> MR3  0.008672754 #> MR2 -1.662210036 #> MR1 -1.056626928 #> MR5 -2.263921509 # Compare to results from psych::fa() bscores$scores[1, ] #>          MR4          MR3          MR2          MR1          MR5  #> -0.379596163  0.008672754 -1.662210036 -1.056626928 -2.263921509 # Covariance of Bartlett scores cov(bscores$scores, use = \"complete\") |>     (`[`)(paste0(\"MR\", 1:5), paste0(\"MR\", 1:5)) |>     knitr::kable(digits = 2, caption = \"With Bartlett scores\")"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/efa-score.html","id":"using-compute_fscore-and-perform-a-two-stage-analysis","dir":"Articles","previous_headings":"Example: Big Five Inventory","what":"Using compute_fscore() and perform a two-stage analysis","title":"EFA Scores","text":"Use R2spa::compute_fscore() Recover factor covariances 2S-PA Bartlett scores 2S-PA","code":"# Obtain error covariances yc <- scale(bfi[, 1:25]) yc <- yc[complete.cases(yc), ] lam <- fa_target_bfi$loadings colnames(lam) <- c(\"N\", \"E\", \"C\", \"A\", \"O\") phi <- fa_target_bfi$Phi th <- diag(fa_target_bfi$uniquenesses) # # scoring weights # a <- solve(crossprod(lam, solve(th, lam)), t(solve(th, lam))) # ecov_fs <- a %*% th %*% t(a) # dimnames(ecov_fs) <- rep(list(c(\"N\", \"E\", \"C\", \"A\", \"O\")), 2) # Two-stage analysis library(R2spa) bfi_fs <- compute_fscore(yc, lambda = lam, theta = th,                          method = \"Bartlett\", center_y = FALSE,                          fs_matrices = TRUE) head(bfi_fs) #>                 N            E           C           A          O #> 61617 -0.37959616  0.008672754 -1.66221004 -1.05662693 -2.2639215 #> 61618  0.05502992  0.657251998 -0.81281047 -0.21048629 -0.3557784 #> 61620  0.66912918  0.351719435 -0.00993787 -0.96855007  0.3107586 #> 61621 -0.14122632 -0.048191450 -1.35947549  0.01849824 -1.5743464 #> 61622 -0.36044079  0.557318357 -0.07396380 -1.02840458 -1.0629543 #> 61623  0.18540827  1.461174698  1.82621364  0.11707095  0.5289163 # Scoring matrix attr(bfi_fs, which = \"scoring_matrix\") #>          [,1]        [,2]         [,3]         [,4]         [,5]        [,6] #> N  0.01937935  0.03947172  0.038138482  0.008949806 -0.007115326  0.02579355 #> E  0.08857032 -0.05882761 -0.002491405 -0.011419400  0.064563350 -0.01394561 #> C  0.03798435  0.01648041 -0.018448555  0.074609038 -0.028894457  0.25225159 #> A -0.19260924  0.38725387  0.443385489  0.196183077  0.297710866 -0.02566359 #> O -0.04460951  0.01746359 -0.000879195 -0.105812362 -0.003364803  0.08789366 #>          [,7]        [,8]        [,9]       [,10]        [,11]       [,12] #> N  0.06721661  0.02317574  0.04509323  0.05560810  0.002492943  0.06981625 #> E -0.05427650 -0.03972815  0.01770763 -0.04777914 -0.234998207 -0.39814720 #> C  0.38369296  0.26621019 -0.34986919 -0.30320236  0.069439907  0.01719360 #> A  0.04073692  0.04230879  0.04223097  0.04082616  0.033510757  0.09670696 #> O  0.02080356 -0.05329435 -0.02022604  0.10374026 -0.007444329  0.05343564 #>         [,13]        [,14]       [,15]       [,16]         [,17]        [,18] #> N  0.02588979 -0.004903761  0.03424465  0.37648985  0.3156483287  0.274596160 #> E  0.19297782  0.315619052  0.19707136  0.17400440  0.0985176996 -0.019282873 #> C -0.03164214 -0.010031608  0.12884372  0.03636571  0.0335173471 -0.009840675 #> A  0.06947965  0.099826497 -0.04976316 -0.15368157 -0.1039728964  0.065937424 #> O  0.16459209 -0.169908121  0.09037007 -0.09004104  0.0008553925  0.033173181 #>         [,19]        [,20]        [,21]       [,22]        [,23]       [,24] #> N  0.18219651  0.144850775  0.007965717  0.04545993  0.016818868  0.05220481 #> E -0.18609184 -0.078929736  0.045782965  0.01423122  0.080919045 -0.12104097 #> C -0.06987023  0.008603568  0.014054766 -0.02176125 -0.024446602 -0.01964399 #> A  0.12363431  0.133251027 -0.025087007  0.08216300 -0.001049249  0.10303755 #> O  0.12724792 -0.083490212  0.313586974 -0.27542852  0.478331381  0.25127992 #>          [,25] #> N  0.024318925 #> E  0.034957969 #> C  0.005514044 #> A  0.026713299 #> O -0.349846525 # Error covariance attr(bfi_fs, which = \"fsT\") #>              fs_N         fs_E         fs_C        fs_A         fs_O #> fs_N  0.169330114 -0.005940885  0.008966234  0.02689064  0.006632488 #> fs_E -0.005940885  0.267237548 -0.008256099 -0.06840869 -0.028478148 #> fs_C  0.008966234 -0.008256099  0.316471763 -0.01719329 -0.016556329 #> fs_A  0.026890643 -0.068408687 -0.017193295  0.34849177 -0.010843643 #> fs_O  0.006632488 -0.028478148 -0.016556329 -0.01084364  0.454791519 ts_fit <- tspa(\"\",                data = data.frame(bscores$scores) |>                    setNames(c(\"fs_N\", \"fs_E\", \"fs_C\", \"fs_A\", \"fs_O\")),                fsT = attr(bfi_fs, which = \"fsT\"),                fsL = diag(5) |>                    `dimnames<-`(list(c(\"fs_N\", \"fs_E\", \"fs_C\", \"fs_A\", \"fs_O\"),                                      c(\"N\", \"E\", \"C\", \"A\", \"O\")))) lavInspect(ts_fit, what = \"cor.lv\") |>     (`[`)(c(\"A\", \"C\", \"E\", \"N\", \"O\"), c(\"A\", \"C\", \"E\", \"N\", \"O\")) |>     knitr::kable(digits = 2, caption = \"With Bartlett scores and 2S-PA\")"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/get_fs_int-vignette.html","id":"get_fs_int-function","dir":"Articles","previous_headings":"","what":"get_fs_int Function","title":"get_fs_int-example","text":"function designed create product indicators first-order factor score indicators standard errors model-implied loadings. specified model, product indicator pairs generated based model syntax; without specified model, function default generate possible pairs product indicators.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/get_fs_int-vignette.html","id":"illustrative-example","dir":"Articles","previous_headings":"","what":"Illustrative Example","title":"get_fs_int-example","text":"simulate dataset four firsr-order latent variables: x, m, z, y Obtain factor scores using get_fs() Obtain product-indicator factor scores, corresponding standard errors. Double-mean-centering used.","code":"set.seed(2116) ## Sample size: num_obs <- 5000  # Structural Parameters:  ## gamma_x = 0.3, gamma_m = 0.4, gamma_z = 0.2 ## gamma_xm = 0.1, gamma_xz = 0.15, gamma_mz = 0.12  # Correlation between latent variables: ## cor_xm = 0.2, cor_xz = 0.3, cor_zm = 0.4  # Data simulation:  ## x, z, m, ey cov_xmz_ey <- matrix(c(1, 0.1, 0.15, 0,                        0.1, 1, 0.12, 0,                        0.15, 0.12, 1, 0,                        0, 0, 0, 0.481351), nrow = 4) eta <- as.data.frame(   MASS::mvrnorm(num_obs,     mu = rep(0, 4), Sigma = cov_xmz_ey,     empirical = FALSE   ) ) names(eta) <- c(\"x\", \"m\", \"z\", \"ey\")  # xm, xz, mz eta <- eta |>   transform(     xm = x * m,     xz = x * z,     mz = m * z   )  # y etay <- 0.3 * eta$x + 0.4 * eta$m + 0.2 * eta$z +   0.1 * eta$xm + 0.15 * eta$xz + 0.12 * eta$mz + eta$ey  # Observed Indicators lambda_x <- c(0.9, 0.8, 0.7) lambda_m <- c(0.85, 0.75, 0.65) lambda_z <- c(0.8, 0.7, 0.6) lambda_y <- c(0.75, 0.7, 0.65)  x_obs <- eta$x %*% t(lambda_x) + rnorm(num_obs * length(lambda_x)) m_obs <- eta$m %*% t(lambda_m) + rnorm(num_obs * length(lambda_m)) z_obs <- eta$z %*% t(lambda_z) + rnorm(num_obs * length(lambda_z)) y_obs <- etay %*% t(lambda_y) + rnorm(num_obs * length(lambda_z))  # Dataset: raw score df <- cbind(x_obs, m_obs, z_obs, y_obs) df <- as.data.frame(df) names(df) <- c(   paste0(\"x\", 1:3), paste0(\"m\", 1:3),   paste0(\"z\", 1:3), paste0(\"y\", 1:3) ) fs_dat <- get_fs(df, model = \"x =~ x1 + x2 + x3                               m =~ m1 + m2 + m3                               z =~ z1 + z2 + z3                               y =~ y1 + y2 + y3\",                  std.lv = TRUE,                  method = \"Bartlett\") head(fs_dat) #>          fs_x       fs_m       fs_z       fs_y   fs_x_se   fs_m_se   fs_z_se #> 1 -1.57082033  0.5158033 -2.4166532  0.6803742 0.7007543 0.7621661 0.7919491 #> 2 -1.28761379  1.5073520 -1.2254840  0.6479675 0.7007543 0.7621661 0.7919491 #> 3  1.05055317  1.3509779  0.9886338 -1.0717407 0.7007543 0.7621661 0.7919491 #> 4 -1.24981810 -0.4465930 -1.1941697 -0.2898219 0.7007543 0.7621661 0.7919491 #> 5  0.03723966 -2.0745305 -0.7534680  0.8267039 0.7007543 0.7621661 0.7919491 #> 6 -0.30949355  0.3911142 -0.6878682 -1.2723421 0.7007543 0.7621661 0.7919491 #>     fs_y_se x_by_fs_x x_by_fs_m x_by_fs_z x_by_fs_y m_by_fs_x m_by_fs_m #> 1 0.9035465         1         0         0         0         0         1 #> 2 0.9035465         1         0         0         0         0         1 #> 3 0.9035465         1         0         0         0         0         1 #> 4 0.9035465         1         0         0         0         0         1 #> 5 0.9035465         1         0         0         0         0         1 #> 6 0.9035465         1         0         0         0         0         1 #>   m_by_fs_z m_by_fs_y z_by_fs_x z_by_fs_m z_by_fs_z z_by_fs_y y_by_fs_x #> 1         0         0         0         0         1         0         0 #> 2         0         0         0         0         1         0         0 #> 3         0         0         0         0         1         0         0 #> 4         0         0         0         0         1         0         0 #> 5         0         0         0         0         1         0         0 #> 6         0         0         0         0         1         0         0 #>   y_by_fs_m y_by_fs_z y_by_fs_y   ev_fs_x ecov_fs_m_fs_x   ev_fs_m #> 1         0         0         1 0.4910567              0 0.5808972 #> 2         0         0         1 0.4910567              0 0.5808972 #> 3         0         0         1 0.4910567              0 0.5808972 #> 4         0         0         1 0.4910567              0 0.5808972 #> 5         0         0         1 0.4910567              0 0.5808972 #> 6         0         0         1 0.4910567              0 0.5808972 #>   ecov_fs_z_fs_x ecov_fs_z_fs_m   ev_fs_z ecov_fs_y_fs_x ecov_fs_y_fs_m #> 1              0              0 0.6271834              0              0 #> 2              0              0 0.6271834              0              0 #> 3              0              0 0.6271834              0              0 #> 4              0              0 0.6271834              0              0 #> 5              0              0 0.6271834              0              0 #> 6              0              0 0.6271834              0              0 #>   ecov_fs_y_fs_z   ev_fs_y #> 1              0 0.8163963 #> 2              0 0.8163963 #> 3              0 0.8163963 #> 4              0 0.8163963 #> 5              0 0.8163963 #> 6              0 0.8163963 # With specified model ind_1 <- get_fs_int(dat = fs_dat,                     fs_name = c(\"fs_x\", \"fs_m\", \"fs_z\"),                     se_fs = c(\"fs_x_se\", \"fs_m_se\", \"fs_z_se\"),                     loading_fs = c(\"x_by_fs_x\", \"m_by_fs_m\", \"z_by_fs_z\"),                     model = \"fs_x:fs_m + fs_x:fs_z\") head(ind_1) #>          fs_x       fs_m       fs_z       fs_y   fs_x_se   fs_m_se   fs_z_se #> 1 -1.57082033  0.5158033 -2.4166532  0.6803742 0.7007543 0.7621661 0.7919491 #> 2 -1.28761379  1.5073520 -1.2254840  0.6479675 0.7007543 0.7621661 0.7919491 #> 3  1.05055317  1.3509779  0.9886338 -1.0717407 0.7007543 0.7621661 0.7919491 #> 4 -1.24981810 -0.4465930 -1.1941697 -0.2898219 0.7007543 0.7621661 0.7919491 #> 5  0.03723966 -2.0745305 -0.7534680  0.8267039 0.7007543 0.7621661 0.7919491 #> 6 -0.30949355  0.3911142 -0.6878682 -1.2723421 0.7007543 0.7621661 0.7919491 #>     fs_y_se x_by_fs_x x_by_fs_m x_by_fs_z x_by_fs_y m_by_fs_x m_by_fs_m #> 1 0.9035465         1         0         0         0         0         1 #> 2 0.9035465         1         0         0         0         0         1 #> 3 0.9035465         1         0         0         0         0         1 #> 4 0.9035465         1         0         0         0         0         1 #> 5 0.9035465         1         0         0         0         0         1 #> 6 0.9035465         1         0         0         0         0         1 #>   m_by_fs_z m_by_fs_y z_by_fs_x z_by_fs_m z_by_fs_z z_by_fs_y y_by_fs_x #> 1         0         0         0         0         1         0         0 #> 2         0         0         0         0         1         0         0 #> 3         0         0         0         0         1         0         0 #> 4         0         0         0         0         1         0         0 #> 5         0         0         0         0         1         0         0 #> 6         0         0         0         0         1         0         0 #>   y_by_fs_m y_by_fs_z y_by_fs_y   ev_fs_x ecov_fs_m_fs_x   ev_fs_m #> 1         0         0         1 0.4910567              0 0.5808972 #> 2         0         0         1 0.4910567              0 0.5808972 #> 3         0         0         1 0.4910567              0 0.5808972 #> 4         0         0         1 0.4910567              0 0.5808972 #> 5         0         0         1 0.4910567              0 0.5808972 #> 6         0         0         1 0.4910567              0 0.5808972 #>   ecov_fs_z_fs_x ecov_fs_z_fs_m   ev_fs_z ecov_fs_y_fs_x ecov_fs_y_fs_m #> 1              0              0 0.6271834              0              0 #> 2              0              0 0.6271834              0              0 #> 3              0              0 0.6271834              0              0 #> 4              0              0 0.6271834              0              0 #> 5              0              0 0.6271834              0              0 #> 6              0              0 0.6271834              0              0 #>   ecov_fs_y_fs_z   ev_fs_y  fs_x:fs_m fs_x:fs_m_se fs_x:fs_m_ld  fs_x:fs_z #> 1              0 0.8163963 -0.9160818     1.164992            1  3.6292445 #> 2              0 0.8163963 -2.0467348     1.164992            1  1.4110667 #> 3              0 0.8163963  1.3134266     1.164992            1  0.8717289 #> 4              0 0.8163963  0.4523125     1.164992            1  1.3256114 #> 5              0 0.8163963 -0.1831024     1.164992            1 -0.1949424 #> 6              0 0.8163963 -0.2268949     1.164992            1  0.0460073 #>   fs_x:fs_z_se fs_x:fs_z_ld #> 1     1.194246            1 #> 2     1.194246            1 #> 3     1.194246            1 #> 4     1.194246            1 #> 5     1.194246            1 #> 6     1.194246            1  # Without specified model ind_2 <- get_fs_int(dat = fs_dat,                     fs_name = c(\"fs_x\", \"fs_m\", \"fs_z\"),                     se_fs = c(\"fs_x_se\", \"fs_m_se\", \"fs_z_se\"),                     loading_fs = c(\"x_by_fs_x\", \"m_by_fs_m\", \"z_by_fs_z\")) head(ind_2) #>          fs_x       fs_m       fs_z       fs_y   fs_x_se   fs_m_se   fs_z_se #> 1 -1.57082033  0.5158033 -2.4166532  0.6803742 0.7007543 0.7621661 0.7919491 #> 2 -1.28761379  1.5073520 -1.2254840  0.6479675 0.7007543 0.7621661 0.7919491 #> 3  1.05055317  1.3509779  0.9886338 -1.0717407 0.7007543 0.7621661 0.7919491 #> 4 -1.24981810 -0.4465930 -1.1941697 -0.2898219 0.7007543 0.7621661 0.7919491 #> 5  0.03723966 -2.0745305 -0.7534680  0.8267039 0.7007543 0.7621661 0.7919491 #> 6 -0.30949355  0.3911142 -0.6878682 -1.2723421 0.7007543 0.7621661 0.7919491 #>     fs_y_se x_by_fs_x x_by_fs_m x_by_fs_z x_by_fs_y m_by_fs_x m_by_fs_m #> 1 0.9035465         1         0         0         0         0         1 #> 2 0.9035465         1         0         0         0         0         1 #> 3 0.9035465         1         0         0         0         0         1 #> 4 0.9035465         1         0         0         0         0         1 #> 5 0.9035465         1         0         0         0         0         1 #> 6 0.9035465         1         0         0         0         0         1 #>   m_by_fs_z m_by_fs_y z_by_fs_x z_by_fs_m z_by_fs_z z_by_fs_y y_by_fs_x #> 1         0         0         0         0         1         0         0 #> 2         0         0         0         0         1         0         0 #> 3         0         0         0         0         1         0         0 #> 4         0         0         0         0         1         0         0 #> 5         0         0         0         0         1         0         0 #> 6         0         0         0         0         1         0         0 #>   y_by_fs_m y_by_fs_z y_by_fs_y   ev_fs_x ecov_fs_m_fs_x   ev_fs_m #> 1         0         0         1 0.4910567              0 0.5808972 #> 2         0         0         1 0.4910567              0 0.5808972 #> 3         0         0         1 0.4910567              0 0.5808972 #> 4         0         0         1 0.4910567              0 0.5808972 #> 5         0         0         1 0.4910567              0 0.5808972 #> 6         0         0         1 0.4910567              0 0.5808972 #>   ecov_fs_z_fs_x ecov_fs_z_fs_m   ev_fs_z ecov_fs_y_fs_x ecov_fs_y_fs_m #> 1              0              0 0.6271834              0              0 #> 2              0              0 0.6271834              0              0 #> 3              0              0 0.6271834              0              0 #> 4              0              0 0.6271834              0              0 #> 5              0              0 0.6271834              0              0 #> 6              0              0 0.6271834              0              0 #>   ecov_fs_y_fs_z   ev_fs_y  fs_x:fs_m fs_x:fs_m_se fs_x:fs_m_ld  fs_x:fs_z #> 1              0 0.8163963 -0.9160818     1.164992            1  3.6292445 #> 2              0 0.8163963 -2.0467348     1.164992            1  1.4110667 #> 3              0 0.8163963  1.3134266     1.164992            1  0.8717289 #> 4              0 0.8163963  0.4523125     1.164992            1  1.3256114 #> 5              0 0.8163963 -0.1831024     1.164992            1 -0.1949424 #> 6              0 0.8163963 -0.2268949     1.164992            1  0.0460073 #>   fs_x:fs_z_se fs_x:fs_z_ld  fs_m:fs_z fs_m:fs_z_se fs_m:fs_z_ld #> 1     1.194246            1 -1.3748087     1.253958            1 #> 2     1.194246            1 -1.9755270     1.253958            1 #> 3     1.194246            1  1.2073313     1.253958            1 #> 4     1.194246            1  0.4050167     1.253958            1 #> 5     1.194246            1  1.4348013     1.253958            1 #> 6     1.194246            1 -0.3973261     1.253958            1"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/get_fs_int-vignette.html","id":"s-pa-with-product-factor-score-indicator","dir":"Articles","previous_headings":"Illustrative Example","what":"2S-PA with product factor score indicator","title":"get_fs_int-example","text":"example assumes factor scores obtained model latent variances assumed 1. case, users specify corresponding latent variance estimate model, following example, factor scores obtained separate measurement models.","code":"# Rename product factor scores to be used with `tspa()` ind_2$fs_xm <- ind_2$`fs_x:fs_m` ind_2$fs_xz <- ind_2$`fs_x:fs_z` ind_2$fs_mz <- ind_2$`fs_m:fs_z` tspa_fit <- tspa(\"   y ~ x + m + z + xm + xz + mz \", data = ind_2,   se_fs = c(     y = ind_2[1, \"fs_y_se\"],     x = ind_2[1, \"fs_x_se\"],     m = ind_2[1, \"fs_m_se\"],     z = ind_2[1, \"fs_z_se\"],     xm = ind_2[1, \"fs_x:fs_m_se\"],     xz = ind_2[1, \"fs_x:fs_z_se\"],     mz = ind_2[1, \"fs_m:fs_z_se\"]   )) standardizedSolution(tspa_fit) #>      lhs op   rhs est.std    se       z pvalue ci.lower ci.upper #> 1      y =~  fs_y   0.742 0.006 122.490  0.000    0.730    0.754 #> 2      x =~  fs_x   0.819 0.004 203.643  0.000    0.811    0.827 #> 3      m =~  fs_m   0.795 0.005 172.148  0.000    0.786    0.804 #> 4      z =~  fs_z   0.784 0.005 159.443  0.000    0.774    0.794 #> 5     xm =~ fs_xm   0.662 0.008  77.988  0.000    0.645    0.679 #> 6     xz =~ fs_xz   0.649 0.009  72.742  0.000    0.631    0.666 #> 7     mz =~ fs_mz   0.651 0.009  73.382  0.000    0.633    0.668 #> 8   fs_y ~~  fs_y   0.449 0.009  50.000  0.000    0.432    0.467 #> 9   fs_x ~~  fs_x   0.329 0.007  50.000  0.000    0.316    0.342 #> 10  fs_m ~~  fs_m   0.367 0.007  50.000  0.000    0.353    0.382 #> 11  fs_z ~~  fs_z   0.385 0.008  50.000  0.000    0.370    0.401 #> 12 fs_xm ~~ fs_xm   0.562 0.011  50.000  0.000    0.540    0.584 #> 13 fs_xz ~~ fs_xz   0.579 0.012  50.000  0.000    0.556    0.602 #> 14 fs_mz ~~ fs_mz   0.577 0.012  50.000  0.000    0.554    0.599 #> 15     y  ~     x   0.341 0.022  15.484  0.000    0.298    0.385 #> 16     y  ~     m   0.406 0.022  18.561  0.000    0.363    0.448 #> 17     y  ~     z   0.189 0.023   8.056  0.000    0.143    0.235 #> 18     y  ~    xm   0.062 0.030   2.037  0.042    0.002    0.122 #> 19     y  ~    xz   0.178 0.029   6.106  0.000    0.121    0.236 #> 20     y  ~    mz   0.083 0.030   2.740  0.006    0.024    0.143 #> 21     y ~~     y   0.556 0.025  22.657  0.000    0.508    0.604 #> 22     x ~~     x   1.000 0.000      NA     NA    1.000    1.000 #> 23     m ~~     m   1.000 0.000      NA     NA    1.000    1.000 #> 24     z ~~     z   1.000 0.000      NA     NA    1.000    1.000 #> 25    xm ~~    xm   1.000 0.000      NA     NA    1.000    1.000 #> 26    xz ~~    xz   1.000 0.000      NA     NA    1.000    1.000 #> 27    mz ~~    mz   1.000 0.000      NA     NA    1.000    1.000 #> 28     x ~~     m   0.106 0.022   4.907  0.000    0.064    0.148 #> 29     x ~~     z   0.167 0.022   7.699  0.000    0.124    0.209 #> 30     x ~~    xm  -0.030 0.026  -1.157  0.247   -0.081    0.021 #> 31     x ~~    xz  -0.067 0.027  -2.520  0.012   -0.119   -0.015 #> 32     x ~~    mz   0.027 0.027   1.007  0.314   -0.025    0.079 #> 33     m ~~     z   0.128 0.022   5.708  0.000    0.084    0.172 #> 34     m ~~    xm   0.034 0.027   1.256  0.209   -0.019    0.086 #> 35     m ~~    xz   0.028 0.027   1.029  0.304   -0.026    0.082 #> 36     m ~~    mz  -0.013 0.027  -0.491  0.624   -0.067    0.040 #> 37     z ~~    xm   0.028 0.027   1.024  0.306   -0.026    0.081 #> 38     z ~~    xz   0.047 0.028   1.701  0.089   -0.007    0.102 #> 39     z ~~    mz   0.028 0.028   1.017  0.309   -0.026    0.083 #> 40    xm ~~    xz   0.213 0.033   6.544  0.000    0.149    0.277 #> 41    xm ~~    mz   0.282 0.032   8.769  0.000    0.219    0.345 #> 42    xz ~~    mz   0.173 0.033   5.204  0.000    0.108    0.238 mody <- \"y =~ y1 + y2 + y3\" mod1 <- \"x =~ x1 + x2 + x3\" mod2 <- \"m =~ m1 + m2 + m3\" mod3 <- \"z =~ z1 + z2 + z3\" cfay <- cfa(mody, data = df) cfa1 <- cfa(mod1, data = df) cfa2 <- cfa(mod2, data = df) cfa3 <- cfa(mod3, data = df) # Latent variances (lat_var <- vapply(list(cfa1, cfa2, cfa3),                    FUN = function(x) lavInspect(x, \"cov.lv\"),                    FUN.VALUE = numeric(1))) #> [1] 0.8296271 0.7118338 0.6750885 fs_dat2 <- lapply(list(cfay, cfa1, cfa2, cfa3),   FUN = augment_lav_predict,   method = \"Bartlett\" ) |>   do.call(what = cbind) # With user-specified latent variance ind_3 <- get_fs_int(dat = fs_dat2,                     fs_name = c(\"fs_x\", \"fs_m\", \"fs_z\"),                     se_fs = c(\"se_fs_x\", \"se_fs_m\", \"se_fs_z\"),                     lat_var = lat_var,                     loading_fs = c(\"x_by_fs_x\", \"m_by_fs_m\", \"z_by_fs_z\")) head(ind_3) #>         fs_y   se_fs_y y_by_fs_y   ev_fs_y        fs_x   se_fs_x x_by_fs_x #> 1  0.4575840 0.6283493         1 0.3948229 -1.42777196 0.6396597         1 #> 2  0.4401277 0.6283493         1 0.3948229 -1.17905836 0.6396597         1 #> 3 -0.7629969 0.6283493         1 0.3948229  0.94924494 0.6396597         1 #> 4 -0.1888135 0.6283493         1 0.3948229 -1.13986480 0.6396597         1 #> 5  0.5901120 0.6283493         1 0.3948229  0.04530066 0.6396597         1 #> 6 -0.9137330 0.6283493         1 0.3948229 -0.30930312 0.6396597         1 #>     ev_fs_x       fs_m   se_fs_m m_by_fs_m   ev_fs_m       fs_z   se_fs_z #> 1 0.4091646  0.4518302 0.6441129         1 0.4148814 -1.9838745 0.6533299 #> 2 0.4091646  1.2746252 0.6441129         1 0.4148814 -0.9859401 0.6533299 #> 3 0.4091646  1.1487800 0.6441129         1 0.4148814  0.8175038 0.6533299 #> 4 0.4091646 -0.3622108 0.6441129         1 0.4148814 -1.0054432 0.6533299 #> 5 0.4091646 -1.7556345 0.6441129         1 0.4148814 -0.6048528 0.6533299 #> 6 0.4091646  0.3447030 0.6441129         1 0.4148814 -0.5758447 0.6533299 #>   z_by_fs_z ev_fs_z  fs_x:fs_m fs_x:fs_m_se fs_x:fs_m_ld   fs_x:fs_z #> 1         1 0.42684 -0.7264351    0.8973343            1  2.70788210 #> 2         1 0.42684 -1.5841821    0.8973343            1  1.03784270 #> 3         1 0.42684  1.0091489    0.8973343            1  0.65137310 #> 4         1 0.42684  0.3315466    0.8973343            1  1.02143110 #> 5         1 0.42684 -0.1608561    0.8973343            1 -0.15203845 #> 6         1 0.42684 -0.1879424    0.8973343            1  0.05347234 #>   fs_x:fs_z_se fs_x:fs_z_ld  fs_m:fs_z fs_m:fs_z_se fs_m:fs_z_ld #> 1    0.8972113            1 -0.9848401    0.8723582            1 #> 2    0.8972113            1 -1.3451698    0.8723582            1 #> 3    0.8972113            1  0.8506663    0.8723582            1 #> 4    0.8972113            1  0.2757167    0.8723582            1 #> 5    0.8972113            1  0.9734347    0.8723582            1 #> 6    0.8972113            1 -0.2869611    0.8723582            1"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/get_fs_int-vignette.html","id":"s-pa-with-product-factor-score-indicator-1","dir":"Articles","previous_headings":"Illustrative Example","what":"2S-PA with product factor score indicator","title":"get_fs_int-example","text":"Note unstandardized coefficients different scale compared previous example, latent variables scaled differently.","code":"# Rename product factor scores to be used with `tspa()` ind_3$fs_xm <- ind_3$`fs_x:fs_m` ind_3$fs_xz <- ind_3$`fs_x:fs_z` ind_3$fs_mz <- ind_3$`fs_m:fs_z` tspa_fit <- tspa(\"   y ~ x + m + z + xm + xz + mz \", data = ind_3,   se_fs = c(     y = ind_3[1, \"se_fs_y\"],     x = ind_3[1, \"se_fs_x\"],     m = ind_3[1, \"se_fs_m\"],     z = ind_3[1, \"se_fs_z\"],     xm = ind_3[1, \"fs_x:fs_m_se\"],     xz = ind_3[1, \"fs_x:fs_z_se\"],     mz = ind_3[1, \"fs_m:fs_z_se\"]   )) standardizedSolution(tspa_fit) #>      lhs op   rhs est.std    se       z pvalue ci.lower ci.upper #> 1      y =~  fs_y   0.742 0.006 122.531  0.000    0.730    0.754 #> 2      x =~  fs_x   0.818 0.004 202.761  0.000    0.810    0.826 #> 3      m =~  fs_m   0.795 0.005 171.575  0.000    0.786    0.804 #> 4      z =~  fs_z   0.783 0.005 158.160  0.000    0.773    0.792 #> 5     xm =~ fs_xm   0.661 0.009  77.654  0.000    0.644    0.678 #> 6     xz =~ fs_xz   0.647 0.009  72.179  0.000    0.630    0.665 #> 7     mz =~ fs_mz   0.649 0.009  72.823  0.000    0.632    0.667 #> 8   fs_y ~~  fs_y   0.449 0.009  50.000  0.000    0.432    0.467 #> 9   fs_x ~~  fs_x   0.330 0.007  50.000  0.000    0.317    0.343 #> 10  fs_m ~~  fs_m   0.368 0.007  50.000  0.000    0.354    0.383 #> 11  fs_z ~~  fs_z   0.387 0.008  50.000  0.000    0.372    0.403 #> 12 fs_xm ~~ fs_xm   0.563 0.011  50.000  0.000    0.541    0.585 #> 13 fs_xz ~~ fs_xz   0.581 0.012  50.000  0.000    0.558    0.604 #> 14 fs_mz ~~ fs_mz   0.579 0.012  50.000  0.000    0.556    0.601 #> 15     y  ~     x   0.341 0.022  15.440  0.000    0.297    0.384 #> 16     y  ~     m   0.406 0.022  18.551  0.000    0.363    0.448 #> 17     y  ~     z   0.189 0.024   8.040  0.000    0.143    0.235 #> 18     y  ~    xm   0.063 0.031   2.050  0.040    0.003    0.122 #> 19     y  ~    xz   0.179 0.029   6.098  0.000    0.121    0.236 #> 20     y  ~    mz   0.083 0.031   2.717  0.007    0.023    0.143 #> 21     y ~~     y   0.556 0.025  22.639  0.000    0.508    0.604 #> 22     x ~~     x   1.000 0.000      NA     NA    1.000    1.000 #> 23     m ~~     m   1.000 0.000      NA     NA    1.000    1.000 #> 24     z ~~     z   1.000 0.000      NA     NA    1.000    1.000 #> 25    xm ~~    xm   1.000 0.000      NA     NA    1.000    1.000 #> 26    xz ~~    xz   1.000 0.000      NA     NA    1.000    1.000 #> 27    mz ~~    mz   1.000 0.000      NA     NA    1.000    1.000 #> 28     x ~~     m   0.106 0.022   4.900  0.000    0.063    0.148 #> 29     x ~~     z   0.167 0.022   7.665  0.000    0.124    0.209 #> 30     x ~~    xm  -0.029 0.026  -1.118  0.264   -0.080    0.022 #> 31     x ~~    xz  -0.065 0.027  -2.447  0.014   -0.117   -0.013 #> 32     x ~~    mz   0.027 0.027   1.022  0.307   -0.025    0.079 #> 33     m ~~     z   0.128 0.023   5.665  0.000    0.083    0.172 #> 34     m ~~    xm   0.032 0.027   1.203  0.229   -0.020    0.085 #> 35     m ~~    xz   0.029 0.027   1.044  0.296   -0.025    0.083 #> 36     m ~~    mz  -0.014 0.027  -0.503  0.615   -0.068    0.040 #> 37     z ~~    xm   0.028 0.027   1.039  0.299   -0.025    0.082 #> 38     z ~~    xz   0.047 0.028   1.687  0.092   -0.008    0.102 #> 39     z ~~    mz   0.028 0.028   1.003  0.316   -0.027    0.082 #> 40    xm ~~    xz   0.214 0.033   6.567  0.000    0.150    0.278 #> 41    xm ~~    mz   0.282 0.032   8.751  0.000    0.219    0.346 #> 42    xz ~~    mz   0.175 0.033   5.245  0.000    0.110    0.241"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"single-group","dir":"Articles","previous_headings":"","what":"Single Group","title":"Grand Standardized Coefficients","text":"single groups, standardized solution can obtained first obtaining latent variable covariance matrix: 𝛈=𝛂+𝚪𝐗+𝐁𝛈+𝛇(𝐈−𝐁)𝛈=𝛂+𝚪𝐗+𝛇(𝐈−𝐁)Var(𝛈)(𝐈−𝐁)⊤=Var(𝚪𝐗)+𝚿Var(𝛈)=(𝐈−𝐁)−1[Var(𝚪𝐗)+𝚿](𝐈−𝐁)⊤−1   \\begin{aligned}     \\boldsymbol{\\mathbf{\\eta }}& = \\boldsymbol{\\mathbf{\\alpha }}+ \\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}} + \\boldsymbol{\\mathbf{B}} \\boldsymbol{\\mathbf{\\eta }}+ \\boldsymbol{\\mathbf{\\zeta }}\\\\     (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}}) \\boldsymbol{\\mathbf{\\eta }}& = \\boldsymbol{\\mathbf{\\alpha }}+ \\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}} + \\boldsymbol{\\mathbf{\\zeta }}\\\\     (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}}) \\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}) (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^\\top & = \\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}}) + \\boldsymbol{\\mathbf{\\Psi }}\\\\     \\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}) & = (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^{-1} [\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\Gamma }}\\boldsymbol{\\mathbf{X}}) + \\boldsymbol{\\mathbf{\\Psi}}] {(\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^\\top}^{-1}   \\end{aligned} standardized estimates 𝐁\\boldsymbol{\\mathbf{B}} matrix obtained 𝐁s=𝐒η−1𝐁𝐒η−1 \\boldsymbol{\\mathbf{B}}_s = \\boldsymbol{\\mathbf{S}}_\\eta^{-1} \\boldsymbol{\\mathbf{B}} \\boldsymbol{\\mathbf{S}}_\\eta^{-1} 𝐒η\\boldsymbol{\\mathbf{S}}_\\eta diagonal matrix containing square root diagonal elements Var(𝛈)\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}})","code":"myModel <- '    # latent variables      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4    # regressions      dem60 ~ ind60 ' fit <- sem(model = myModel,            data  = PoliticalDemocracy) # Latent variable covariances lavInspect(fit, \"cov.lv\") #>       ind60 dem60 #> ind60 0.449       #> dem60 0.646 4.382 # Using R2spa::veta()` fit_est <- lavInspect(fit, \"est\") R2spa:::veta(fit_est$beta, psi = fit_est$psi) #>           ind60     dem60 #> ind60 0.4485415 0.6455929 #> dem60 0.6455929 4.3824834 S_eta <- diag(   sqrt(diag(     R2spa:::veta(fit_est$beta, psi = fit_est$psi)   )) ) solve(S_eta) %*% fit_est$beta %*% S_eta #>           [,1] [,2] #> [1,] 0.0000000    0 #> [2,] 0.4604657    0 # Compare to lavaan standardizedSolution(fit)[8, ] #>     lhs op   rhs est.std  se     z pvalue ci.lower ci.upper #> 8 dem60  ~ ind60    0.46 0.1 4.593      0    0.264    0.657"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"multiple-groups","dir":"Articles","previous_headings":"","what":"Multiple Groups","title":"Grand Standardized Coefficients","text":"","code":"reg <- '   # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9    # regressions     visual ~ c(b1, b1) * speed ' reg_fit <- sem(reg, data = HolzingerSwineford1939,                group = \"school\",                group.equal = c(\"loadings\", \"intercepts\"))"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"separate-standardization-by-group","dir":"Articles","previous_headings":"Multiple Groups","what":"Separate standardization by group","title":"Grand Standardized Coefficients","text":"","code":"standardizedSolution(reg_fit, type = \"std.lv\") |>   subset(subset = label == \"b1\") #>       lhs op   rhs group label est.std    se     z pvalue ci.lower ci.upper #> 7  visual  ~ speed     1    b1   0.369 0.073 5.032      0    0.226    0.513 #> 30 visual  ~ speed     2    b1   0.496 0.086 5.768      0    0.327    0.664 # Compare to doing it by hand reg_fit_est <- lavInspect(reg_fit, what = \"est\") # Group 1: S_eta1 <- diag(   sqrt(diag(     R2spa:::veta(reg_fit_est[[1]]$beta, psi = reg_fit_est[[1]]$psi)   )) ) solve(S_eta1) %*% reg_fit_est[[1]]$beta %*% S_eta1 #>      [,1]      [,2] #> [1,]    0 0.3694845 #> [2,]    0 0.0000000 # Group 2: S_eta2 <- diag(   sqrt(diag(     R2spa:::veta(reg_fit_est[[2]]$beta, psi = reg_fit_est[[2]]$psi)   )) ) solve(S_eta2) %*% reg_fit_est[[2]]$beta %*% S_eta2 #>      [,1]      [,2] #> [1,]    0 0.4958876 #> [2,]    0 0.0000000"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/gr-std-coef.html","id":"grand-standardization","dir":"Articles","previous_headings":"Multiple Groups","what":"Grand standardization","title":"Grand Standardized Coefficients","text":"group group-specific covariance matrix Var(𝛈g)\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}_g). group-specific latent means E(𝛈g)=(𝐈−𝐁)−1[𝛂+ΓE(𝐗)]\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) = (\\boldsymbol{\\mathbf{}} - \\boldsymbol{\\mathbf{B}})^{-1}[\\boldsymbol{\\mathbf{\\alpha }}+ \\Gamma \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{X}})] grand mean E(𝛈)=∑g=1GngE(𝛈g)/N\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}) = \\sum_{g = 1}^G n_g \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) / N grand covariance matrix : Var(𝛈)=1N∑g=1Gng{Var(𝛈g)+[E(𝛈g)−E(𝛈)][E(𝛈g)−E(𝛈)]⊤}\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}) = \\frac{1}{N} \\sum_{g = 1}^G n_g \\left\\{\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{\\eta}}_g) + [\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) - \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}})][\\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}}_g) - \\mathop{\\mathrm{\\mathrm{E}}}(\\boldsymbol{\\mathbf{\\eta}})]^\\top \\right\\} need involve mean well function grandStandardizdSolution() automates computation grand standardized coefficients, asymptotic standard error obtained using delta method:","code":"# Latent means lavInspect(reg_fit, what = \"mean.lv\")  # lavaan #> $Pasteur #> visual  speed  #>      0      0  #>  #> $`Grant-White` #> visual  speed  #> -0.204 -0.167 R2spa:::eeta(reg_fit_est[[1]]$beta, alpha = reg_fit_est[[1]]$alpha) #>        intercept #> visual         0 #> speed          0 R2spa:::eeta(reg_fit_est[[2]]$beta, alpha = reg_fit_est[[2]]$alpha) #>         intercept #> visual -0.2036667 #> speed  -0.1666424 # Grand covariance ns <- lavInspect(reg_fit, what = \"nobs\") R2spa:::veta_grand(ns, beta_list = lapply(reg_fit_est, `[[`, \"beta\"),                    psi_list = lapply(reg_fit_est, `[[`, \"psi\"),                    alpha_list = lapply(reg_fit_est, `[[`, \"alpha\")) #>           visual     speed #> visual 0.5497595 0.2085522 #> speed  0.2085522 0.4059395 grandStandardizedSolution(reg_fit) #>       lhs op   rhs exo group block label est.std    se     z pvalue ci.lower #> 7  visual  ~ speed   0     1     1    b1   0.431 0.073 5.867      0    0.287 #> 30 visual  ~ speed   0     2     2    b1   0.431 0.073 5.867      0    0.287 #>    ci.upper #> 7     0.575 #> 30    0.575"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/missing-data.html","id":"single-factor-model","dir":"Articles","previous_headings":"","what":"Single-Factor Model","title":"2S-PA with Missing Data","text":"illustration, create artificial data set two missing data pattern: Factor scores lavaan::lavPredict():","code":"set.seed(2225) library(lavaan) #> This is lavaan 0.6-18 #> lavaan is FREE software! Please report any bugs. library(R2spa) library(umx) #> Loading required package: OpenMx #> To take full advantage of multiple cores, use: #>   mxOption(key='Number of Threads', value=parallel::detectCores()) #now #>   Sys.setenv(OMP_NUM_THREADS=parallel::detectCores()) #before library(OpenMx) #> For an overview type '?umx' #>  #> Attaching package: 'umx' #> The following object is masked from 'package:stats': #>  #>     loadings data(PoliticalDemocracy) pd2 <- PoliticalDemocracy # Add MCAR missing data pd2[!rbinom(75, size = 1, prob = 0.7), 9] <- NA fit <- cfa(\"ind60 =~ x1 + x2 + x3\", data = pd2, missing = \"fiml\") fs_lavaan <- lavPredict(fit, method = \"Bartlett\", se = TRUE, fsm = TRUE) # # extract scoring matrix # fsm <- attr(fs_lavaan, \"fsm\")[[1]] # # se for observations with complete data # th <- lavInspect(fit, what = \"est\")$theta # sqrt(diag(fsm %*% th %*% t(fsm))) # # se for observations missing item 1 (need conditional normal) # fsm_mis1 <- with(lavInspect(fit, what = \"est\"), { #     lambda <- lambda[2:3, , drop = FALSE] #     theta <- theta[2:3, 2:3, drop = FALSE] #     # covy <- (lambda %*% psi %*% t(lambda) + theta) #     # ginvcovy <- MASS::ginv(covy) #     # tlam_invcov <- crossprod(lambda, ginvcovy) #     # psi %*% tlam_invcov #     lamt_th_lam <- crossprod(lambda, solve(theta, lambda)) #     solve(lamt_th_lam, crossprod(lambda, solve(theta))) # }) # sqrt(diag(fsm_mis1 %*% th[2:3, 2:3, drop = FALSE] %*% t(fsm_mis1))) # From R2spa fs <- R2spa::get_fs_lavaan(fit, method = \"Bartlett\") # Compare factor scores plot(fs_lavaan, fs$fs_ind60) abline(a = 0, b = 1)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/missing-data.html","id":"multiple-group","dir":"Articles","previous_headings":"Single-Factor Model","what":"Multiple group","title":"2S-PA with Missing Data","text":"","code":"mg_fit <- cfa(\"ind60 =~ x1 + x2 + x3\",   data = cbind(pd2, group = rep(1:2, c(40, 35))),   missing = \"fiml\",   group = \"group\" ) R2spa::get_fs_lavaan(mg_fit) #> $`1` #>        fs_ind60 fs_ind60_se ind60_by_fs_ind60 ev_fs_ind60 group #> 1  -0.629897437  0.06327033         0.9907480 0.004003135     1 #> 2  -0.003597801  0.06481192         0.9902871 0.004200585     1 #> 3   0.531621454  0.06327033         0.9907480 0.004003135     1 #> 4   1.093150997  0.06481192         0.9902871 0.004200585     1 #> 5   0.751322856  0.06481192         0.9902871 0.004200585     1 #> 6   0.032310335  0.06481192         0.9902871 0.004200585     1 #> 7  -0.001194933  0.06327033         0.9907480 0.004003135     1 #> 8  -0.081265180  0.06481192         0.9902871 0.004200585     1 #> 9   0.087671139  0.06327033         0.9907480 0.004003135     1 #> 10  0.158890547  0.06327033         0.9907480 0.004003135     1 #> 11  0.575151483  0.06327033         0.9907480 0.004003135     1 #> 12  0.490722985  0.06327033         0.9907480 0.004003135     1 #> 13  1.199738770  0.06481192         0.9902871 0.004200585     1 #> 14  0.080689638  0.06481192         0.9902871 0.004200585     1 #> 15  0.479293472  0.06327033         0.9907480 0.004003135     1 #> 16  0.304725162  0.06327033         0.9907480 0.004003135     1 #> 17 -0.006880589  0.06481192         0.9902871 0.004200585     1 #> 18 -0.232201592  0.06327033         0.9907480 0.004003135     1 #> 19  0.734594167  0.06481192         0.9902871 0.004200585     1 #> 20  0.871472984  0.06327033         0.9907480 0.004003135     1 #> 21  0.749217422  0.06327033         0.9907480 0.004003135     1 #> 22  0.726471949  0.06327033         0.9907480 0.004003135     1 #> 23  0.397941787  0.06481192         0.9902871 0.004200585     1 #> 24  0.446676514  0.06481192         0.9902871 0.004200585     1 #> 25  0.726400152  0.06327033         0.9907480 0.004003135     1 #> 26 -1.023191030  0.06481192         0.9902871 0.004200585     1 #> 27 -0.229774171  0.06327033         0.9907480 0.004003135     1 #> 28 -0.589883012  0.06481192         0.9902871 0.004200585     1 #> 29 -0.944983987  0.06327033         0.9907480 0.004003135     1 #> 30 -1.473429633  0.06327033         0.9907480 0.004003135     1 #> 31 -0.386068258  0.06481192         0.9902871 0.004200585     1 #> 32 -1.339301841  0.06327033         0.9907480 0.004003135     1 #> 33 -0.282299077  0.06327033         0.9907480 0.004003135     1 #> 34 -0.197502508  0.06327033         0.9907480 0.004003135     1 #> 35 -0.419983839  0.06327033         0.9907480 0.004003135     1 #> 36 -0.737560746  0.06327033         0.9907480 0.004003135     1 #> 37  0.066032960  0.06327033         0.9907480 0.004003135     1 #> 38 -0.644149975  0.06327033         0.9907480 0.004003135     1 #> 39 -1.198883624  0.06481192         0.9902871 0.004200585     1 #> 40 -0.082046566  0.06327033         0.9907480 0.004003135     1 #>  #> $`2` #>       fs_ind60 fs_ind60_se ind60_by_fs_ind60 ev_fs_ind60 group #> 1   0.11486330    0.132523         0.9432493  0.01756235     2 #> 2  -0.41073122    0.132523         0.9432493  0.01756235     2 #> 3  -0.15711924    0.132523         0.9432493  0.01756235     2 #> 4  -0.76084565    0.172422         0.8992304  0.02972934     2 #> 5  -0.88026683    0.132523         0.9432493  0.01756235     2 #> 6  -0.74845673    0.132523         0.9432493  0.01756235     2 #> 7  -0.85061057    0.132523         0.9432493  0.01756235     2 #> 8  -0.55025232    0.172422         0.8992304  0.02972934     2 #> 9   0.26138642    0.132523         0.9432493  0.01756235     2 #> 10  0.32087466    0.132523         0.9432493  0.01756235     2 #> 11  0.50410306    0.132523         0.9432493  0.01756235     2 #> 12 -0.30439462    0.132523         0.9432493  0.01756235     2 #> 13  0.57282757    0.132523         0.9432493  0.01756235     2 #> 14  0.21646719    0.172422         0.8992304  0.02972934     2 #> 15  0.56163448    0.132523         0.9432493  0.01756235     2 #> 16  0.30298279    0.172422         0.8992304  0.02972934     2 #> 17  0.50001516    0.172422         0.8992304  0.02972934     2 #> 18  0.74949815    0.172422         0.8992304  0.02972934     2 #> 19  0.62556020    0.172422         0.8992304  0.02972934     2 #> 20  1.22362501    0.172422         0.8992304  0.02972934     2 #> 21  0.28280329    0.132523         0.9432493  0.01756235     2 #> 22 -0.72202779    0.132523         0.9432493  0.01756235     2 #> 23 -0.53354639    0.172422         0.8992304  0.02972934     2 #> 24  0.02656067    0.132523         0.9432493  0.01756235     2 #> 25 -0.10062174    0.132523         0.9432493  0.01756235     2 #> 26  0.19661034    0.172422         0.8992304  0.02972934     2 #> 27  0.17401287    0.172422         0.8992304  0.02972934     2 #> 28 -0.85294266    0.132523         0.9432493  0.01756235     2 #> 29 -0.30843400    0.132523         0.9432493  0.01756235     2 #> 30 -0.49547393    0.172422         0.8992304  0.02972934     2 #> 31 -0.46151949    0.132523         0.9432493  0.01756235     2 #> 32  0.39305019    0.132523         0.9432493  0.01756235     2 #> 33  1.01756027    0.132523         0.9432493  0.01756235     2 #> 34  0.23274175    0.132523         0.9432493  0.01756235     2 #> 35 -0.13995593    0.132523         0.9432493  0.01756235     2 #>  #> attr(,\"fsT\") #> attr(,\"fsT\")$`1` #>             fs_ind60 #> fs_ind60 0.004003135 #>  #> attr(,\"fsT\")$`2` #>            fs_ind60 #> fs_ind60 0.01756235 #>  #> attr(,\"fsL\") #> attr(,\"fsL\")$`1` #>             ind60 #> fs_ind60 0.990748 #>  #> attr(,\"fsL\")$`2` #>              ind60 #> fs_ind60 0.9432493 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$`1` #> fs_ind60  #>        0  #>  #> attr(,\"fsb\")$`2` #> fs_ind60  #>        0  #>  #> attr(,\"scoring_matrix\") #> attr(,\"scoring_matrix\")$`1` #>             [,1]      [,2]       [,3] #> ind60 0.04744877 0.4008488 0.01499426 #>  #> attr(,\"scoring_matrix\")$`2` #>            [,1]      [,2]      [,3] #> ind60 0.4368268 0.1137155 0.1263754"},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/articles/missing-data.html","id":"regression-scores","dir":"Articles","previous_headings":"Multiple-Factor Model","what":"Regression scores","title":"2S-PA with Missing Data","text":"","code":"# Make last 10 cases completely missing on y5-y8 pd2[66:75, 5:8] <- NA fit2 <- cfa(\"   dem60 =~ a * y1 + b * y2 + c * y3 + d * y4   dem65 =~ a * y5 + b * y6 + c * y7 + d * y8   y1 ~~ y5   y2 ~~ y6   y3 ~~ y7   y4 ~~ y8 \", data = pd2, missing = \"fiml\") fs2r <- lavPredict(fit2,   method = \"EBM\", se = TRUE,   acov = TRUE, fsm = TRUE ) acov_r <- attr(fs2r, \"acov\")[[1]] # Obtain tidy-ed factor scores data fs_dat <- R2spa::augment_lav_predict(fit2) # Run 2spa with OpenMx # Build OpenMx model fsreg_umx <- umxLav2RAM(   \"     dem65 ~ dem60     dem65 + dem60 ~ 1   \",   printTab = FALSE ) #>  #> ?plot.MxModel options: std, means, digits, strip_zero, file, splines=T/F/ortho,..., min=, max =, same = , fixed, resid= 'circle|line|none' cross_load <- matrix(c(   \"dem60_by_fs_dem60\", \"dem60_by_fs_dem65\",   \"dem65_by_fs_dem60\", \"dem65_by_fs_dem65\" ), nrow = 2) |>   `dimnames<-`(list(c(\"fs_dem60\", \"fs_dem65\"), c(\"dem60\", \"dem65\"))) err_cov <- matrix(c(   \"ev_fs_dem60\", \"ecov_fs_dem60_fs_dem65\",   \"ecov_fs_dem60_fs_dem65\", \"ev_fs_dem65\" ), nrow = 2) |>   `dimnames<-`(rep(list(c(\"fs_dem60\", \"fs_dem65\")), 2)) # Need to make factor scores NA when loadings/error variances # are inadmissible fs_dat[66:75, \"fs_dem65\"] <- NA tspa_mx <- tspa_mx_model(fsreg_umx,   data = fs_dat,   mat_ld = cross_load, mat_ev = err_cov ) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx) #> Running 2SPAD with 5 parameters # Summarize the results summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>               name matrix   row   col      Estimate  Std.Error A #> 1   dem60_to_dem65   m1.A dem65 dem60  9.125862e-01 0.07157358   #> 2 dem65_with_dem65   m1.S dem65 dem65  5.369808e-01 0.23101709   #> 3 dem60_with_dem60   m1.S dem60 dem60  4.559667e+00 0.84137910   #> 4     one_to_dem65   m1.M     1 dem65  1.424734e-08 0.14212140   #> 5     one_to_dem60   m1.M     1 dem60 -2.019359e-08 0.26210295   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                    275              404.9387 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 75/280 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -145.0613               414.9387                 415.8083 #> BIC:      -782.3705               426.5261                 410.7675 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-09-06 04:19:22  #> Wall clock time: 0.05901241 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.12  #> Need help?  See help(mxSummary)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/missing-data.html","id":"bartlett-scores","dir":"Articles","previous_headings":"Multiple-Factor Model","what":"Bartlett scores","title":"2S-PA with Missing Data","text":"","code":"# Obtain tidy-ed factor scores data fsb_dat <- augment_lav_predict(fit2, method = \"Bartlett\") #> Warning in sqrt(diag(fsT)): NaNs produced tspab_mx <- tspa_mx_model(fsreg_umx,   data = fsb_dat,   mat_ld = attr(fsb_dat, which = \"ld\"),   mat_ev = attr(fsb_dat, which = \"ev\") ) # Run OpenMx tspab_mx_fit <- mxRun(tspab_mx) #> Running 2SPAD with 5 parameters # Summarize the results summary(tspab_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>               name matrix   row   col      Estimate  Std.Error A #> 1   dem60_to_dem65   m1.A dem65 dem60  9.125862e-01 0.07157513   #> 2 dem65_with_dem65   m1.S dem65 dem65  5.369808e-01 0.23103302   #> 3 dem60_with_dem60   m1.S dem60 dem60  4.559667e+00 0.84138189   #> 4     one_to_dem65   m1.M     1 dem65  1.424717e-08 0.14211902   #> 5     one_to_dem60   m1.M     1 dem60 -2.004115e-08 0.26209661   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                    275              536.4213 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 75/280 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -13.57873               546.4213                 547.2908 #> BIC:     -650.88796               558.0087                 542.2500 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-09-06 04:19:22  #> Wall clock time: 0.07445168 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.12  #> Need help?  See help(mxSummary)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/missing-data.html","id":"compared-to-joint-model","dir":"Articles","previous_headings":"Multiple-Factor Model","what":"Compared to Joint Model","title":"2S-PA with Missing Data","text":"","code":"jfit <- sem(\"   dem60 =~ a * y1 + b * y2 + c * y3 + d * y4   dem65 =~ a * y5 + b * y6 + c * y7 + d * y8   y1 ~~ y5   y2 ~~ y6   y3 ~~ y7   y4 ~~ y8   dem65 ~ dem60 \", data = pd2, missing = \"fiml\") summary(jfit) #> lavaan 0.6-18 ended normally after 53 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        29 #>   Number of equality constraints                     3 #>  #>   Number of observations                            75 #>   Number of missing patterns                         2 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                27.499 #>   Degrees of freedom                                18 #>   P-value (Chi-square)                           0.070 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Observed #>   Observed information based on                Hessian #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   dem60 =~                                             #>     y1         (a)    1.000                            #>     y2         (b)    1.337    0.153    8.738    0.000 #>     y3         (c)    1.182    0.132    8.942    0.000 #>     y4         (d)    1.334    0.136    9.777    0.000 #>   dem65 =~                                             #>     y5         (a)    1.000                            #>     y6         (b)    1.337    0.153    8.738    0.000 #>     y7         (c)    1.182    0.132    8.942    0.000 #>     y8         (d)    1.334    0.136    9.777    0.000 #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   dem65 ~                                              #>     dem60             0.913    0.074   12.357    0.000 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>  .y1 ~~                                                #>    .y5                0.627    0.395    1.587    0.113 #>  .y2 ~~                                                #>    .y6                1.238    0.765    1.618    0.106 #>  .y3 ~~                                                #>    .y7                1.534    0.695    2.207    0.027 #>  .y4 ~~                                                #>    .y8                0.200    0.551    0.363    0.716 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .y1                5.465    0.298   18.348    0.000 #>    .y2                4.256    0.442    9.629    0.000 #>    .y3                6.563    0.397   16.534    0.000 #>    .y4                4.453    0.379   11.733    0.000 #>    .y5                5.181    0.313   16.561    0.000 #>    .y6                2.782    0.403    6.899    0.000 #>    .y7                6.249    0.367   17.043    0.000 #>    .y8                3.996    0.383   10.420    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .y1                2.093    0.490    4.276    0.000 #>    .y2                6.502    1.252    5.195    0.000 #>    .y3                5.443    1.038    5.242    0.000 #>    .y4                2.689    0.716    3.756    0.000 #>    .y5                2.528    0.551    4.588    0.000 #>    .y6                3.677    0.827    4.446    0.000 #>    .y7                3.386    0.760    4.453    0.000 #>    .y8                2.628    0.702    3.745    0.000 #>     dem60             4.560    1.038    4.394    0.000 #>    .dem65             0.537    0.258    2.085    0.037"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/missing-data.html","id":"multiple-group-1","dir":"Articles","previous_headings":"Multiple-Factor Model","what":"Multiple group","title":"2S-PA with Missing Data","text":"Just demonstrate syntax obtaining input data","code":"mg_fit2 <- cfa(\"   dem60 =~ a * y1 + b * y2 + c * y3 + d * y4   dem65 =~ a * y5 + b * y6 + c * y7 + d * y8   y1 ~~ y5   y2 ~~ y6   y3 ~~ y7   y4 ~~ y8 \",   data = cbind(pd2, group = rep(1:2, c(40, 35))),   missing = \"fiml\", group = \"group\" ) #> Warning: lavaan->lavParTable():   #>    using a single label per parameter in a multiple group setting implies  #>    imposing equality constraints across all the groups; If this is not  #>    intended, either remove the label(s), or use a vector of labels (one for  #>    each group); See the Multiple groups section in the man page of  #>    model.syntax. augment_lav_predict(mg_fit2)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/multiple-factors.html","id":"factor-score","dir":"Articles","previous_headings":"","what":"Factor score","title":"Multi-Factor Measurement Model","text":"CFA model multiple latent factors, even indicator loads one factor, resulting factor scores generally weighted composites indicators. Consider regression score, form 𝛈̃=𝐀(𝐲−𝛍̂)+𝛂\\tilde{\\boldsymbol \\eta} = \\mathbf{}(\\mathbf{y} - \\hat{\\boldsymbol \\mu}) + \\boldsymbol{\\alpha} 𝐀=𝚿𝚲⊤𝚺̂−1\\mathbf{} = \\boldsymbol{\\Psi}\\boldsymbol{\\Lambda}^\\top \\hat{\\boldsymbol{\\Sigma}}^{-1} qq×\\timespp matrix, 𝛍̂=𝛎+𝚲𝛂\\hat{\\boldsymbol \\mu} = \\boldsymbol{\\nu} + \\boldsymbol{\\Lambda} \\boldsymbol{\\alpha} 𝚺=𝚲𝚿𝚲⊤+𝚯\\boldsymbol{\\Sigma} = \\boldsymbol{\\Lambda} \\boldsymbol{\\Psi}\\boldsymbol{\\Lambda}^\\top + \\boldsymbol{\\Theta} model-implied means covariances indicators 𝐲\\mathbf{y}, 𝛂\\boldsymbol{\\alpha} 𝚿\\boldsymbol{\\Psi} latent means latent covariances. Therefore, assuming model correctly specified 𝐲=𝛎+𝚲𝛈+𝛆\\mathbf{y} = \\boldsymbol{\\nu} + \\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\boldsymbol{\\varepsilon}, 𝛈̃=𝐀(𝚲𝛈+𝛆−𝚲𝛂)+𝛂=(𝐈−𝐀𝚲)𝛂+𝐀𝚲𝛈+𝐀𝛆.   \\begin{aligned}   \\tilde{\\boldsymbol \\eta} & = \\mathbf{}(\\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\boldsymbol{\\varepsilon} - \\boldsymbol{\\Lambda} \\boldsymbol{\\alpha}) + \\boldsymbol{\\alpha} \\\\   & = (\\mathbf{} - \\mathbf{}\\boldsymbol{\\Lambda}) \\boldsymbol{\\alpha} +   \\mathbf{}\\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\mathbf{}\\boldsymbol{\\varepsilon}.   \\end{aligned} consider 𝛈̃\\tilde{\\boldsymbol \\eta} indicators 𝛈\\boldsymbol \\eta, can see 𝛎𝛈̃=(𝐈−𝐀𝚲)𝛂\\boldsymbol{\\nu}_\\tilde{\\boldsymbol{\\eta}} = (\\mathbf{} - \\mathbf{}\\boldsymbol{\\Lambda}) \\boldsymbol{\\alpha} intercept, 𝚲𝛈̃=𝐀𝚲\\boldsymbol{\\Lambda}_\\tilde{\\boldsymbol{\\eta}} = \\mathbf{}\\boldsymbol{\\Lambda} qq×\\timesqq loading matrix, 𝚯𝛈̃=𝐀𝛆\\boldsymbol{\\Theta}_\\tilde{\\boldsymbol{\\eta}} = \\mathbf{}\\boldsymbol{\\varepsilon} error covariance matrix. can see 𝚲𝛈̃\\boldsymbol{\\Lambda}_\\tilde{\\boldsymbol{\\eta}} generally diagonal, following shows: can also use R2spa::get_fs(): Therefore, need specify cross-loadings using 2S-PA. consistent SEM results.","code":"# CFA my_cfa <- \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4 \" cfa_fit <- cfa(model = my_cfa,                data  = PoliticalDemocracy,                std.lv = TRUE) # A matrix pars <- lavInspect(cfa_fit, what = \"est\") lambda_mat <- pars$lambda psi_mat <- pars$psi sigma_mat <- cfa_fit@implied$cov[[1]] ginvsigma <- MASS::ginv(sigma_mat) alambda <- psi_mat %*% crossprod(lambda_mat, ginvsigma %*% lambda_mat) alambda #>            ind60      dem60 #> ind60 0.95538579 0.01834111 #> dem60 0.05816994 0.86888887 (fs_dat <- get_fs(PoliticalDemocracy, model = my_cfa, std.lv = TRUE)) |> head() #>     fs_ind60   fs_dem60 fs_ind60_se fs_dem60_se ind60_by_fs_ind60 #> 1 -0.8101568 -1.3119114   0.1859987   0.3012901         0.9553858 #> 2  0.1888466 -1.3644831   0.1859987   0.3012901         0.9553858 #> 3  1.0960931  1.3107705   0.1859987   0.3012901         0.9553858 #> 4  1.8702043  1.4849083   0.1859987   0.3012901         0.9553858 #> 5  1.2446060  0.9193277   0.1859987   0.3012901         0.9553858 #> 6  0.3348621  0.4886331   0.1859987   0.3012901         0.9553858 #>   ind60_by_fs_dem60 dem60_by_fs_ind60 dem60_by_fs_dem60 ev_fs_ind60 #> 1        0.05816994        0.01834111         0.8688889  0.03459552 #> 2        0.05816994        0.01834111         0.8688889  0.03459552 #> 3        0.05816994        0.01834111         0.8688889  0.03459552 #> 4        0.05816994        0.01834111         0.8688889  0.03459552 #> 5        0.05816994        0.01834111         0.8688889  0.03459552 #> 6        0.05816994        0.01834111         0.8688889  0.03459552 #>   ecov_fs_dem60_fs_ind60 ev_fs_dem60 #> 1            0.004017388  0.09077571 #> 2            0.004017388  0.09077571 #> 3            0.004017388  0.09077571 #> 4            0.004017388  0.09077571 #> 5            0.004017388  0.09077571 #> 6            0.004017388  0.09077571 tspa_fit <- tspa(model = \"dem60 ~ ind60\", data = fs_dat,                  fsT = attr(fs_dat, \"fsT\"),                   fsL = attr(fs_dat, \"fsL\")) cat(attr(tspa_fit, \"tspaModel\")) #> # latent variables (indicated by factor scores) #>  ind60 =~ c(0.955385785456617) * fs_ind60 + c(0.0581699407736295) * fs_dem60 #>  # latent variables (indicated by factor scores) #>  dem60 =~ c(0.0183411131911887) * fs_ind60 + c(0.868888868390616) * fs_dem60 #>  # constrain the errors #> fs_ind60 ~~ c(0.0345955179271981) * fs_ind60 #>  # constrain the errors #> fs_dem60 ~~ c(0.00401738814566851) * fs_ind60 #>  # constrain the errors #> fs_dem60 ~~ c(0.0907757082269278) * fs_dem60 #>   #>  # structural model #>  dem60 ~ ind60 parameterestimates(tspa_fit) #>         lhs op      rhs   est    se     z pvalue ci.lower ci.upper #> 1     ind60 =~ fs_ind60 0.955 0.000    NA     NA    0.955    0.955 #> 2     ind60 =~ fs_dem60 0.058 0.000    NA     NA    0.058    0.058 #> 3     dem60 =~ fs_ind60 0.018 0.000    NA     NA    0.018    0.018 #> 4     dem60 =~ fs_dem60 0.869 0.000    NA     NA    0.869    0.869 #> 5  fs_ind60 ~~ fs_ind60 0.035 0.000    NA     NA    0.035    0.035 #> 6  fs_ind60 ~~ fs_dem60 0.004 0.000    NA     NA    0.004    0.004 #> 7  fs_dem60 ~~ fs_dem60 0.091 0.000    NA     NA    0.091    0.091 #> 8     dem60  ~    ind60 0.460 0.113 4.089      0    0.240    0.681 #> 9     ind60 ~~    ind60 1.000 0.169 5.900      0    0.668    1.332 #> 10    dem60 ~~    dem60 0.788 0.150 5.267      0    0.495    1.081"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/multiple-factors.html","id":"three-factor-model-example","dir":"Articles","previous_headings":"","what":"Three-factor model example","title":"Multi-Factor Measurement Model","text":"can also use R2spa::get_fs(): Therefore, need specify cross-loadings using 2S-PA. Compare SEM:","code":"# CFA cfa_3fac <-  \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     dem65 =~ y5 + y6 + y7 + y8 \" cfa_3fac_fit <- cfa(model = cfa_3fac,                     data  = PoliticalDemocracy,                     std.lv = TRUE) # A matrix pars <- lavInspect(cfa_3fac_fit, what = \"est\") lambda_mat <- pars$lambda psi_mat <- pars$psi sigma_mat <- cfa_3fac_fit@implied$cov[[1]] ginvsigma <- MASS::ginv(sigma_mat) alambda <- psi_mat %*% crossprod(lambda_mat, ginvsigma %*% lambda_mat) alambda #>             ind60        dem60      dem65 #> ind60  0.95064774 -0.005967124 0.02951139 #> dem60 -0.02069724  0.533020047 0.41603787 #> dem65  0.09868528  0.401095944 0.49133377 (fs_dat_3fac <- get_fs(PoliticalDemocracy, model = cfa_3fac, std.lv = TRUE)) |>   head() #>     fs_ind60   fs_dem60    fs_dem65 fs_ind60_se fs_dem60_se fs_dem65_se #> 1 -0.7990475 -1.1571745 -1.13720655   0.1844193   0.2422541   0.2270976 #> 2  0.2152486 -1.0238236 -0.80871922   0.1844193   0.2422541   0.2270976 #> 3  1.1028297  1.3890842  1.46389950   0.1844193   0.2422541   0.2270976 #> 4  1.8585004  1.3163888  1.43045560   0.1844193   0.2422541   0.2270976 #> 5  1.2432985  0.9522026  1.03348589   0.1844193   0.2422541   0.2270976 #> 6  0.3067994  0.1109206  0.05023217   0.1844193   0.2422541   0.2270976 #>   ind60_by_fs_ind60 ind60_by_fs_dem60 ind60_by_fs_dem65 dem60_by_fs_ind60 #> 1         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 2         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 3         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 4         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 5         0.9506477       -0.02069724        0.09868528      -0.005967124 #> 6         0.9506477       -0.02069724        0.09868528      -0.005967124 #>   dem60_by_fs_dem60 dem60_by_fs_dem65 dem65_by_fs_ind60 dem65_by_fs_dem60 #> 1           0.53302         0.4010959        0.02951139         0.4160379 #> 2           0.53302         0.4010959        0.02951139         0.4160379 #> 3           0.53302         0.4010959        0.02951139         0.4160379 #> 4           0.53302         0.4010959        0.02951139         0.4160379 #> 5           0.53302         0.4010959        0.02951139         0.4160379 #> 6           0.53302         0.4010959        0.02951139         0.4160379 #>   dem65_by_fs_dem65 ev_fs_ind60 ecov_fs_dem60_fs_ind60 ev_fs_dem60 #> 1         0.4913338   0.0340105           0.0003881641  0.05868703 #> 2         0.4913338   0.0340105           0.0003881641  0.05868703 #> 3         0.4913338   0.0340105           0.0003881641  0.05868703 #> 4         0.4913338   0.0340105           0.0003881641  0.05868703 #> 5         0.4913338   0.0340105           0.0003881641  0.05868703 #> 6         0.4913338   0.0340105           0.0003881641  0.05868703 #>   ecov_fs_dem65_fs_ind60 ecov_fs_dem65_fs_dem60 ev_fs_dem65 #> 1            0.005026024             0.05337525   0.0515733 #> 2            0.005026024             0.05337525   0.0515733 #> 3            0.005026024             0.05337525   0.0515733 #> 4            0.005026024             0.05337525   0.0515733 #> 5            0.005026024             0.05337525   0.0515733 #> 6            0.005026024             0.05337525   0.0515733 tspa_fit_3fac <- tspa(model = \"dem60 ~ ind60               dem65 ~ ind60 + dem60\",               data = fs_dat_3fac,               fsT = attr(fs_dat_3fac, \"fsT\"),               fsL = attr(fs_dat_3fac, \"fsL\")) cat(attr(tspa_fit_3fac, \"tspaModel\")) #> # latent variables (indicated by factor scores) #>  ind60 =~ c(0.950647742844847) * fs_ind60 + c(-0.0206972362902851) * fs_dem60 + c(0.0986852834195767) * fs_dem65 #>  # latent variables (indicated by factor scores) #>  dem60 =~ c(-0.00596712444175395) * fs_ind60 + c(0.533020047181513) * fs_dem60 + c(0.401095943690546) * fs_dem65 #>  # latent variables (indicated by factor scores) #>  dem65 =~ c(0.0295113941487134) * fs_ind60 + c(0.416037872255944) * fs_dem60 + c(0.491333767947424) * fs_dem65 #>  # constrain the errors #> fs_ind60 ~~ c(0.0340104951546674) * fs_ind60 #>  # constrain the errors #> fs_dem60 ~~ c(0.00038816407039879) * fs_ind60 #>  # constrain the errors #> fs_dem65 ~~ c(0.00502602420598866) * fs_ind60 #>  # constrain the errors #> fs_dem60 ~~ c(0.0586870313996517) * fs_dem60 #>  # constrain the errors #> fs_dem65 ~~ c(0.0533752457536215) * fs_dem60 #>  # constrain the errors #> fs_dem65 ~~ c(0.0515732993693881) * fs_dem65 #>   #>  # structural model #>  dem60 ~ ind60 #>               dem65 ~ ind60 + dem60 parameterestimates(tspa_fit_3fac) #>         lhs op      rhs    est    se      z pvalue ci.lower ci.upper #> 1     ind60 =~ fs_ind60  0.951 0.000     NA     NA    0.951    0.951 #> 2     ind60 =~ fs_dem60 -0.021 0.000     NA     NA   -0.021   -0.021 #> 3     ind60 =~ fs_dem65  0.099 0.000     NA     NA    0.099    0.099 #> 4     dem60 =~ fs_ind60 -0.006 0.000     NA     NA   -0.006   -0.006 #> 5     dem60 =~ fs_dem60  0.533 0.000     NA     NA    0.533    0.533 #> 6     dem60 =~ fs_dem65  0.401 0.000     NA     NA    0.401    0.401 #> 7     dem65 =~ fs_ind60  0.030 0.000     NA     NA    0.030    0.030 #> 8     dem65 =~ fs_dem60  0.416 0.000     NA     NA    0.416    0.416 #> 9     dem65 =~ fs_dem65  0.491 0.000     NA     NA    0.491    0.491 #> 10 fs_ind60 ~~ fs_ind60  0.034 0.000     NA     NA    0.034    0.034 #> 11 fs_ind60 ~~ fs_dem60  0.000 0.000     NA     NA    0.000    0.000 #> 12 fs_ind60 ~~ fs_dem65  0.005 0.000     NA     NA    0.005    0.005 #> 13 fs_dem60 ~~ fs_dem60  0.059 0.000     NA     NA    0.059    0.059 #> 14 fs_dem60 ~~ fs_dem65  0.053 0.000     NA     NA    0.053    0.053 #> 15 fs_dem65 ~~ fs_dem65  0.052 0.000     NA     NA    0.052    0.052 #> 16    dem60  ~    ind60  0.448 0.114  3.937  0.000    0.225    0.671 #> 17    dem65  ~    ind60  0.146 0.069  2.112  0.035    0.010    0.281 #> 18    dem65  ~    dem60  0.913 0.073 12.435  0.000    0.769    1.057 #> 19    ind60 ~~    ind60  1.000 0.169  5.902  0.000    0.668    1.332 #> 20    dem60 ~~    dem60  0.799 0.153  5.224  0.000    0.499    1.099 #> 21    dem65 ~~    dem65  0.026 0.043  0.620  0.535   -0.057    0.110 sem_3fac <- sem(\"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     dem65 =~ y5 + y6 + y7 + y8   # structural model     dem60 ~ ind60     dem65 ~ ind60 + dem60   \",   data = PoliticalDemocracy ) standardizedSolution(sem_3fac) #>      lhs op   rhs est.std    se      z pvalue ci.lower ci.upper #> 1  ind60 =~    x1   0.920 0.023 39.823  0.000    0.874    0.965 #> 2  ind60 =~    x2   0.973 0.017 58.858  0.000    0.941    1.006 #> 3  ind60 =~    x3   0.872 0.031 28.034  0.000    0.811    0.933 #> 4  dem60 =~    y1   0.845 0.039 21.698  0.000    0.769    0.921 #> 5  dem60 =~    y2   0.760 0.054 14.142  0.000    0.655    0.866 #> 6  dem60 =~    y3   0.705 0.063 11.225  0.000    0.582    0.828 #> 7  dem60 =~    y4   0.860 0.036 23.650  0.000    0.789    0.931 #> 8  dem65 =~    y5   0.803 0.046 17.602  0.000    0.714    0.893 #> 9  dem65 =~    y6   0.783 0.049 15.918  0.000    0.687    0.879 #> 10 dem65 =~    y7   0.819 0.043 19.122  0.000    0.735    0.903 #> 11 dem65 =~    y8   0.847 0.038 22.389  0.000    0.773    0.921 #> 12 dem60  ~ ind60   0.448 0.102  4.393  0.000    0.248    0.648 #> 13 dem65  ~ ind60   0.146 0.070  2.071  0.038    0.008    0.283 #> 14 dem65  ~ dem60   0.913 0.048 19.120  0.000    0.819    1.006 #> 15    x1 ~~    x1   0.154 0.042  3.636  0.000    0.071    0.238 #> 16    x2 ~~    x2   0.053 0.032  1.634  0.102   -0.010    0.116 #> 17    x3 ~~    x3   0.240 0.054  4.417  0.000    0.133    0.346 #> 18    y1 ~~    y1   0.286 0.066  4.348  0.000    0.157    0.415 #> 19    y2 ~~    y2   0.422 0.082  5.166  0.000    0.262    0.582 #> 20    y3 ~~    y3   0.503 0.089  5.676  0.000    0.329    0.676 #> 21    y4 ~~    y4   0.261 0.063  4.173  0.000    0.138    0.383 #> 22    y5 ~~    y5   0.355 0.073  4.842  0.000    0.211    0.499 #> 23    y6 ~~    y6   0.387 0.077  5.024  0.000    0.236    0.538 #> 24    y7 ~~    y7   0.329 0.070  4.696  0.000    0.192    0.467 #> 25    y8 ~~    y8   0.283 0.064  4.416  0.000    0.157    0.408 #> 26 ind60 ~~ ind60   1.000 0.000     NA     NA    1.000    1.000 #> 27 dem60 ~~ dem60   0.799 0.091  8.737  0.000    0.620    0.978 #> 28 dem65 ~~ dem65   0.026 0.046  0.579  0.562   -0.063    0.116"},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/articles/reliability.html","id":"theoretical-values","dir":"Articles","previous_headings":"Analytic Formula for Factor Scores","what":"Theoretical Values","title":"Reliability of Factor Scores","text":"aka, using true parameter values compute weights unidimensional factor model 𝐲=𝛌η+𝐞\\boldsymbol{\\mathbf{y}}= \\boldsymbol{\\mathbf{\\lambda }}\\eta + \\boldsymbol{\\mathbf{e}}, factor scores form η̃=𝐚′(𝐲−𝛍̂)=𝐚′(𝛎+𝛌η+𝐞−𝛎)=𝐚′𝛌η+𝐚′𝐞, \\begin{aligned}   \\tilde \\eta & = \\boldsymbol{\\mathbf{}}' (\\boldsymbol{\\mathbf{y}} - \\hat {\\boldsymbol{\\mathbf{\\mu}}}) = \\boldsymbol{\\mathbf{}}' (\\boldsymbol{\\mathbf{\\nu }}+ \\boldsymbol{\\mathbf{\\lambda }}\\eta + \\boldsymbol{\\mathbf{e}} - \\boldsymbol{\\mathbf{\\nu}}) \\\\   & = \\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{\\lambda }}\\eta + \\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{e}}, \\end{aligned} set E(η)\\mathop{\\mathrm{\\mathrm{E}}}(\\eta) = 0 𝛍̂\\hat {\\boldsymbol{\\mathbf{\\mu}}} = 𝛎\\boldsymbol{\\mathbf{\\nu}}, theoretical reliability formula factor scores, assuming sampling errors weights given ρ=(𝐚′𝛌)2(𝐚′𝛌)2+𝐚′𝚯𝐚, \\rho = \\frac{(\\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{\\lambda}})^2}{(\\boldsymbol{\\mathbf{}}'\\boldsymbol{\\mathbf{\\lambda}})^2 + \\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{\\Theta }}\\boldsymbol{\\mathbf{}}}, assuming set Var(η)=1\\mathop{\\mathrm{\\mathrm{Var}}}(\\eta) = 1.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/reliability.html","id":"regression-factor-scores","dir":"Articles","previous_headings":"Analytic Formula for Factor Scores > Theoretical Values","what":"Regression factor scores","title":"Reliability of Factor Scores","text":"𝐚\\boldsymbol{\\mathbf{}} = (𝚺−1𝛌)(\\boldsymbol{\\mathbf{\\Sigma}}^{-1} \\boldsymbol{\\mathbf{\\lambda}}), Rel=(𝛌′𝚺−1𝛌)2𝛌′𝚺−1𝛌𝛌𝚺−1𝛌+𝛌′𝚺−1𝚯𝚺−1𝛌=(𝛌′𝚺−1𝛌)2𝛌′𝚺−1(𝛌𝛌′+𝚯)𝚺−1𝛌=(𝛌′𝚺−1𝛌)2𝛌′𝚺−1𝛌=𝐚′𝛌 \\begin{align*} \\text{Rel}  &= \\frac{(\\boldsymbol{\\mathbf{\\lambda}}' \\boldsymbol{\\mathbf{\\Sigma}}^{-1} \\boldsymbol{\\mathbf{\\lambda}})^2}{\\boldsymbol{\\mathbf{\\lambda}}'\\boldsymbol{\\mathbf{\\Sigma}}^{-1}\\boldsymbol{\\mathbf{\\lambda }}\\boldsymbol{\\mathbf{\\lambda }}\\boldsymbol{\\mathbf{\\Sigma}}^{-1}\\boldsymbol{\\mathbf{\\lambda }}+ \\boldsymbol{\\mathbf{\\lambda}}'\\boldsymbol{\\mathbf{\\Sigma}}^{-1}\\boldsymbol{\\mathbf{\\Theta }}\\boldsymbol{\\mathbf{\\Sigma}}^{-1}\\boldsymbol{\\mathbf{\\lambda}}} \\\\ &= \\frac{(\\boldsymbol{\\mathbf{\\lambda}}'\\boldsymbol{\\mathbf{\\Sigma}}^{-1}\\boldsymbol{\\mathbf{\\lambda}})^2}{\\boldsymbol{\\mathbf{\\lambda}}'\\boldsymbol{\\mathbf{\\Sigma}}^{-1}(\\boldsymbol{\\mathbf{\\lambda}}\\boldsymbol{\\mathbf{\\lambda}}'+\\boldsymbol{\\mathbf{\\Theta}})\\boldsymbol{\\mathbf{\\Sigma}}^{-1}\\boldsymbol{\\mathbf{\\lambda}}} \\\\ &= \\frac{(\\boldsymbol{\\mathbf{\\lambda}}'\\boldsymbol{\\mathbf{\\Sigma}}^{-1}\\boldsymbol{\\mathbf{\\lambda}})^2}{\\boldsymbol{\\mathbf{\\lambda}}'\\boldsymbol{\\mathbf{\\Sigma}}^{-1}\\boldsymbol{\\mathbf{\\lambda}}} \\\\ &= \\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{\\lambda}} \\end{align*}","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/reliability.html","id":"bartlett-factor-scores","dir":"Articles","previous_headings":"Analytic Formula for Factor Scores > Theoretical Values","what":"Bartlett factor scores","title":"Reliability of Factor Scores","text":"𝐚\\boldsymbol{\\mathbf{}} = 𝚯−1𝛌(𝛌′𝚯−1𝛌)−1\\boldsymbol{\\mathbf{\\Theta}}^{-1} \\boldsymbol{\\mathbf{\\lambda }}(\\boldsymbol{\\mathbf{\\lambda}}' \\boldsymbol{\\mathbf{\\Theta}}^{-1} \\boldsymbol{\\mathbf{\\lambda}})^{-1}, thus 𝐚′𝛌=1\\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{\\lambda }}= 1, Rel=11+(𝛌′𝚯−1𝛌)−1𝛌′𝚯−1𝚯𝚯−1𝛌(𝛌′𝚯−1𝛌)−1=11+(𝛌′𝚯−1𝛌)−1 \\begin{align*} \\text{Rel}  &= \\frac{1}{1 + (\\boldsymbol{\\mathbf{\\lambda}}' \\boldsymbol{\\mathbf{\\Theta}}^{-1} \\boldsymbol{\\mathbf{\\lambda}})^{-1} \\boldsymbol{\\mathbf{\\lambda}}' \\boldsymbol{\\mathbf{\\Theta}}^{-1} \\boldsymbol{\\mathbf{\\Theta }}\\boldsymbol{\\mathbf{\\Theta}}^{-1} \\boldsymbol{\\mathbf{\\lambda }}(\\boldsymbol{\\mathbf{\\lambda}}' \\boldsymbol{\\mathbf{\\Theta}}^{-1} \\boldsymbol{\\mathbf{\\lambda}})^{-1}} \\\\ &= \\frac{1}{1 + (\\boldsymbol{\\mathbf{\\lambda}}' \\boldsymbol{\\mathbf{\\Theta}}^{-1} \\boldsymbol{\\mathbf{\\lambda}})^{-1}} \\end{align*}","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/reliability.html","id":"sample-estimators","dir":"Articles","previous_headings":"Analytic Formula for Factor Scores","what":"Sample Estimators","title":"Reliability of Factor Scores","text":"factor scores computed using sample estimates 𝛌\\boldsymbol{\\mathbf{\\lambda}} 𝚯\\boldsymbol{\\mathbf{\\Theta}}, 𝛍̂\\hat{\\boldsymbol{\\mathbf{\\mu}}} = 𝛎̂\\hat{\\boldsymbol{\\mathbf{\\nu}}}, η̃=𝐚̂′(𝐲−𝛍̂)=𝐚̂′(𝛎+𝛌η+𝐞−𝛎̂)=𝐚̂′𝛌η+𝐚̂′(𝛎−𝛎̂)+𝐚̂′𝐞, \\begin{aligned}   \\tilde \\eta & = \\hat{\\boldsymbol{\\mathbf{}}}' (\\boldsymbol{\\mathbf{y}} - \\hat {\\boldsymbol{\\mathbf{\\mu}}}) = \\hat{\\boldsymbol{\\mathbf{}}}' (\\boldsymbol{\\mathbf{\\nu }}+ \\boldsymbol{\\mathbf{\\lambda }}\\eta + \\boldsymbol{\\mathbf{e}} - \\hat{\\boldsymbol{\\mathbf{\\nu}}}) \\\\   & = \\hat{\\boldsymbol{\\mathbf{}}}' \\boldsymbol{\\mathbf{\\lambda }}\\eta + \\hat{\\boldsymbol{\\mathbf{}}}'(\\boldsymbol{\\mathbf{\\nu }}- \\hat{\\boldsymbol{\\mathbf{\\nu}}}) + \\hat{\\boldsymbol{\\mathbf{}}}' \\boldsymbol{\\mathbf{e}}, \\end{aligned} 𝐚̂\\boldsymbol{\\mathbf{\\hat }} = vector estimated weights length pp 𝐚\\boldsymbol{\\mathbf{}} = vector true weights length pp, 𝐲\\boldsymbol{\\mathbf{y}} = vector observed scores length pp, λ\\lambda = vector factor loadings length pp, η\\eta = latent variable, 𝐞\\boldsymbol{\\mathbf{e}} = vector measurement errors length pp, Θ\\Theta = Var(𝐞)\\mathop{\\mathrm{\\mathrm{Var}}}(\\boldsymbol{\\mathbf{e}}) = p×pp \\times p variance-covariance matrix errors, pp = number items. assume 𝐚̂\\hat{\\boldsymbol{\\mathbf{}}} 𝛎̂\\hat{\\boldsymbol{\\mathbf{\\nu}}} independent, E(𝐚̂)=𝐚E(\\hat{\\boldsymbol{\\mathbf{}}}) = \\boldsymbol{\\mathbf{}}, Var(𝐚̂)=Vâ\\mathop{\\mathrm{\\mathrm{Var}}}(\\hat{\\boldsymbol{\\mathbf{}}}) = V_{\\hat }, E(𝛎̂)=𝛎E(\\hat{\\boldsymbol{\\mathbf{\\nu}}}) = \\boldsymbol{\\mathbf{\\nu}}, Var(𝛎̂)=Vν̂\\mathop{\\mathrm{\\mathrm{Var}}}(\\hat{\\boldsymbol{\\mathbf{\\nu}}}) = V_{\\hat \\nu}, using law total variance, Var(η̃)=E[Var(η̃|𝐚̂)]+Var[E(η̃|𝐚̂)]=E[𝐚̂′𝛌𝛌′𝐚̂+𝐚′Vν̂𝐚+𝐚̂′𝚯𝐚̂]+V[𝐚̂′0]=𝐚′𝛌𝛌′𝐚+Tr(Vâ𝛌𝛌′)+𝐚′Vν̂𝐚+Tr(VâVν̂)+𝐚′𝚯𝐚+Tr(Vâ𝚯) \\begin{aligned}   \\mathop{\\mathrm{\\mathrm{Var}}}(\\tilde \\eta) & = \\mathop{\\mathrm{\\mathrm{E}}}[\\mathop{\\mathrm{\\mathrm{Var}}}(\\tilde \\eta | \\hat{\\boldsymbol{\\mathbf{}}})] + \\mathop{\\mathrm{\\mathrm{Var}}}[\\mathop{\\mathrm{\\mathrm{E}}}(\\tilde \\eta | \\hat{\\boldsymbol{\\mathbf{}}})] \\\\   & = \\mathop{\\mathrm{\\mathrm{E}}}[\\hat {\\boldsymbol{\\mathbf{}}}' \\boldsymbol{\\mathbf{\\lambda }}\\boldsymbol{\\mathbf{\\lambda}}' \\hat{\\boldsymbol{\\mathbf{}}} + \\boldsymbol{\\mathbf{}}' V_{\\hat \\nu} \\boldsymbol{\\mathbf{}} + \\hat {\\boldsymbol{\\mathbf{}}}' \\boldsymbol{\\mathbf{\\Theta }}\\hat{\\boldsymbol{\\mathbf{}}}] + V[\\hat{\\boldsymbol{\\mathbf{}}}' 0] \\\\   & = \\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{\\lambda }}\\boldsymbol{\\mathbf{\\lambda}}' \\boldsymbol{\\mathbf{}} + \\mathop{\\mathrm{\\mathrm{Tr}}}(V_{\\hat } \\boldsymbol{\\mathbf{\\lambda }}\\boldsymbol{\\mathbf{\\lambda}}') + \\boldsymbol{\\mathbf{}}' V_{\\hat \\nu} \\boldsymbol{\\mathbf{}} + \\mathop{\\mathrm{\\mathrm{Tr}}}(V_{\\hat } V_{\\hat \\nu}) + \\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{\\Theta }}\\boldsymbol{\\mathbf{}} + \\mathop{\\mathrm{\\mathrm{Tr}}}(V_{\\hat } \\boldsymbol{\\mathbf{\\Theta}}) \\end{aligned} , accounting sampling error, ρ=(𝐚′𝛌)2+𝛌′Vâ𝛌(𝐚′𝛌)2+𝐚′𝚯𝐚+𝛌′Vâ𝛌+𝐚′Vν̂𝐚+Tr(VâVν̂)+Tr(𝚯𝐕â) \\rho = \\frac{(\\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{\\lambda}})^2 + \\boldsymbol{\\mathbf{\\lambda}}'V_{\\hat } \\boldsymbol{\\mathbf{\\lambda}}}{(\\boldsymbol{\\mathbf{}}' \\boldsymbol{\\mathbf{\\lambda}})^2 + \\boldsymbol{\\mathbf{}}'\\boldsymbol{\\mathbf{\\Theta }}\\boldsymbol{\\mathbf{}} + \\boldsymbol{\\mathbf{\\lambda}}'V_{\\hat }\\boldsymbol{\\mathbf{\\lambda }}+ \\boldsymbol{\\mathbf{}}' V_{\\hat \\nu} \\boldsymbol{\\mathbf{}} + \\mathop{\\mathrm{\\mathrm{Tr}}}(V_{\\hat } V_{\\hat \\nu}) + \\mathop{\\mathrm{\\mathrm{Tr}}}(\\boldsymbol{\\mathbf{\\Theta }}\\boldsymbol{\\mathbf{V}}_{\\hat })} sample estimates can obtained substituting sample estimates 𝛌\\boldsymbol{\\mathbf{\\lambda}} 𝚯\\boldsymbol{\\mathbf{\\Theta}} using corresponding form 𝐚\\boldsymbol{\\mathbf{}}.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/reliability.html","id":"quadratic-formula-for-regression-factor-scores","dir":"Articles","previous_headings":"Analytic Formula for Factor Scores > Sample Estimators","what":"Quadratic formula for regression factor scores","title":"Reliability of Factor Scores","text":"reliability formula regression factor scores derived using quadratic formula Rel=1±1−4θcorrected/ψ2\\text{Rel} = \\frac{1\\pm\\sqrt{1 - 4\\theta_{corrected}/\\psi}}{2} θcorrected\\theta_{corrected} corrected error term factor scores.","code":""},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/articles/reliability.html","id":"regression-factor-scores-1","dir":"Articles","previous_headings":"Empirical Example","what":"Regression Factor Scores","title":"Reliability of Factor Scores","text":"","code":"# Three items from the classic Holzinger and Swineford (1939) data cfa_fit <- cfa(\"   visual =~ x1 + x2 + x3 \",   data = HolzingerSwineford1939,   std.lv = TRUE ) # Uncorrected reliability of regression factor scores fsr <- lavPredict(cfa_fit, fsm = TRUE, acov = TRUE) attr(fsr, \"fsm\")[[1]] %*% lavInspect(cfa_fit, what = \"est\")$lambda ##           visual ## visual 0.6598382 # Empirical reliability of regression factor scores var(fsr) / (var(fsr) + attr(fsr, \"acov\")[[1]]) ##           visual ## visual 0.6605848 # Corrected reliability of regression factor scores get_a <- function(lambda, theta) {   solve(tcrossprod(lambda) + theta, lambda) } lam <- lavInspect(cfa_fit, what = \"est\")$lambda th <- lavInspect(cfa_fit, what = \"est\")$theta jac_a <- lavaan::lav_func_jacobian_complex(   \\(x) get_a(x[1:3], diag(x[4:6])),   x = c(lam, diag(th)) ) # Estimated sampling variance of weights va <- jac_a %*% vcov(cfa_fit)[1:6, 1:6] %*% t(jac_a) aa <- tcrossprod(get_a(lam, th)) + va sum(diag(tcrossprod(lam) %*% aa)) /   sum(diag(cfa_fit@implied$cov[[1]] %*% aa)) ## [1] 0.64774 # Or using `get_fs_lavaan` get_fs_lavaan(cfa_fit, reliability = TRUE) |>   attr(which = \"reliability\") ## [1] 0.64774"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/reliability.html","id":"bartlett-factor-scores-1","dir":"Articles","previous_headings":"Empirical Example","what":"Bartlett Factor Scores","title":"Reliability of Factor Scores","text":"","code":"# Uncorrected reliability of Bartlett factor scores fsb <- lavPredict(cfa_fit, method = \"Bartlett\", fsm = TRUE, acov = TRUE) 1 / (1 + attr(fsb, \"acov\")[[1]]) ##           visual ## visual 0.6598382 # Empirical reliability of regression factor scores (var(fsb) - attr(fsb, \"acov\")[[1]]) / var(fsb) ##           visual ## visual 0.6609684 # Corrected reliability of regression factor scores get_a <- function(lambda, theta) {   thinv_lam <- solve(theta, lambda)   solve(crossprod(lambda, thinv_lam), t(thinv_lam)) } lam <- lavInspect(cfa_fit, what = \"est\")$lambda th <- lavInspect(cfa_fit, what = \"est\")$theta jac_a <- lavaan::lav_func_jacobian_complex(   \\(x) get_a(x[1:3], diag(x[4:6])),   x = c(lam, diag(th)) ) # Estimated sampling variance of weights va <- jac_a %*% vcov(cfa_fit)[1:6, 1:6] %*% t(jac_a) aa <- tcrossprod(t(get_a(lam, th))) + va sum(diag(tcrossprod(lam) %*% aa)) /   sum(diag(cfa_fit@implied$cov[[1]] %*% aa)) ## [1] 0.6477805 # Or using `get_fs_lavaan` (need to be fixed) R2spa::get_fs_lavaan(cfa_fit, method = \"Bartlett\", reliability = TRUE) |>   attr(which = \"reliability\") ## [1] 0.6477805"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/reliability.html","id":"multiple-group-example","dir":"Articles","previous_headings":"Empirical Example","what":"Multiple-group example","title":"Reliability of Factor Scores","text":"","code":"mod <- \"   visual =~ x1 + x2 + x3 \" get_fs(HolzingerSwineford1939, model = mod, group = \"school\",        reliability = TRUE, std.lv = TRUE, method = \"regression\") |>   attr(which = \"reliability\") ## $Pasteur ## [1] 0.6473336 ##  ## $`Grant-White` ## [1] 0.6789831 ##  ## $overall ## [1] 0.66258 get_fs(HolzingerSwineford1939, model = mod, group = \"school\",        reliability = TRUE, std.lv = TRUE, method = \"Bartlett\") |>   attr(which = \"reliability\") ## $Pasteur ## [1] 0.6474885 ##  ## $`Grant-White` ## [1] 0.6790979 ##  ## $overall ## [1] 0.6627156"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/reliability.html","id":"simulations","dir":"Articles","previous_headings":"","what":"Simulations","title":"Reliability of Factor Scores","text":"","code":"set.seed(2356) lambda <- seq(.3, .9, length.out = 6) theta <- 1 - lambda^2 num_obs <- 100  num_sims <- 2000 out <- matrix(ncol = 7, nrow = num_sims)  # Copy function for computing a get_a <- function(lambda, theta) {   solve(tcrossprod(lambda) + theta, lambda) }  # Function for getting all versions all_rel <- function(lam, th, vc) {   sigma <- tcrossprod(lam) + th   ahat <- crossprod(lam, solve(sigma))   # Formula 1: no adjustment   rel1 <- ahat %*% lam    jac_a <- lavaan::lav_func_jacobian_complex(     function(x, p) {       get_a(x[seq_len(p)], theta = diag(x[-(seq_len(p))]))     },     x = c(lam, diag(th)),     p = length(lam)   )   va <- jac_a %*% vc %*% t(jac_a)   aa <- crossprod(ahat) + va   # Formula 2: adjust for both error in weights and true variances   rel2 <- sum(diag(tcrossprod(lam) %*% aa)) / sum(diag(sigma %*% aa))    ev_fs <- sum(diag(th %*% aa))   # Formula 3: solve quadratic equation with adjusted error   rel3 <- (1 + sqrt(1 - 4 * ev_fs)) / 2    c(rel1, rel2, rel3) }  # Function for getting all bias-corrected versions all_bc_rel <- function(fit, nsim = 500) {   vc <- vcov(fit)   mc_sim <- MASS::mvrnorm(nsim, mu = coef(fit), Sigma = vc)   mc_rel <- apply(mc_sim, MARGIN = 1,                   FUN = function(x) {                     all_rel(x[1:6], th = diag(x[7:12]), vc = vc)                   })   2 * with(     lavInspect(fit, what = \"est\"),     all_rel(lambda, th = theta, vc = vc)   ) - rowMeans(mc_rel) # bias-corrected }  for (i in seq_len(num_sims)) {   eta <- rnorm(num_obs)   err <- matrix(     rnorm(num_obs * length(lambda), sd = sqrt(theta)),     ncol = num_obs   )   y <- t(     tcrossprod(lambda, eta) + err   )   # Run cfa   fit <- cfa(\"f =~ y1 + y2 + y3 + y4 + y5 + y6\",     data = data.frame(y) |> setNames(paste0(\"y\", seq_along(lambda))),     std.lv = TRUE   )   pars_fit <- lavInspect(fit, what = \"est\")   tilde_eta <- lavPredict(fit, fsm = TRUE)   true_rel <- cor(tilde_eta, eta)^2   est_a <- attr(tilde_eta, \"fsm\")[[1]]   out[i, ] <- c(     true_rel,     all_rel(pars_fit$lambda, pars_fit$theta, vcov(fit)),     all_bc_rel(fit)   )   setTxtProgressBar(     txtProgressBar(min = 0, max = num_sims, style = 3, width = 50, char = \"=\"),      i   ) } colnames(out) <- c(\"true_rel\", \"no_adj_rel\", \"adj_rel\", \"quadratic\",                    \"no_adj_rel_bc\", \"adj_rel_bc\", \"quadratic_bc\")  # save the file saveRDS(out, \"vignettes/sim_results_reliability.RDS\") # Mean bias out <- readRDS(\"sim_results_reliability.RDS\") data.frame(   Type = c(\"no_adj_rel\", \"adj_rel\", \"quadratic\",            \"no_adj_rel_bc\", \"adj_rel_bc\", \"quadratic_bc\"),   Raw_Biases = c(mean(out[, \"no_adj_rel\"] - out[, \"true_rel\"]),                   mean(out[, \"adj_rel\"] - out[, \"true_rel\"]),                   mean(out[, \"quadratic\"] - out[, \"true_rel\"]),                   mean(out[, \"no_adj_rel_bc\"] - out[, \"true_rel\"]),                   mean(out[, \"adj_rel_bc\"] - out[, \"true_rel\"]),                   mean(out[, \"quadratic_bc\"] - out[, \"true_rel\"], na.rm = TRUE)) ) |>   knitr::kable(digits = 3) # RMSE:  data.frame(   Type = c(\"no_adj_rel\", \"adj_rel\", \"quadratic\",            \"no_adj_rel_bc\", \"adj_rel_bc\", \"quadratic_bc\"),   RMSE = c(sqrt(mean((out[, \"no_adj_rel\"] - out[, \"true_rel\"])^2)),            sqrt(mean((out[, \"adj_rel\"] - out[, \"true_rel\"])^2)),            sqrt(mean((out[, \"quadratic\"] - out[, \"true_rel\"])^2)),            sqrt(mean((out[, \"no_adj_rel_bc\"] - out[, \"true_rel\"])^2)),            sqrt(mean((out[, \"adj_rel_bc\"] - out[, \"true_rel\"])^2)),            sqrt(mean((out[, \"quadratic_bc\"] - out[, \"true_rel\"])^2,                       na.rm = TRUE))) ) |>   knitr::kable(digits = 3) # Boxplot to compare the bias across formula bias_tab <- as.data.frame(out[, -1] - out[, 1]) |>   pivot_longer(everything(), names_to = \"formula\", values_to = \"bias\") bias_tab |>   ggplot(aes(x = formula, y = bias, col = formula)) +   geom_boxplot() +   geom_hline(yintercept = 0) +   theme(legend.position = \"none\",         axis.text.x = element_text(angle = 45, hjust = 1)) ## Warning: Removed 6 rows containing non-finite outside the scale range ## (`stat_boxplot()`)."},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-growth-vignette.html","id":"joint-structural-equation-model-latent-growth-with-strict-invariance-model","dir":"Articles","previous_headings":"","what":"Joint structural equation model: Latent growth with strict invariance model","title":"Linear Growth Modeling with Two-Stage Path Analysis","text":"tutorial, authors first performed longitudinal invariance testing found support strict invariance. moved fit latent growth model based strict invariance model, shown .","code":"jsem_growth_mod <- \" # factor loadings (with constraints) eta1 =~ 15.1749088 * s_g3 + l2 * r_g3 +  l3 * m_g3 eta2 =~ 15.1749088 * s_g5 + l2 * r_g5 +  l3 * m_g5 eta3 =~ 15.1749088 * s_g8 + l2 * r_g8 +  l3 * m_g8  # factor variances eta1 ~~ psi * eta1 eta2 ~~ psi * eta2 eta3 ~~ psi * eta3  # unique variances/covariances  s_g3 ~~ u1 * s_g3 + s_g5 + s_g8 s_g5 ~~ u1 * s_g5 + s_g8 s_g8 ~~ u1 * s_g8 r_g3 ~~ u2 * r_g3 + r_g5 + r_g8 r_g5 ~~ u2 * r_g5 + r_g8 r_g8 ~~ u2 * r_g8 m_g3 ~~ u3 * m_g3 + m_g5 + m_g8 m_g5 ~~ u3 * m_g5 + m_g8 m_g8 ~~ u3 * m_g8  # observed variable intercepts s_g3 ~ i1 * 1 s_g5 ~ i1 * 1 s_g8 ~ i1 * 1 r_g3 ~ i2 * 1 r_g5 ~ i2 * 1 r_g8 ~ i2 * 1 m_g3 ~ i3 * 1 m_g5 ~ i3 * 1 m_g8 ~ i3 * 1  # latent basis model i =~ 1 * eta1 + 1 * eta2 + 1 * eta3 s =~ 0 * eta1 + start(.5) * eta2 + 1 * eta3  i ~~ start(.8) * i s ~~ start(.5) * s i ~~ start(0) * s  i ~ 0 * 1 s ~ 1 \" jsem_growth_fit <- lavaan(jsem_growth_mod,                           data = eclsk,                           meanstructure = TRUE,                           estimator = \"ML\",                           fixed.x = FALSE)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-growth-vignette.html","id":"s-pa-latent-growth-with-strict-invariance-model","dir":"Articles","previous_headings":"","what":"2S-PA: Latent growth with strict invariance model","title":"Linear Growth Modeling with Two-Stage Path Analysis","text":"2S-PA, first specify strict invariance model, directly adopted tutorial. Based strict invariance model, obtain factor scores. second stage, use factor scores obtained strict invariance model model growth trajectory across time points. growth model “latent basis model” joint structural equation model.","code":"strict_mod <- \" # factor loadings eta1 =~ 15.1749088 * s_g3 + l2 * r_g3 + l3 * m_g3 eta2 =~ 15.1749088 * s_g5 + l2 * r_g5 + l3 * m_g5 eta3 =~ 15.1749088 * s_g8 + l2 * r_g8 + l3 * m_g8  # factor variances/covariances eta1 ~~ 1 * eta1 + eta2 + eta3 eta2 ~~ eta2 + eta3 eta3 ~~ eta3  # unique variances/covariances s_g3 ~~ u1 * s_g3 + s_g5 + s_g8 s_g5 ~~ u1 * s_g5 + s_g8 s_g8 ~~ u1 * s_g8 r_g3 ~~ u2 * r_g3 + r_g5 + r_g8 r_g5 ~~ u2 * r_g5 + r_g8 r_g8 ~~ u2 * r_g8 m_g3 ~~ u3 * m_g3 + m_g5 + m_g8 m_g5 ~~ u3 * m_g5 + m_g8 m_g8 ~~ u3 * m_g8  # latent variable intercepts eta1 ~ 0 * 1 eta2 ~ 1 eta3 ~ 1  # observed variable intercepts s_g3 ~ i1 * 1 s_g5 ~ i1 * 1 s_g8 ~ i1 * 1 r_g3 ~ i2 * 1 r_g5 ~ i2 * 1 r_g8 ~ i2 * 1 m_g3 ~ i3 * 1 m_g5 ~ i3 * 1 m_g8 ~ i3 * 1 \" # Get factor scores fs_dat <- get_fs(eclsk, model = strict_mod) # Growth model tspa_growth_mod <- \" i =~ 1 * eta1 + 1 * eta2 + 1 * eta3 s =~ 0 * eta1 + start(.5) * eta2 + 1 * eta3  # factor variances eta1 ~~ psi * eta1 eta2 ~~ psi * eta2 eta3 ~~ psi * eta3  i ~~ start(.8) * i s ~~ start(.5) * s i ~~ start(0) * s  i ~ 1 s ~ 1 \" # Fit the growth model tspa_growth_fit <- tspa(tspa_growth_mod, fs_dat,                         fsT = attr(fs_dat, \"fsT\"),                         fsL = attr(fs_dat, \"fsL\"),                         fsb = attr(fs_dat, \"fsb\"),                         estimator = \"ML\")"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-growth-vignette.html","id":"comparison","dir":"Articles","previous_headings":"","what":"Comparison","title":"Linear Growth Modeling with Two-Stage Path Analysis","text":"joint structural equation model 2S-PA yielded comparable estimates mean variances intercept slope factors.","code":"parameterestimates(tspa_growth_fit) |>   subset(subset = lhs %in% c(\"i\", \"s\") & op %in% c(\"~1\", \"~~\")) #>    lhs op rhs label    est    se       z pvalue ci.lower ci.upper #> 28   i ~~   i        0.944 0.051  18.592  0.000    0.845    1.044 #> 29   s ~~   s        0.118 0.016   7.298  0.000    0.086    0.149 #> 30   i ~~   s       -0.076 0.020  -3.757  0.000   -0.116   -0.036 #> 31   i ~1            0.001 0.035   0.016  0.987   -0.068    0.069 #> 32   s ~1            1.993 0.019 107.413  0.000    1.957    2.030 parameterestimates(jsem_growth_fit) |>   subset(subset = lhs %in% c(\"i\", \"s\") & op  %in% c(\"~1\", \"~~\")) #>    lhs op rhs label    est    se      z pvalue ci.lower ci.upper #> 46   i ~~   i        0.941 0.052 18.138      0    0.839    1.042 #> 47   s ~~   s        0.117 0.017  6.826      0    0.083    0.150 #> 48   i ~~   s       -0.074 0.020 -3.646      0   -0.115   -0.034 #> 49   i ~1            0.000 0.000     NA     NA    0.000    0.000 #> 50   s ~1            1.991 0.024 84.586      0    1.945    2.038"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-plot-vignette.html","id":"single-group-single-predictor","dir":"Articles","previous_headings":"","what":"Single group, single predictor","title":"Diagnostic plots for `tspa` models","text":"recommend researchers examine diagnostic plot factor scores model. can use function tspa_plot() obtain two plots: () scatter plot factors, (b) residual plot factors. Features tspa_plot(): function able pass arguments base R plot() function. Users can define title scatter plot label names axis. Abbreviation argument allows users choose whether using abbreviated group names. Users can choose whether generating plot one one hitting  keyboard.","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4    # regressions     dem60 ~ ind60 '  fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60)  tspa_fit_1 <- tspa(model = \"dem60 ~ ind60\",                     data = fs_dat,                     se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472),                    meanstructure = T) # par(mar = c(2,2,3,2)) tspa_plot(tspa_fit_1,           col = \"blue\",           cex.lab = 1.2,           cex.axis = 1,           fscores_type = \"original\",           ask = TRUE)"},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-plot-vignette.html","id":"single-group-multiple-predictors","dir":"Articles","previous_headings":"","what":"Single group, multiple predictors","title":"Diagnostic plots for `tspa` models","text":"","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # # residual correlations   #   y1 ~~ y5   #   y2 ~~ y4 + y6   #   y3 ~~ y7   #   y4 ~~ y8   #   y6 ~~ y8 '  fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                         model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                         model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat_dem65 <- get_fs(data = PoliticalDemocracy,                         model = \"dem65 =~ y5 + y6 + y7 + y8\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60, fs_dat_dem65)  tspa_fit_2 <- tspa(model = \"dem60 ~ ind60                             dem65 ~ ind60 + dem60\",                     data = fs_dat,                     se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472,                                  dem65 = 0.5724405)) # Title, xlab, and ylab each with same names. tspa_plot(tspa_fit_2,           ps = 10,           col = \"blue\",           cex.lab = 1.2,           cex.axis = 1) # Title, xlab, and ylab each with separate names. tspa_plot(tspa_fit_2,           ps = 10,           col = \"darkgray\",           cex.lab = 1.2,           cex.axis = 1,           title = c(\"Scatterplot_I\", \"Scatterplot_II\", \"Scatterplot_III\"),           label_x = c(\"factor_1\", \"factor_2\", \"factor_3\"),           label_y = c(\"factor_a\", \"factor_b\", \"factor_c\"))"},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-plot-vignette.html","id":"multigroup-single-predictor","dir":"Articles","previous_headings":"","what":"Multigroup, single predictor","title":"Diagnostic plots for `tspa` models","text":"","code":"model <- '    # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9    # regressions     visual ~ speed '  # get factor scores fs_dat_visual <- get_fs(model = \"visual =~ x1 + x2 + x3\",                         data = HolzingerSwineford1939,                          group = \"school\") fs_dat_speed <- get_fs(model = \"speed =~ x7 + x8 + x9\",                         data = HolzingerSwineford1939,                         group = \"school\") fs_hs <- cbind(do.call(rbind, fs_dat_visual),                do.call(rbind, fs_dat_speed))  tspa_fit_3 <- tspa(model = \"visual ~ speed\",                    data = fs_hs,                    se_fs = data.frame(visual = c(0.3391326, 0.311828),                                       speed = c(0.2786875, 0.2740507)),                    group = \"school\"                    # group.equal = \"regressions\"                    ) tspa_plot(tspa_fit_3,           ps = 10,           col = \"darkgray\",           cex.lab = 1.2,           ask = FALSE,           cex.axis = 1) tspa_plot(tspa_fit_3,           ps = 10,           col = \"darkgray\",           cex.lab = 1.2,           cex.axis = 1,           ask = TRUE,           abbreviation = FALSE)"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"with-a-linear-factor-model","dir":"Articles","previous_headings":"","what":"With a Linear Factor Model","title":"Multi-Factor Measurement Model (OpenMx)","text":"example https://lavaan.ugent./tutorial/sem.html.","code":"# CFA my_cfa <- \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4 \" (fs_dat <- get_fs(PoliticalDemocracy, model = my_cfa, std.lv = TRUE)) |> head() #>     fs_ind60   fs_dem60 fs_ind60_se fs_dem60_se ind60_by_fs_ind60 #> 1 -0.8101568 -1.3119114   0.1859987   0.3012901         0.9553858 #> 2  0.1888466 -1.3644831   0.1859987   0.3012901         0.9553858 #> 3  1.0960931  1.3107705   0.1859987   0.3012901         0.9553858 #> 4  1.8702043  1.4849083   0.1859987   0.3012901         0.9553858 #> 5  1.2446060  0.9193277   0.1859987   0.3012901         0.9553858 #> 6  0.3348621  0.4886331   0.1859987   0.3012901         0.9553858 #>   ind60_by_fs_dem60 dem60_by_fs_ind60 dem60_by_fs_dem60 ev_fs_ind60 #> 1        0.05816994        0.01834111         0.8688889  0.03459552 #> 2        0.05816994        0.01834111         0.8688889  0.03459552 #> 3        0.05816994        0.01834111         0.8688889  0.03459552 #> 4        0.05816994        0.01834111         0.8688889  0.03459552 #> 5        0.05816994        0.01834111         0.8688889  0.03459552 #> 6        0.05816994        0.01834111         0.8688889  0.03459552 #>   ecov_fs_dem60_fs_ind60 ev_fs_dem60 #> 1            0.004017388  0.09077571 #> 2            0.004017388  0.09077571 #> 3            0.004017388  0.09077571 #> 4            0.004017388  0.09077571 #> 5            0.004017388  0.09077571 #> 6            0.004017388  0.09077571 tspa_fit <- tspa(model = \"dem60 ~ ind60\", data = fs_dat,                  fsT = attr(fs_dat, \"fsT\"),                  fsL = attr(fs_dat, \"fsL\")) parameterestimates(tspa_fit) #>         lhs op      rhs   est    se     z pvalue ci.lower ci.upper #> 1     ind60 =~ fs_ind60 0.955 0.000    NA     NA    0.955    0.955 #> 2     ind60 =~ fs_dem60 0.058 0.000    NA     NA    0.058    0.058 #> 3     dem60 =~ fs_ind60 0.018 0.000    NA     NA    0.018    0.018 #> 4     dem60 =~ fs_dem60 0.869 0.000    NA     NA    0.869    0.869 #> 5  fs_ind60 ~~ fs_ind60 0.035 0.000    NA     NA    0.035    0.035 #> 6  fs_ind60 ~~ fs_dem60 0.004 0.000    NA     NA    0.004    0.004 #> 7  fs_dem60 ~~ fs_dem60 0.091 0.000    NA     NA    0.091    0.091 #> 8     dem60  ~    ind60 0.460 0.113 4.089      0    0.240    0.681 #> 9     ind60 ~~    ind60 1.000 0.169 5.900      0    0.668    1.332 #> 10    dem60 ~~    dem60 0.788 0.150 5.267      0    0.495    1.081"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"using-openmx","dir":"Articles","previous_headings":"With a Linear Factor Model","what":"Using OpenMx","title":"Multi-Factor Measurement Model (OpenMx)","text":"Create OpenMx model without latent variables. also use umx package. Create loading error covariance matrices (need reordering) Run 2S-PA","code":"fsreg_umx <- umxLav2RAM(     \"     dem60 ~ ind60     dem60 + ind60 ~ 1     \",     printTab = FALSE) #>  #> ?plot.MxModel options: std, means, digits, strip_zero, file, splines=T/F/ortho,..., min=, max =, same = , fixed, resid= 'circle|line|none' # Loading matL <- mxMatrix(     type = \"Full\", nrow = 2, ncol = 2,     free = FALSE,     values = attr(fs_dat, \"fsL\")[2:1, 2:1],     name = \"L\" ) # Error matE <- mxMatrix(     type = \"Symm\", nrow = 2, ncol = 2,     free = FALSE,     values = attr(fs_dat, \"fsT\")[2:1, 2:1],     name = \"E\" ) tspa_mx <- tspa_mx_model(fsreg_umx, data = fs_dat,                          mat_ld = matL, mat_ev = matE,                          fs_lv_names = c(ind60 = \"fs_ind60\",                                          dem60 = \"fs_dem60\")) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx) #> Running 2SPAD with 5 parameters # Summarize the results (takes 20 seconds or so) summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>               name matrix   row   col      Estimate Std.Error A #> 1   ind60_to_dem60   m1.A dem60 ind60  4.604652e-01 0.1126087   #> 2 dem60_with_dem60   m1.S dem60 dem60  7.879711e-01 0.1495940   #> 3 ind60_with_ind60   m1.S ind60 ind60  1.000000e+00 0.1694829   #> 4     one_to_dem60   m1.M     1 dem60  1.083067e-07 0.1105181   #> 5     one_to_ind60   m1.M     1 ind60 -3.096769e-08 0.1176358   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                    295              393.7496 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 75/300 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -196.2504               403.7496                 404.6192 #> BIC:      -879.9094               415.3370                 399.5784 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-09-06 04:19:54  #> Wall clock time: 0.05677581 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.12  #> Need help?  See help(mxSummary) # Standardize coefficients mxStandardizeRAMpaths(tspa_mx_fit, SE = TRUE)$m1[1, ] #>        name          label matrix   row   col Raw.Value    Raw.SE Std.Value #> 1 m1.A[1,2] ind60_to_dem60      A dem60 ind60 0.4604652 0.1126087 0.4604654 #>      Std.SE #> 1 0.1001049 # Alternative specification tspa_mx <- tspa_mx_model(fsreg_umx, data = fs_dat,                          mat_ld = attr(fs_dat, \"fsL\"),                          mat_ev = attr(fs_dat, \"fsT\"))"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"with-mean-structure","dir":"Articles","previous_headings":"With a Linear Factor Model > Using OpenMx","what":"With Mean Structure","title":"Multi-Factor Measurement Model (OpenMx)","text":"lavaan OpenMx","code":"my_cfa <- \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     x1 + y1 ~ 0     ind60 + dem60 ~ 1 \" fs_dat <- get_fs(PoliticalDemocracy, model = my_cfa,                  meanstructure = TRUE) tspa_fit <- tspa(model = \"dem60 ~ ind60\\ndem60 + ind60 ~ 1\", data = fs_dat,                  fsT = attr(fs_dat, \"fsT\"),                  fsL = attr(fs_dat, \"fsL\"),                  fsb = attr(fs_dat, \"fsb\")) parameterEstimates(tspa_fit) #>         lhs op      rhs    est    se      z pvalue ci.lower ci.upper #> 1     ind60 =~ fs_ind60  0.955 0.000     NA     NA    0.955    0.955 #> 2     ind60 =~ fs_dem60  0.182 0.000     NA     NA    0.182    0.182 #> 3     dem60 =~ fs_ind60  0.006 0.000     NA     NA    0.006    0.006 #> 4     dem60 =~ fs_dem60  0.869 0.000     NA     NA    0.869    0.869 #> 5  fs_ind60 ~~ fs_ind60  0.016 0.000     NA     NA    0.016    0.016 #> 6  fs_ind60 ~~ fs_dem60  0.006 0.000     NA     NA    0.006    0.006 #> 7  fs_dem60 ~~ fs_dem60  0.398 0.000     NA     NA    0.398    0.398 #> 8  fs_ind60 ~1           0.193 0.000     NA     NA    0.193    0.193 #> 9  fs_dem60 ~1          -0.203 0.000     NA     NA   -0.203   -0.203 #> 10    dem60  ~    ind60  1.439 0.352  4.089  0.000    0.749    2.129 #> 11    dem60 ~1          -1.810 1.794 -1.009  0.313   -5.327    1.706 #> 12    ind60 ~1           5.054 0.079 64.155  0.000    4.900    5.209 #> 13    ind60 ~~    ind60  0.449 0.076  5.900  0.000    0.300    0.598 #> 14    dem60 ~~    dem60  3.453 0.656  5.267  0.000    2.168    4.738 standardizedSolution(tspa_fit) #>         lhs op      rhs est.std    se       z pvalue ci.lower ci.upper #> 1     ind60 =~ fs_ind60   0.973 0.004 267.474   0.00    0.966    0.980 #> 2     ind60 =~ fs_dem60   0.061 0.006   9.671   0.00    0.049    0.074 #> 3     dem60 =~ fs_ind60   0.019 0.002   9.092   0.00    0.015    0.023 #> 4     dem60 =~ fs_dem60   0.918 0.011  83.338   0.00    0.897    0.940 #> 5  fs_ind60 ~~ fs_ind60   0.036 0.006   6.124   0.00    0.024    0.047 #> 6  fs_ind60 ~~ fs_dem60   0.072 0.000      NA     NA    0.072    0.072 #> 7  fs_dem60 ~~ fs_dem60   0.101 0.017   6.124   0.00    0.069    0.134 #> 8  fs_ind60 ~1            0.294 0.024  12.247   0.00    0.247    0.341 #> 9  fs_dem60 ~1           -0.102 0.008 -12.247   0.00   -0.119   -0.086 #> 10    dem60  ~    ind60   0.460 0.100   4.600   0.00    0.264    0.657 #> 11    dem60 ~1           -0.865 0.817  -1.058   0.29   -2.467    0.737 #> 12    ind60 ~1            7.547 0.650  11.606   0.00    6.272    8.821 #> 13    ind60 ~~    ind60   1.000 0.000      NA     NA    1.000    1.000 #> 14    dem60 ~~    dem60   0.788 0.092   8.547   0.00    0.607    0.969 # Force symmetric E matE <- attr(fs_dat, \"fsT\") matE <- (matE + t(matE)) / 2 matb <- t(as.matrix(attr(fs_dat, \"fsb\"))) tspa_mx <- tspa_mx_model(fsreg_umx, data = fs_dat,                          mat_ld = attr(fs_dat, \"fsL\"),                          mat_ev = matE,                          mat_int = matb) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx) #> Running 2SPAD with 5 parameters # Summarize the results (takes 20 seconds or so) summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>               name matrix   row   col   Estimate  Std.Error A #> 1   ind60_to_dem60   m1.A dem60 ind60  1.4393156 0.35192047   #> 2 dem60_with_dem60   m1.S dem60 dem60  3.4532716 0.65558270   #> 3 ind60_with_ind60   m1.S ind60 ind60  0.4485416 0.07601934   #> 4     one_to_dem60   m1.M     1 dem60 -1.8101866 1.79371569   #> 5     one_to_ind60   m1.M     1 ind60  5.0543839 0.07878429   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                    295              444.4392 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 75/300 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -145.5608               454.4392                 455.3088 #> BIC:      -829.2198               466.0266                 450.2680 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-09-06 04:19:55  #> Wall clock time: 0.04486251 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.12  #> Need help?  See help(mxSummary) # Standardize coefficients mxStandardizeRAMpaths(tspa_mx_fit, SE = TRUE)$m1[1, ] #>        name          label matrix   row   col Raw.Value    Raw.SE Std.Value #> 1 m1.A[1,2] ind60_to_dem60      A dem60 ind60  1.439316 0.3519205 0.4604657 #>      Std.SE #> 1 0.1000874"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"compare-to-joint-model-with-openmx","dir":"Articles","previous_headings":"With a Linear Factor Model","what":"Compare to joint model with OpenMx","title":"Multi-Factor Measurement Model (OpenMx)","text":"","code":"jreg_umx_fit <- umxRAM(     \"     # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     # latent regression     dem60 ~ ind60     \",     data = PoliticalDemocracy) #> 2 latent variables were created:ind60, dem60. #> You have raw data, but no means model. I added #> mxPath('one', to = manifestVars) #> Running latent_variables with 22 parameters #> ?umxSummary options: std=T|F', digits=, report= 'html', filter= 'NS' & more #> Running Saturated latent_variables with 35 parameters #> Running Independence latent_variables with 14 parameters #>  #>  #> Table: Parameter loadings for model 'latent_variables' #>  #> |   |name             | Estimate|SE    |type             | #> |:--|:----------------|--------:|:-----|:----------------| #> |1  |ind60_to_x1      |    1.000|0     |Factor loading   | #> |2  |ind60_to_x2      |    2.180|0.14  |Factor loading   | #> |3  |ind60_to_x3      |    1.818|0.152 |Factor loading   | #> |5  |dem60_to_y1      |    1.000|0     |Factor loading   | #> |6  |dem60_to_y2      |    1.419|0.212 |Factor loading   | #> |7  |dem60_to_y3      |    1.095|0.167 |Factor loading   | #> |8  |dem60_to_y4      |    1.428|0.191 |Factor loading   | #> |4  |ind60_to_dem60   |    1.439|0.377 |Factor to factor | #> |16 |ind60_with_ind60 |    0.449|0.087 |Factor Variance  | #> |17 |dem60_with_dem60 |    3.453|0.917 |Factor Variance  | #> |18 |one_to_y1        |    5.465|0.301 |Mean             | #> |19 |one_to_y2        |    4.256|0.453 |Mean             | #> |20 |one_to_y3        |    6.563|0.376 |Mean             | #> |21 |one_to_y4        |    4.453|0.384 |Mean             | #> |22 |one_to_x1        |    5.054|0.084 |Mean             | #> |23 |one_to_x2        |    4.792|0.173 |Mean             | #> |24 |one_to_x3        |    3.558|0.161 |Mean             | #> |9  |y1_with_y1       |    2.404|0.586 |Residual         | #> |10 |y2_with_y2       |    6.552|1.286 |Residual         | #> |11 |y3_with_y3       |    5.363|1.053 |Residual         | #> |12 |y4_with_y4       |    2.137|0.822 |Residual         | #> |13 |x1_with_x1       |    0.081|0.02  |Residual         | #> |14 |x2_with_x2       |    0.120|0.072 |Residual         | #> |15 |x3_with_x3       |    0.467|0.089 |Residual         | #>  #> Model Fit: χ²(13) = 23.74, p = 0.034; CFI = 0.972; TLI = 0.955; RMSEA = 0.105 #> RMSEA is worse than desired (<.06) mxStandardizeRAMpaths(jreg_umx_fit, SE = TRUE)[4, ] #>                      name          label matrix   row   col Raw.Value    Raw.SE #> 4 latent_variables.A[9,8] ind60_to_dem60      A dem60 ind60  1.439314 0.3769464 #>   Std.Value    Std.SE #> 4 0.4604654 0.1010676"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"combined-with-irt","dir":"Articles","previous_headings":"","what":"Combined with IRT","title":"Multi-Factor Measurement Model (OpenMx)","text":"Example Lai & Hsiao (2021, Psychological Methods)","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"not-accounting-for-error","dir":"Articles","previous_headings":"Combined with IRT","what":"Not accounting for error","title":"Multi-Factor Measurement Model (OpenMx)","text":"","code":"# Simulate data with mirt set.seed(1235) num_obs <- 1000 # Simulate theta eta <- MASS::mvrnorm(num_obs, mu = c(0, 0), Sigma = diag(c(1, 1 - 0.5^2)),                      empirical = TRUE) th1 <- eta[, 1] th2 <- -1 + 0.5 * th1 + eta[, 2] # items and response data a1 <- matrix(1, 10) d1 <- matrix(rnorm(10)) a2 <- matrix(runif(10, min = 0.5, max = 1.5)) d2 <- matrix(rnorm(10)) dat1 <- simdata(a = a1, d = d1, N = num_obs, itemtype = \"2PL\", Theta = th1) dat2 <- simdata(a = a2, d = d2, N = num_obs, itemtype = \"2PL\", Theta = th2) # Factor scores mod1 <- mirt(dat1, model = 1, itemtype = \"Rasch\", verbose = FALSE) mod2 <- mirt(dat2, model = 1, itemtype = \"2PL\", verbose = FALSE) fs1 <- fscores(mod1, full.scores.SE = TRUE) fs2 <- fscores(mod2, full.scores.SE = TRUE) lm(fs2[, 1] ~ fs1[, 1])  # attenuated coefficient #>  #> Call: #> lm(formula = fs2[, 1] ~ fs1[, 1]) #>  #> Coefficients: #> (Intercept)     fs1[, 1]   #>  -3.730e-06    2.812e-01"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"joint-model","dir":"Articles","previous_headings":"Combined with IRT","what":"Joint model","title":"Multi-Factor Measurement Model (OpenMx)","text":"WLS","code":"dat <- cbind(dat1, dat2) colnames(dat) <- paste0(\"i\", 1:20) wls_fit <- sem(\" f1 =~ i1 + i2 + i3 + i4 + i5 + i6 + i7 + i8 + i9 + i10 f2 =~ i11 + i12 + i13 + i14 + i15 + i16 + i17 + i18 + i19 + i20 f2 ~ f1 \", data = dat, ordered = TRUE, std.lv = TRUE) coef(wls_fit)[\"f2~f1\"] #>     f2~f1  #> 0.5490816"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"with-maximum-likelihood","dir":"Articles","previous_headings":"Combined with IRT","what":"With Maximum Likelihood","title":"Multi-Factor Measurement Model (OpenMx)","text":"Note: extremely computational intensive","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"using-tspa_mx_model","dir":"Articles","previous_headings":"Combined with IRT","what":"Using tspa_mx_model()","title":"Multi-Factor Measurement Model (OpenMx)","text":"EAP (shrinkage) scores used, use following properties scores (η̃\\tilde \\eta_i person ii): Standard error (SE, software output) = V(η)(1−ρη̃,)\\sqrt{V(\\eta) (1 - \\rho_{\\tilde \\eta, })}, ρη̃,\\rho_{\\tilde \\eta, } reliability η̃\\tilde \\eta_i Loading η̃\\tilde \\eta_i η\\eta equal ρη̃,\\rho_{\\tilde \\eta, } = 1−SE2/V(η)1 - \\text{SE}^2 / V(\\eta) Standard error measurement η̃\\tilde \\eta_i equal ρη̃,iSE\\rho_{\\tilde \\eta, } \\text{SE} Based , total variance η̃\\tilde \\eta_i also ρη̃,\\rho_{\\tilde \\eta, }. Alternatively, one can use named matrices (mat_ld mat_ev) specify names columns loading error covariance matrices. Compare joint model OpenMx Note: FIML couldn’t finish within 12 hours","code":"# Combine into data set fs_dat <- cbind(fs1, fs2) |>     as.data.frame() |>     setNames(c(\"fs1\", \"se_fs1\", \"fs2\", \"se_fs2\")) |>     # Compute reliability and error variances     within(expr = {         rel_fs1 <- 1 - se_fs1^2         rel_fs2 <- 1 - se_fs2^2         ev_fs1 <- se_fs1^2 * (1 - se_fs1^2)         ev_fs2 <- se_fs2^2 * (1 - se_fs2^2)     }) # Loading matL <- mxMatrix(     type = \"Diag\", nrow = 2, ncol = 2,     free = FALSE,     labels = c(\"data.rel_fs2\", \"data.rel_fs1\"),     name = \"L\" ) # Error matE <- mxMatrix(     type = \"Diag\", nrow = 2, ncol = 2,     free = FALSE,     labels = c(\"data.ev_fs2\", \"data.ev_fs1\"),     name = \"E\" ) fsreg_umx <- umxLav2RAM(     \"       fs2 ~ fs1       fs2 + fs1 ~ 1     \",     printTab = FALSE) #>  #> ?plot.MxModel options: std, means, digits, strip_zero, file, splines=T/F/ortho,..., min=, max =, same = , fixed, resid= 'circle|line|none' tspa_mx <- tspa_mx_model(fsreg_umx, data = fs_dat,                          mat_ld = matL, mat_ev = matE) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx) #> Running 2SPAD with 5 parameters # Summarize the results summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>           name matrix row col     Estimate  Std.Error A #> 1   fs1_to_fs2   m1.A fs2 fs1  0.500417224 0.05514849   #> 2 fs2_with_fs2   m1.S fs2 fs2  0.778233329 0.07526000   #> 3 fs1_with_fs1   m1.S fs1 fs1  0.905084857 0.06795953   #> 4   one_to_fs2   m1.M   1 fs2 -0.000463287 0.04094454   #> 5   one_to_fs1   m1.M   1 fs1 -0.001261722 0.03807646   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                   3995              4644.151 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 1000/4000 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -3345.849               4654.151                 4654.212 #> BIC:     -22952.331               4678.690                 4662.810 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-09-06 04:19:59  #> Wall clock time: 0.7556674 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.12  #> Need help?  See help(mxSummary) cross_load <- matrix(c(\"rel_fs2\", NA, NA, \"rel_fs1\"), nrow = 2) |>     `dimnames<-`(rep(list(c(\"fs2\", \"fs1\")), 2)) err_cov <- matrix(c(\"ev_fs2\", NA, NA, \"ev_fs1\"), nrow = 2) |>     `dimnames<-`(rep(list(c(\"fs2\", \"fs1\")), 2)) tspa_mx <- tspa_mx_model(fsreg_umx, data = fs_dat,                          mat_ld = cross_load, mat_ev = err_cov) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx) #> Running 2SPAD with 5 parameters # Summarize the results summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>           name matrix row col     Estimate  Std.Error A #> 1   fs1_to_fs2   m1.A fs2 fs1  0.500417224 0.05514849   #> 2 fs2_with_fs2   m1.S fs2 fs2  0.778233329 0.07526000   #> 3 fs1_with_fs1   m1.S fs1 fs1  0.905084857 0.06795953   #> 4   one_to_fs2   m1.M   1 fs2 -0.000463287 0.04094454   #> 5   one_to_fs1   m1.M   1 fs1 -0.001261722 0.03807646   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                   3995              4644.151 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 1000/4000 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -3345.849               4654.151                 4654.212 #> BIC:     -22952.331               4678.690                 4662.810 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-09-06 04:20:00  #> Wall clock time: 0.7746925 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.12  #> Need help?  See help(mxSummary) jreg_umx_fit <- umxRAM(     \"     f1 =~ a * i1 + a * i2 + a * i3 + a * i4 + a * i5 +           a * i6 + a * i7 + a * i8 + a* i9 + a * i10     f2 =~ i11 + i12 + i13 + i14 + i15 + i16 + i17 + i18 + i19 + i20     f2 ~ f1     i1 + i2 + i3 + i4 + i5 + i6 + i7 + i8 + i9 + i10 ~ 0     i11 + i12 + i13 + i14 + i15 + i16 + i17 + i18 + i19 + i20 ~ 0     i1 ~~ 1 * i1     i2 ~~ 1 * i2     i3 ~~ 1 * i3     i4 ~~ 1 * i4     i5 ~~ 1 * i5     i6 ~~ 1 * i6     i7 ~~ 1 * i7     i8 ~~ 1 * i8     i9 ~~ 1 * i9     i10 ~~ 1 * i10     i11 ~~ 1 * i11     i12 ~~ 1 * i12     i13 ~~ 1 * i13     i14 ~~ 1 * i14     i15 ~~ 1 * i15     i16 ~~ 1 * i16     i17 ~~ 1 * i17     i18 ~~ 1 * i18     i19 ~~ 1 * i19     i20 ~~ 1 * i20     \",     data = lapply(as.data.frame(dat), FUN = mxFactor,                  levels = c(0, 1)) |>                as.data.frame(),     type = \"DWLS\") #> 2 latent variables were created:f1, f2. #> object 'threshMat' created to handle: 20 binary variables:'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7', 'i8', 'i9', 'i10', 'i11', 'i12', 'i13', 'i14', 'i15', 'i16', 'i17', 'i18', 'i19', and 'i20' #> 20 trait(s) are binary: 'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7', 'i8', 'i9', 'i10', 'i11', 'i12', 'i13', 'i14', 'i15', 'i16', 'i17', 'i18', 'i19', and 'i20' #> For these, you you MUST fix the mean and variance of the latent traits driving each variable (usually 0 & 1 respectively) . #> See ?umxThresholdMatrix #> Using deviation-based model: Thresholds will be in'threshMat' based on deviations in 'deviations_for_thresh' #> Running m1 with 32 parameters #> ?umxSummary options: std=T|F', digits=, report= 'html', filter= 'NS' & more #>  #>  #> Table: Parameter loadings for model 'm1' #>  #> |   |name         | Estimate|SE    |type             | #> |:--|:------------|--------:|:-----|:----------------| #> |1  |a            |    1.000|0     |Factor loading   | #> |12 |f2_to_i11    |    1.000|0     |Factor loading   | #> |13 |f2_to_i12    |    0.784|0.254 |Factor loading   | #> |14 |f2_to_i13    |    1.805|0.4   |Factor loading   | #> |15 |f2_to_i14    |    1.268|0.277 |Factor loading   | #> |16 |f2_to_i15    |    1.935|0.42  |Factor loading   | #> |17 |f2_to_i16    |    2.137|0.453 |Factor loading   | #> |18 |f2_to_i17    |    1.279|0.289 |Factor loading   | #> |19 |f2_to_i18    |    0.824|0.198 |Factor loading   | #> |20 |f2_to_i19    |    0.730|0.194 |Factor loading   | #> |21 |f2_to_i20    |    0.941|0.238 |Factor loading   | #> |11 |f1_to_f2     |    0.335|0.068 |Factor to factor | #> |42 |f1_with_f1   |    0.346|0.026 |Factor Variance  | #> |43 |f2_with_f2   |    0.131|0.048 |Factor Variance  | #> |22 |i1_with_i1   |    1.000|0     |Residual         | #> |23 |i2_with_i2   |    1.000|0     |Residual         | #> |24 |i3_with_i3   |    1.000|0     |Residual         | #> |25 |i4_with_i4   |    1.000|0     |Residual         | #> |26 |i5_with_i5   |    1.000|0     |Residual         | #> |27 |i6_with_i6   |    1.000|0     |Residual         | #> |28 |i7_with_i7   |    1.000|0     |Residual         | #> |29 |i8_with_i8   |    1.000|0     |Residual         | #> |30 |i9_with_i9   |    1.000|0     |Residual         | #> |31 |i10_with_i10 |    1.000|0     |Residual         | #> |32 |i11_with_i11 |    1.000|0     |Residual         | #> |33 |i12_with_i12 |    1.000|0     |Residual         | #> |34 |i13_with_i13 |    1.000|0     |Residual         | #> |35 |i14_with_i14 |    1.000|0     |Residual         | #> |36 |i15_with_i15 |    1.000|0     |Residual         | #> |37 |i16_with_i16 |    1.000|0     |Residual         | #> |38 |i17_with_i17 |    1.000|0     |Residual         | #> |39 |i18_with_i18 |    1.000|0     |Residual         | #> |40 |i19_with_i19 |    1.000|0     |Residual         | #> |41 |i20_with_i20 |    1.000|0     |Residual         | #>  #> Model Fit: χ²(178) = 264.48, p < 0.001; CFI = NA; TLI = NA; RMSEA = 0.022 #> Algebra'threshMat': #>  #>  #> |     |    i1|     i2|     i3|    i4|     i5|    i6|     i7|     i8|    i9|   i10|  i11|   i12|   i13|    i14|   i15|   i16|   i17|    i18|    i19|   i20| #> |:----|-----:|------:|------:|-----:|------:|-----:|------:|------:|-----:|-----:|----:|-----:|-----:|------:|-----:|-----:|-----:|------:|------:|-----:| #> |th_1 | 0.193| -0.801| -0.231| 0.134| -1.027| 0.336| -1.159| -0.058| 0.469| 0.149| 1.13| 1.484| 0.643| -0.082| 0.953| 0.702| 0.689| -0.026| -0.065| 0.581| jreg_std <- mxStandardizeRAMpaths(jreg_umx_fit, SE = TRUE) jreg_std[jreg_std$label == \"f1_to_f2\", ] #>           name    label matrix row col Raw.Value     Raw.SE Std.Value #> 11 m1.A[22,21] f1_to_f2      A  f2  f1 0.3349439 0.06796137  0.478655 #>        Std.SE #> 11 0.04374742"},{"path":"https://gengrui-zhang.github.io/R2spa/articles/tspa-vignette-mx.html","id":"multidimensional-measurement-model","dir":"Articles","previous_headings":"Combined with IRT","what":"Multidimensional Measurement Model","title":"Multi-Factor Measurement Model (OpenMx)","text":"multidimensional model used, EAP scores, use following properties generalized unidimensional case. Specifically, let V(𝛈)V(\\boldsymbol \\eta) = 𝛙\\boldsymbol{\\psi}. Loading 𝛈̃\\tilde{\\boldsymbol \\eta}_i 𝛈\\boldsymbol \\eta 𝚲η̃,\\boldsymbol{\\Lambda}_{\\tilde \\eta, }, also reliability matrix (.e., squared covariance 𝛈̃\\tilde{\\boldsymbol \\eta}_i 𝛈\\boldsymbol \\eta). Error matrix (𝐄\\mathbf{E}, software output) = (𝐈−𝚲η̃,)𝛙(\\mathbf{} - \\boldsymbol{\\Lambda}_{\\tilde \\eta, })\\boldsymbol{\\psi}. Therefore, 𝚲η̃,=𝐈−𝐄𝛙−1\\boldsymbol{\\Lambda}_{\\tilde \\eta, } = \\mathbf{} - \\mathbf{E} \\boldsymbol{\\psi}^{-1}. Error variance η̃\\tilde \\eta_i 𝚲η̃,i𝐄\\boldsymbol{\\Lambda}_{\\tilde \\eta, } \\mathbf{E}.","code":"dat <- cbind(dat1, dat2) colnames(dat) <- paste0(\"Item_\", 1:20) # Multidimensional IRT mod <- \" F1 = 1-10 F2 = 11-20 COV = F1*F2 CONSTRAIN = (1-10, a1) \" mfit <- mirt(dat, model = mod, itemtype = \"2PL\", verbose = FALSE) # Factor scores fs <- fscores(mfit) # Error portion of factor scores fs_ecov <- fscores(mfit, return.acov = TRUE) # Latent variable covariance psi <- diag(2) psi[1, 2] <- psi[2, 1] <- coef(mfit)$GroupPars[1, \"COV_21\"] inv_psi <- solve(psi) load <- lapply(fs_ecov, FUN = function(err) {     diag(2) - err %*% inv_psi }) fs_err <- Map(`%*%`, load, fs_ecov) # Convert to data frame load_dat <- lapply(load, FUN = c) |>     do.call(what = rbind) |>     as.data.frame() |>     setNames(c(\"F1_by_fs1\", \"F1_by_fs2\", \"F2_by_fs1\", \"F2_by_fs2\")) fs_err_dat <- lapply(fs_err, FUN = `[`, c(1, 2, 4)) |>     do.call(what = rbind) |>     as.data.frame() |>     setNames(c(\"ev_fs1\", \"ecov_fs1_fs2\", \"ev_fs2\")) # Combine into data set fs_dat <- cbind(fs, load_dat, fs_err_dat) # Build OpenMx model fsreg_umx <- umxLav2RAM(     \"       F2 ~ F1       F2 + F1 ~ 1     \",     printTab = FALSE) #>  #> ?plot.MxModel options: std, means, digits, strip_zero, file, splines=T/F/ortho,..., min=, max =, same = , fixed, resid= 'circle|line|none' cross_load <- matrix(c(\"F1_by_fs1\", \"F1_by_fs2\",                        \"F2_by_fs1\", \"F2_by_fs2\"), nrow = 2) |>     `dimnames<-`(rep(list(c(\"F1\", \"F2\")), 2)) err_cov <- matrix(c(\"ev_fs1\", \"ecov_fs1_fs2\",                     \"ecov_fs1_fs2\", \"ev_fs2\"), nrow = 2) |>     `dimnames<-`(rep(list(c(\"F1\", \"F2\")), 2)) tspa_mx2 <- tspa_mx_model(fsreg_umx, data = fs_dat,                           mat_ld = cross_load, mat_ev = err_cov) # Run OpenMx tspa_mx2_fit <- mxRun(tspa_mx2) #> Running 2SPAD with 5 parameters # Summarize the results summary(tspa_mx2_fit) #> Summary of 2SPAD  #>   #> free parameters: #>         name matrix row col      Estimate  Std.Error A #> 1   F1_to_F2   m1.A  F2  F1  4.753222e-01 0.05207370   #> 2 F2_with_F2   m1.S  F2  F2  7.740976e-01 0.07360716   #> 3 F1_with_F1   m1.S  F1  F1  1.000064e+00 0.07398508   #> 4  one_to_F2   m1.M   1  F2  5.438812e-05 0.04081612   #> 5  one_to_F1   m1.M   1  F1 -1.312738e-06 0.03972658   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                   3995              4327.057 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 1000/4000 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -3662.943               4337.057                 4337.117 #> BIC:     -23269.425               4361.596                 4345.716 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-09-06 04:20:11  #> Wall clock time: 0.9092374 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.12  #> Need help?  See help(mxSummary) # Standardize coefficients mxStandardizeRAMpaths(tspa_mx2_fit, SE = TRUE)$m1[1, ] #> Warning in mxStandardizeRAMpaths(tspa_mx2_fit, SE = TRUE): 'model' (or one of #> its submodels) contains definition variables; interpret results of #> mxStandardizeRAMpaths() cautiously #>        name    label matrix row col Raw.Value    Raw.SE Std.Value     Std.SE #> 1 m1.A[1,2] F1_to_F2      A  F2  F1 0.4753222 0.0520737 0.4753271 0.04563049"},{"path":"https://gengrui-zhang.github.io/R2spa/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Mark Hok Chio Lai. Author, maintainer. Yixiao Li. Author. Winnie Wing-Yee Tse. Author. Gengrui Zhang Zhang. Author.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lai M, Li Y, Tse W, Zhang G (2024). R2spa: R package two-stage path analysis (2S-PA) adjust measurement errors. R package version 0.0.4, https://gengrui-zhang.github.io/R2spa/.","code":"@Manual{,   title = {R2spa: An R package for two-stage path analysis (2S-PA) to adjust for measurement errors},   author = {Mark Hok Chio Lai and Yixiao Li and Winnie Wing-Yee Tse and Gengrui Zhang Zhang},   year = {2024},   note = {R package version 0.0.4},   url = {https://gengrui-zhang.github.io/R2spa/}, }"},{"path":"https://gengrui-zhang.github.io/R2spa/index.html","id":"r2spa","dir":"","previous_headings":"","what":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","title":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","text":"R2spa free open-source R package performs two-stage path analysis (2S-PA). 2S-PA, researchers can perform path analysis first obtaining factor scores adjusting measurement errors using estimates observation-specific reliability standard error factor scores. viable alternative SEM, 2S-PA shown give equally-good estimates SEM relatively simple models large sample sizes, well give accurate parameter estimates, better control Type error rates, substantially less convergence problems complex models small sample sizes.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","text":"package still developmental stage can installed GitHub :","code":"# install.packages(\"remotes\") remotes::install_github(\"Gengrui-Zhang/R2spa\")"},{"path":"https://gengrui-zhang.github.io/R2spa/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"An R package for two-stage path analysis (2S-PA) to adjust for measurement errors","text":"package based upon work supported National Science Foundation Grant . 2141790.","code":"library(lavaan) library(R2spa)  # Joint model model <- '   # latent variable definitions     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4    # regression     dem60 ~ ind60 ' # 2S-PA # Stage 1: Get factor scores and standard errors for each latent construct fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                        model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                        model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60)  # get_fs() gives a dataframe with factor scores and standard errors head(fs_dat) #>     fs_ind60 fs_ind60_se ind60_by_fs_ind60 ev_fs_ind60   fs_dem60 fs_dem60_se #> 1 -0.5261683   0.1213615         0.9657673  0.01472862 -2.7487224   0.6756472 #> 2  0.1436527   0.1213615         0.9657673  0.01472862 -3.0360803   0.6756472 #> 3  0.7143559   0.1213615         0.9657673  0.01472862  2.6718589   0.6756472 #> 4  1.2399257   0.1213615         0.9657673  0.01472862  2.9936997   0.6756472 #> 5  0.8319080   0.1213615         0.9657673  0.01472862  1.9242932   0.6756472 #> 6  0.2123845   0.1213615         0.9657673  0.01472862  0.9922798   0.6756472 #>   dem60_by_fs_dem60 ev_fs_dem60 #> 1         0.8868049   0.4564991 #> 2         0.8868049   0.4564991 #> 3         0.8868049   0.4564991 #> 4         0.8868049   0.4564991 #> 5         0.8868049   0.4564991 #> 6         0.8868049   0.4564991 # Stage 2: Perform 2S-PA tspa_fit <- tspa(   model = \"dem60 ~ ind60\",   data = fs_dat,   se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472) ) parameterestimates(tspa_fit) #>        lhs op      rhs   est    se     z pvalue ci.lower ci.upper #> 1    ind60 =~ fs_ind60 1.000 0.000    NA     NA    1.000    1.000 #> 2    dem60 =~ fs_dem60 1.000 0.000    NA     NA    1.000    1.000 #> 3 fs_ind60 ~~ fs_ind60 0.015 0.000    NA     NA    0.015    0.015 #> 4 fs_dem60 ~~ fs_dem60 0.456 0.000    NA     NA    0.456    0.456 #> 5    dem60  ~    ind60 1.329 0.332 4.000      0    0.678    1.981 #> 6    ind60 ~~    ind60 0.416 0.070 5.914      0    0.278    0.553 #> 7    dem60 ~~    dem60 2.842 0.543 5.235      0    1.778    3.906"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/augment_lav_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain factor scores and related definition variables from a lavaan object for 2S-PA analyses. — augment_lav_predict","title":"Obtain factor scores and related definition variables from a lavaan object for 2S-PA analyses. — augment_lav_predict","text":"function obtained factor scores, standard errors, loading matrix, variance covariance matrix calling lavaan::lavPredict() function.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/augment_lav_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain factor scores and related definition variables from a lavaan object for 2S-PA analyses. — augment_lav_predict","text":"","code":"augment_lav_predict(   lavobj,   method = c(\"regression\", \"Bartlett\"),   drop_list_single = TRUE,   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/augment_lav_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain factor scores and related definition variables from a lavaan object for 2S-PA analyses. — augment_lav_predict","text":"lavobj fitted lavaan::lavaan object method character string indicating scoring method use. Must either \"regression\" \"Bartlett\". drop_list_single logical. results unlisted single-group models? ... Additional arguments passed lavaan::lavPredict()","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/augment_lav_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain factor scores and related definition variables from a lavaan object for 2S-PA analyses. — augment_lav_predict","text":"data.frame containing factor scores, corresponding standard errors, loadings cross-loadings factor scores indicators latent variables, error variance-covariance matrix factor scores, measurement intercepts. addition, three character matrices added attributes can used input tspa_mx_model(): ld: cross-loading matrix ev: error variance-covariance matrix int: measurement intercepts","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/augment_lav_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain factor scores and related definition variables from a lavaan object for 2S-PA analyses. — augment_lav_predict","text":"","code":"library(lavaan) #> This is lavaan 0.6-18 #> lavaan is FREE software! Please report any bugs. hs_model <- ' visual  =~ x1 + x2 + x3 ' fit <- cfa(hs_model,            data = HolzingerSwineford1939,            group = \"school\") augment_lav_predict(fit) #> $Pasteur #>        fs_visual se_fs_visual visual_by_fs_visual ev_fs_visual int_fs_visual #> 1   -0.821165191    0.3391326           0.6734826    0.1150109             0 #> 2   -0.124009418    0.3391326           0.6734826    0.1150109             0 #> 3   -0.370072089    0.3391326           0.6734826    0.1150109             0 #> 4    0.440928618    0.3391326           0.6734826    0.1150109             0 #> 5   -0.691389016    0.3391326           0.6734826    0.1150109             0 #> 6   -0.110032619    0.3391326           0.6734826    0.1150109             0 #> 7   -0.904127845    0.3391326           0.6734826    0.1150109             0 #> 8   -0.031747573    0.3391326           0.6734826    0.1150109             0 #> 9   -0.439478981    0.3391326           0.6734826    0.1150109             0 #> 10  -0.938939050    0.3391326           0.6734826    0.1150109             0 #> 11  -0.436821880    0.3391326           0.6734826    0.1150109             0 #> 12   0.305033497    0.3391326           0.6734826    0.1150109             0 #> 13   0.522076263    0.3391326           0.6734826    0.1150109             0 #> 14  -0.090367931    0.3391326           0.6734826    0.1150109             0 #> 15   0.526276771    0.3391326           0.6734826    0.1150109             0 #> 16  -0.226580678    0.3391326           0.6734826    0.1150109             0 #> 17  -0.582016192    0.3391326           0.6734826    0.1150109             0 #> 18   0.017040431    0.3391326           0.6734826    0.1150109             0 #> 19   0.563052459    0.3391326           0.6734826    0.1150109             0 #> 20   0.746621910    0.3391326           0.6734826    0.1150109             0 #> 21   0.234672405    0.3391326           0.6734826    0.1150109             0 #> 22   1.157487518    0.3391326           0.6734826    0.1150109             0 #> 23  -0.162272449    0.3391326           0.6734826    0.1150109             0 #> 24  -0.556027059    0.3391326           0.6734826    0.1150109             0 #> 25  -0.321443540    0.3391326           0.6734826    0.1150109             0 #> 26   0.153141050    0.3391326           0.6734826    0.1150109             0 #> 27   0.696234416    0.3391326           0.6734826    0.1150109             0 #> 28  -0.020961039    0.3391326           0.6734826    0.1150109             0 #> 29   0.532601236    0.3391326           0.6734826    0.1150109             0 #> 30  -0.727687585    0.3391326           0.6734826    0.1150109             0 #> 31  -0.676719580    0.3391326           0.6734826    0.1150109             0 #> 32  -1.120216393    0.3391326           0.6734826    0.1150109             0 #> 33  -0.313631732    0.3391326           0.6734826    0.1150109             0 #> 34  -0.187091845    0.3391326           0.6734826    0.1150109             0 #> 35  -0.887709484    0.3391326           0.6734826    0.1150109             0 #> 36  -0.760795908    0.3391326           0.6734826    0.1150109             0 #> 37   0.556943532    0.3391326           0.6734826    0.1150109             0 #> 38  -0.458666570    0.3391326           0.6734826    0.1150109             0 #> 39   0.514741536    0.3391326           0.6734826    0.1150109             0 #> 40   0.373009089    0.3391326           0.6734826    0.1150109             0 #> 41  -0.528550562    0.3391326           0.6734826    0.1150109             0 #> 42  -0.865864795    0.3391326           0.6734826    0.1150109             0 #> 43  -1.182344640    0.3391326           0.6734826    0.1150109             0 #> 44  -0.435334517    0.3391326           0.6734826    0.1150109             0 #> 45   0.306520860    0.3391326           0.6734826    0.1150109             0 #> 46   0.821604565    0.3391326           0.6734826    0.1150109             0 #> 47   1.213927875    0.3391326           0.6734826    0.1150109             0 #> 48  -0.851887996    0.3391326           0.6734826    0.1150109             0 #> 49  -0.085053749    0.3391326           0.6734826    0.1150109             0 #> 50  -0.508885873    0.3391326           0.6734826    0.1150109             0 #> 51   0.502467638    0.3391326           0.6734826    0.1150109             0 #> 52   0.284732253    0.3391326           0.6734826    0.1150109             0 #> 53   0.202677755    0.3391326           0.6734826    0.1150109             0 #> 54  -0.335953502    0.3391326           0.6734826    0.1150109             0 #> 55   0.556410369    0.3391326           0.6734826    0.1150109             0 #> 56  -0.058746970    0.3391326           0.6734826    0.1150109             0 #> 57  -0.066932487    0.3391326           0.6734826    0.1150109             0 #> 58   0.554230368    0.3391326           0.6734826    0.1150109             0 #> 59  -0.321761185    0.3391326           0.6734826    0.1150109             0 #> 60  -0.421834819    0.3391326           0.6734826    0.1150109             0 #> 61   0.345476529    0.3391326           0.6734826    0.1150109             0 #> 62   0.194809883    0.3391326           0.6734826    0.1150109             0 #> 63  -0.207870208    0.3391326           0.6734826    0.1150109             0 #> 64  -0.441658981    0.3391326           0.6734826    0.1150109             0 #> 65   0.102070958    0.3391326           0.6734826    0.1150109             0 #> 66   0.311198487    0.3391326           0.6734826    0.1150109             0 #> 67   0.676364229    0.3391326           0.6734826    0.1150109             0 #> 68   0.297858262    0.3391326           0.6734826    0.1150109             0 #> 69  -1.055487128    0.3391326           0.6734826    0.1150109             0 #> 70  -0.737997019    0.3391326           0.6734826    0.1150109             0 #> 71  -1.576099236    0.3391326           0.6734826    0.1150109             0 #> 72   0.534360181    0.3391326           0.6734826    0.1150109             0 #> 73  -0.105888156    0.3391326           0.6734826    0.1150109             0 #> 74   0.266237302    0.3391326           0.6734826    0.1150109             0 #> 75  -0.352427927    0.3391326           0.6734826    0.1150109             0 #> 76  -0.334783784    0.3391326           0.6734826    0.1150109             0 #> 77   0.133588508    0.3391326           0.6734826    0.1150109             0 #> 78  -1.035662965    0.3391326           0.6734826    0.1150109             0 #> 79   0.762507108    0.3391326           0.6734826    0.1150109             0 #> 80  -0.260699265    0.3391326           0.6734826    0.1150109             0 #> 81  -0.329095893    0.3391326           0.6734826    0.1150109             0 #> 82   0.752413211    0.3391326           0.6734826    0.1150109             0 #> 83   0.149268188    0.3391326           0.6734826    0.1150109             0 #> 84  -0.208880471    0.3391326           0.6734826    0.1150109             0 #> 85  -1.078285998    0.3391326           0.6734826    0.1150109             0 #> 86   0.306043760    0.3391326           0.6734826    0.1150109             0 #> 87   0.349677056    0.3391326           0.6734826    0.1150109             0 #> 88   0.165686549    0.3391326           0.6734826    0.1150109             0 #> 89   0.077307606    0.3391326           0.6734826    0.1150109             0 #> 90  -0.077401396    0.3391326           0.6734826    0.1150109             0 #> 91  -0.081863485    0.3391326           0.6734826    0.1150109             0 #> 92   0.106748566    0.3391326           0.6734826    0.1150109             0 #> 93  -0.211593616    0.3391326           0.6734826    0.1150109             0 #> 94  -0.926665153    0.3391326           0.6734826    0.1150109             0 #> 95  -0.739484382    0.3391326           0.6734826    0.1150109             0 #> 96   0.570387167    0.3391326           0.6734826    0.1150109             0 #> 97  -0.913642554    0.3391326           0.6734826    0.1150109             0 #> 98   0.547484887    0.3391326           0.6734826    0.1150109             0 #> 99  -0.602850599    0.3391326           0.6734826    0.1150109             0 #> 100  0.225794270    0.3391326           0.6734826    0.1150109             0 #> 101  0.620447015    0.3391326           0.6734826    0.1150109             0 #> 102  0.158885005    0.3391326           0.6734826    0.1150109             0 #> 103 -0.127938344    0.3391326           0.6734826    0.1150109             0 #> 104 -0.420347455    0.3391326           0.6734826    0.1150109             0 #> 105  1.327978307    0.3391326           0.6734826    0.1150109             0 #> 106  0.181843348    0.3391326           0.6734826    0.1150109             0 #> 107 -0.148932224    0.3391326           0.6734826    0.1150109             0 #> 108  0.612373626    0.3391326           0.6734826    0.1150109             0 #> 109 -0.066558798    0.3391326           0.6734826    0.1150109             0 #> 110 -0.420880619    0.3391326           0.6734826    0.1150109             0 #> 111  1.127036295    0.3391326           0.6734826    0.1150109             0 #> 112  0.237591068    0.3391326           0.6734826    0.1150109             0 #> 113  0.853758689    0.3391326           0.6734826    0.1150109             0 #> 114 -0.143618023    0.3391326           0.6734826    0.1150109             0 #> 115  0.475206679    0.3391326           0.6734826    0.1150109             0 #> 116 -0.670554590    0.3391326           0.6734826    0.1150109             0 #> 117  0.022672257    0.3391326           0.6734826    0.1150109             0 #> 118  0.302002707    0.3391326           0.6734826    0.1150109             0 #> 119  0.151392125    0.3391326           0.6734826    0.1150109             0 #> 120 -0.475300449    0.3391326           0.6734826    0.1150109             0 #> 121 -0.346740056    0.3391326           0.6734826    0.1150109             0 #> 122 -0.078888759    0.3391326           0.6734826    0.1150109             0 #> 123  1.197237913    0.3391326           0.6734826    0.1150109             0 #> 124  0.539243306    0.3391326           0.6734826    0.1150109             0 #> 125  0.867258388    0.3391326           0.6734826    0.1150109             0 #> 126  0.592287901    0.3391326           0.6734826    0.1150109             0 #> 127 -0.500540901    0.3391326           0.6734826    0.1150109             0 #> 128 -0.361193954    0.3391326           0.6734826    0.1150109             0 #> 129  0.626883588    0.3391326           0.6734826    0.1150109             0 #> 130 -0.437514518    0.3391326           0.6734826    0.1150109             0 #> 131  0.695972854    0.3391326           0.6734826    0.1150109             0 #> 132  0.424715775    0.3391326           0.6734826    0.1150109             0 #> 133 -0.203725744    0.3391326           0.6734826    0.1150109             0 #> 134 -0.441499507    0.3391326           0.6734826    0.1150109             0 #> 135  0.735619838    0.3391326           0.6734826    0.1150109             0 #> 136  0.783874697    0.3391326           0.6734826    0.1150109             0 #> 137  0.565709540    0.3391326           0.6734826    0.1150109             0 #> 138  0.258425494    0.3391326           0.6734826    0.1150109             0 #> 139  0.861093397    0.3391326           0.6734826    0.1150109             0 #> 140 -0.059757233    0.3391326           0.6734826    0.1150109             0 #> 141 -0.920340689    0.3391326           0.6734826    0.1150109             0 #> 142  0.845629236    0.3391326           0.6734826    0.1150109             0 #> 143  1.227427574    0.3391326           0.6734826    0.1150109             0 #> 144  1.054223601    0.3391326           0.6734826    0.1150109             0 #> 145 -1.246596805    0.3391326           0.6734826    0.1150109             0 #> 146 -0.473120468    0.3391326           0.6734826    0.1150109             0 #> 147 -0.560171503    0.3391326           0.6734826    0.1150109             0 #> 148 -0.365394462    0.3391326           0.6734826    0.1150109             0 #> 149  0.084744422    0.3391326           0.6734826    0.1150109             0 #> 150  0.910676146    0.3391326           0.6734826    0.1150109             0 #> 151  1.094189533    0.3391326           0.6734826    0.1150109             0 #> 152 -0.013149231    0.3391326           0.6734826    0.1150109             0 #> 153 -0.166472976    0.3391326           0.6734826    0.1150109             0 #> 154  0.008695459    0.3391326           0.6734826    0.1150109             0 #> 155 -0.094989494    0.3391326           0.6734826    0.1150109             0 #> 156 -0.457123143    0.3391326           0.6734826    0.1150109             0 #>  #> $`Grant-White` #>        fs_visual se_fs_visual visual_by_fs_visual ev_fs_visual int_fs_visual #> 1   -0.915287109     0.311828           0.6990509   0.09723667             0 #> 2    0.035963597     0.311828           0.6990509   0.09723667             0 #> 3    0.355636604     0.311828           0.6990509   0.09723667             0 #> 4   -0.387353871     0.311828           0.6990509   0.09723667             0 #> 5   -0.622393942     0.311828           0.6990509   0.09723667             0 #> 6    0.195944561     0.311828           0.6990509   0.09723667             0 #> 7    1.353023831     0.311828           0.6990509   0.09723667             0 #> 8   -0.341506254     0.311828           0.6990509   0.09723667             0 #> 9   -0.199493575     0.311828           0.6990509   0.09723667             0 #> 10  -0.689869149     0.311828           0.6990509   0.09723667             0 #> 11  -0.463929554     0.311828           0.6990509   0.09723667             0 #> 12  -0.423001505     0.311828           0.6990509   0.09723667             0 #> 13   0.279743296     0.311828           0.6990509   0.09723667             0 #> 14  -0.916908219     0.311828           0.6990509   0.09723667             0 #> 15   0.589344501     0.311828           0.6990509   0.09723667             0 #> 16   0.191474701     0.311828           0.6990509   0.09723667             0 #> 17   0.935275715     0.311828           0.6990509   0.09723667             0 #> 18   0.393715904     0.311828           0.6990509   0.09723667             0 #> 19   0.086569994     0.311828           0.6990509   0.09723667             0 #> 20   0.555606898     0.311828           0.6990509   0.09723667             0 #> 21  -0.558217193     0.311828           0.6990509   0.09723667             0 #> 22   0.766715894     0.311828           0.6990509   0.09723667             0 #> 23   0.115548801     0.311828           0.6990509   0.09723667             0 #> 24   0.901249191     0.311828           0.6990509   0.09723667             0 #> 25   0.174316971     0.311828           0.6990509   0.09723667             0 #> 26  -0.078980322     0.311828           0.6990509   0.09723667             0 #> 27  -0.581882977     0.311828           0.6990509   0.09723667             0 #> 28  -0.661179262     0.311828           0.6990509   0.09723667             0 #> 29  -0.245341176     0.311828           0.6990509   0.09723667             0 #> 30  -0.195801662     0.311828           0.6990509   0.09723667             0 #> 31  -0.281221524     0.311828           0.6990509   0.09723667             0 #> 32  -0.293909378     0.311828           0.6990509   0.09723667             0 #> 33  -0.604192958     0.311828           0.6990509   0.09723667             0 #> 34  -0.738437335     0.311828           0.6990509   0.09723667             0 #> 35  -0.304109345     0.311828           0.6990509   0.09723667             0 #> 36   0.104931733     0.311828           0.6990509   0.09723667             0 #> 37  -0.025781487     0.311828           0.6990509   0.09723667             0 #> 38  -0.897318824     0.311828           0.6990509   0.09723667             0 #> 39  -0.892560027     0.311828           0.6990509   0.09723667             0 #> 40  -0.078402465     0.311828           0.6990509   0.09723667             0 #> 41  -0.379063934     0.311828           0.6990509   0.09723667             0 #> 42  -0.324926380     0.311828           0.6990509   0.09723667             0 #> 43  -0.684299797     0.311828           0.6990509   0.09723667             0 #> 44  -0.304109345     0.311828           0.6990509   0.09723667             0 #> 45   0.622793169     0.311828           0.6990509   0.09723667             0 #> 46  -0.152835419     0.311828           0.6990509   0.09723667             0 #> 47  -0.421902013     0.311828           0.6990509   0.09723667             0 #> 48  -0.060883872     0.311828           0.6990509   0.09723667             0 #> 49  -0.303298790     0.311828           0.6990509   0.09723667             0 #> 50   0.425021826     0.311828           0.6990509   0.09723667             0 #> 51   0.131478875     0.311828           0.6990509   0.09723667             0 #> 52  -0.914998172     0.311828           0.6990509   0.09723667             0 #> 53   0.324226132     0.311828           0.6990509   0.09723667             0 #> 54  -0.086170767     0.311828           0.6990509   0.09723667             0 #> 55   0.428424818     0.311828           0.6990509   0.09723667             0 #> 56   0.188465179     0.311828           0.6990509   0.09723667             0 #> 57  -0.306958111     0.311828           0.6990509   0.09723667             0 #> 58   0.581736955     0.311828           0.6990509   0.09723667             0 #> 59  -0.743485051     0.311828           0.6990509   0.09723667             0 #> 60   0.263580524     0.311828           0.6990509   0.09723667             0 #> 61   0.178786831     0.311828           0.6990509   0.09723667             0 #> 62  -0.064832113     0.311828           0.6990509   0.09723667             0 #> 63   0.499976414     0.311828           0.6990509   0.09723667             0 #> 64  -0.092839593     0.311828           0.6990509   0.09723667             0 #> 65  -0.263702931     0.311828           0.6990509   0.09723667             0 #> 66  -0.983966325     0.311828           0.6990509   0.09723667             0 #> 67   1.434912536     0.311828           0.6990509   0.09723667             0 #> 68  -1.037582228     0.311828           0.6990509   0.09723667             0 #> 69  -0.047569849     0.311828           0.6990509   0.09723667             0 #> 70   1.084767775     0.311828           0.6990509   0.09723667             0 #> 71   0.092011181     0.311828           0.6990509   0.09723667             0 #> 72  -0.562687052     0.311828           0.6990509   0.09723667             0 #> 73  -0.304919900     0.311828           0.6990509   0.09723667             0 #> 74   1.038920175     0.311828           0.6990509   0.09723667             0 #> 75  -0.789854287     0.311828           0.6990509   0.09723667             0 #> 76  -0.602282928     0.311828           0.6990509   0.09723667             0 #> 77  -0.894630830     0.311828           0.6990509   0.09723667             0 #> 78   0.532614527     0.311828           0.6990509   0.09723667             0 #> 79  -0.548955945     0.311828           0.6990509   0.09723667             0 #> 80  -0.221514635     0.311828           0.6990509   0.09723667             0 #> 81  -0.095849115     0.311828           0.6990509   0.09723667             0 #> 82  -0.122235502     0.311828           0.6990509   0.09723667             0 #> 83   0.892276864     0.311828           0.6990509   0.09723667             0 #> 84   0.328439663     0.311828           0.6990509   0.09723667             0 #> 85   1.217391042     0.311828           0.6990509   0.09723667             0 #> 86   0.574513902     0.311828           0.6990509   0.09723667             0 #> 87   0.160168762     0.311828           0.6990509   0.09723667             0 #> 88   0.654909662     0.311828           0.6990509   0.09723667             0 #> 89  -0.509777155     0.311828           0.6990509   0.09723667             0 #> 90   1.201493560     0.311828           0.6990509   0.09723667             0 #> 91   0.584874625     0.311828           0.6990509   0.09723667             0 #> 92   0.075142371     0.311828           0.6990509   0.09723667             0 #> 93   0.550976266     0.311828           0.6990509   0.09723667             0 #> 94  -0.886308302     0.311828           0.6990509   0.09723667             0 #> 95   0.552075757     0.311828           0.6990509   0.09723667             0 #> 96   1.415972940     0.311828           0.6990509   0.09723667             0 #> 97   0.298650301     0.311828           0.6990509   0.09723667             0 #> 98  -0.143028906     0.311828           0.6990509   0.09723667             0 #> 99   0.245195154     0.311828           0.6990509   0.09723667             0 #> 100  0.247072593     0.311828           0.6990509   0.09723667             0 #> 101  0.817322291     0.311828           0.6990509   0.09723667             0 #> 102  0.651771976     0.311828           0.6990509   0.09723667             0 #> 103  1.338875623     0.311828           0.6990509   0.09723667             0 #> 104 -1.160005528     0.311828           0.6990509   0.09723667             0 #> 105  0.163306449     0.311828           0.6990509   0.09723667             0 #> 106 -0.387353871     0.311828           0.6990509   0.09723667             0 #> 107 -0.517128372     0.311828           0.6990509   0.09723667             0 #> 108  0.065103160     0.311828           0.6990509   0.09723667             0 #> 109 -0.115438510     0.311828           0.6990509   0.09723667             0 #> 110  0.094049376     0.311828           0.6990509   0.09723667             0 #> 111  0.396725409     0.311828           0.6990509   0.09723667             0 #> 112  0.672356312     0.311828           0.6990509   0.09723667             0 #> 113  1.165974090     0.311828           0.6990509   0.09723667             0 #> 114 -0.483518949     0.311828           0.6990509   0.09723667             0 #> 115  0.035024877     0.311828           0.6990509   0.09723667             0 #> 116  0.741974248     0.311828           0.6990509   0.09723667             0 #> 117 -0.170386603     0.311828           0.6990509   0.09723667             0 #> 118 -0.205873481     0.311828           0.6990509   0.09723667             0 #> 119  0.714777307     0.311828           0.6990509   0.09723667             0 #> 120 -0.620772831     0.311828           0.6990509   0.09723667             0 #> 121 -0.313626938     0.311828           0.6990509   0.09723667             0 #> 122 -0.157466035     0.311828           0.6990509   0.09723667             0 #> 123  0.118269386     0.311828           0.6990509   0.09723667             0 #> 124  0.101111673     0.311828           0.6990509   0.09723667             0 #> 125 -0.625403463     0.311828           0.6990509   0.09723667             0 #> 126  0.486638761     0.311828           0.6990509   0.09723667             0 #> 127 -0.178676540     0.311828           0.6990509   0.09723667             0 #> 128  0.274013189     0.311828           0.6990509   0.09723667             0 #> 129 -0.316347523     0.311828           0.6990509   0.09723667             0 #> 130 -0.026752814     0.311828           0.6990509   0.09723667             0 #> 131  0.245323318     0.311828           0.6990509   0.09723667             0 #> 132 -0.356336853     0.311828           0.6990509   0.09723667             0 #> 133 -0.581594057     0.311828           0.6990509   0.09723667             0 #> 134  0.263002667     0.311828           0.6990509   0.09723667             0 #> 135 -0.864680712     0.311828           0.6990509   0.09723667             0 #> 136 -0.377964443     0.311828           0.6990509   0.09723667             0 #> 137 -0.112717909     0.311828           0.6990509   0.09723667             0 #> 138  0.114449326     0.311828           0.6990509   0.09723667             0 #> 139  0.001287274     0.311828           0.6990509   0.09723667             0 #> 140  0.597634438     0.311828           0.6990509   0.09723667             0 #> 141 -0.252531637     0.311828           0.6990509   0.09723667             0 #> 142 -0.472901881     0.311828           0.6990509   0.09723667             0 #> 143 -0.187255397     0.311828           0.6990509   0.09723667             0 #> 144 -0.542415283     0.311828           0.6990509   0.09723667             0 #> 145  0.358774274     0.311828           0.6990509   0.09723667             0 #>  #> attr(,\"ld\") #>           visual                #> fs_visual \"visual_by_fs_visual\" #> attr(,\"ev\") #>           fs_visual      #> fs_visual \"ev_fs_visual\" #> attr(,\"int\") #> [1] \"int_fs_visual\""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/block_diag.html","id":null,"dir":"Reference","previous_headings":"","what":"Create block diagonal matrix — block_diag","title":"Create block diagonal matrix — block_diag","text":"Create block diagonal matrix","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/block_diag.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create block diagonal matrix — block_diag","text":"","code":"block_diag(...)"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/block_diag.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create block diagonal matrix — block_diag","text":"... Either multiple matrices list matrices.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute factor scores — compute_fscore","title":"Compute factor scores — compute_fscore","text":"Compute factor scores","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute factor scores — compute_fscore","text":"","code":"compute_fscore(   y,   lambda,   theta,   psi = NULL,   nu = NULL,   alpha = NULL,   method = c(\"regression\", \"Bartlett\"),   center_y = TRUE,   acov = FALSE,   fs_matrices = FALSE )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute factor scores — compute_fscore","text":"y N x p matrix row response vector. one observation, matrix one row. lambda p x q matrix factor loadings. theta p x p matrix unique variance-covariances. psi q x q matrix latent factor variance-covariances. nu vector length p measurement intercepts. alpha vector length q latent means. method character string indicating method computing factor scores. Currently, \"regression\" supported. center_y Logical indicating whether y mean-centered. Default TRUE. acov Logical indicating whether asymptotic covariance matrix factor scores returned attribute. fs_matrices Logical indicating whether covariances error portion factor scores (fsT), factor score loading matrix (\\(L\\); fsL) intercept vector (\\(b\\); fsb) returned. loading intercept matrices implied loadings intercepts model using factor scores indicators latent variables. TRUE, matrices added attributes.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute factor scores — compute_fscore","text":"N x p matrix factor scores.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/compute_fscore.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute factor scores — compute_fscore","text":"","code":"library(lavaan) fit <- cfa(\" ind60 =~ x1 + x2 + x3              dem60 =~ y1 + y2 + y3 + y4 \",            data = PoliticalDemocracy) fs_lavaan <- lavPredict(fit, method = \"Bartlett\") # Using R2spa::compute_fscore() est <- lavInspect(fit, what = \"est\") fs_hand <- compute_fscore(lavInspect(fit, what = \"data\"),                           lambda = est$lambda,                           theta = est$theta,                           psi = est$psi,                           method = \"Bartlett\") fs_hand - fs_lavaan  # same scores #>       ind60 dem60 #>  [1,]     0     0 #>  [2,]     0     0 #>  [3,]     0     0 #>  [4,]     0     0 #>  [5,]     0     0 #>  [6,]     0     0 #>  [7,]     0     0 #>  [8,]     0     0 #>  [9,]     0     0 #> [10,]     0     0 #> [11,]     0     0 #> [12,]     0     0 #> [13,]     0     0 #> [14,]     0     0 #> [15,]     0     0 #> [16,]     0     0 #> [17,]     0     0 #> [18,]     0     0 #> [19,]     0     0 #> [20,]     0     0 #> [21,]     0     0 #> [22,]     0     0 #> [23,]     0     0 #> [24,]     0     0 #> [25,]     0     0 #> [26,]     0     0 #> [27,]     0     0 #> [28,]     0     0 #> [29,]     0     0 #> [30,]     0     0 #> [31,]     0     0 #> [32,]     0     0 #> [33,]     0     0 #> [34,]     0     0 #> [35,]     0     0 #> [36,]     0     0 #> [37,]     0     0 #> [38,]     0     0 #> [39,]     0     0 #> [40,]     0     0 #> [41,]     0     0 #> [42,]     0     0 #> [43,]     0     0 #> [44,]     0     0 #> [45,]     0     0 #> [46,]     0     0 #> [47,]     0     0 #> [48,]     0     0 #> [49,]     0     0 #> [50,]     0     0 #> [51,]     0     0 #> [52,]     0     0 #> [53,]     0     0 #> [54,]     0     0 #> [55,]     0     0 #> [56,]     0     0 #> [57,]     0     0 #> [58,]     0     0 #> [59,]     0     0 #> [60,]     0     0 #> [61,]     0     0 #> [62,]     0     0 #> [63,]     0     0 #> [64,]     0     0 #> [65,]     0     0 #> [66,]     0     0 #> [67,]     0     0 #> [68,]     0     0 #> [69,]     0     0 #> [70,]     0     0 #> [71,]     0     0 #> [72,]     0     0 #> [73,]     0     0 #> [74,]     0     0 #> [75,]     0     0"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"Get Factor Scores Corresponding Standard Error Measurement","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"","code":"get_fs(   data,   model = NULL,   group = NULL,   method = c(\"regression\", \"Bartlett\"),   corrected_fsT = FALSE,   vfsLT = FALSE,   reliability = FALSE,   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"data data frame containing indicators. model optional string specifying measurement model lavaan syntax. See model.syntax information. group Character. Name grouping variable multiple group analysis, passed cfa. method Character. Method computing factor scores (options \"regression\" \"Bartlett\"). Currently, default \"regression\" consistent lavPredict, Bartlett scores desirable properties may preferred 2S-PA. corrected_fsT Logical. Whether correct sampling error factor score weights computing error variance estimates factor scores. vfsLT Logical. Whether return covariance matrix fsT fsL, can used input vcov_corrected() obtain corrected covariances standard errors tspa() results. currently ignored. reliability Logical. Whether return reliability factor scores. ... additional arguments passed cfa. See lavOptions complete list.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"data frame containing factor scores (prefix \"fs_\"), standard errors (suffix \"_se\"), implied loadings factor \"_by_\" factor scores, error variance-covariance factor scores (prefix \"evfs_\"). following also returned attributes: * fsT: error covariance factor scores * fsL: loading matrix factor scores * fsb: intercepts factor scores * scoring_matrix: weights computing factor scores items","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs","text":"","code":"library(lavaan) get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\")]) #>          fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1 #> 1  -0.52616832 0.1213615   0.9657673 0.01472862 #> 2   0.14365274 0.1213615   0.9657673 0.01472862 #> 3   0.71435592 0.1213615   0.9657673 0.01472862 #> 4   1.23992565 0.1213615   0.9657673 0.01472862 #> 5   0.83190803 0.1213615   0.9657673 0.01472862 #> 6   0.21238453 0.1213615   0.9657673 0.01472862 #> 7   0.11880855 0.1213615   0.9657673 0.01472862 #> 8   0.11322703 0.1213615   0.9657673 0.01472862 #> 9   0.25617279 0.1213615   0.9657673 0.01472862 #> 10  0.37112496 0.1213615   0.9657673 0.01472862 #> 11  0.67281395 0.1213615   0.9657673 0.01472862 #> 12  0.56885577 0.1213615   0.9657673 0.01472862 #> 13  1.31369791 0.1213615   0.9657673 0.01472862 #> 14  0.22042629 0.1213615   0.9657673 0.01472862 #> 15  0.57849228 0.1213615   0.9657673 0.01472862 #> 16  0.37805983 0.1213615   0.9657673 0.01472862 #> 17  0.05734046 0.1213615   0.9657673 0.01472862 #> 18 -0.01609202 0.1213615   0.9657673 0.01472862 #> 19  0.88923616 0.1213615   0.9657673 0.01472862 #> 20  1.11445897 0.1213615   0.9657673 0.01472862 #> 21  0.94657339 0.1213615   0.9657673 0.01472862 #> 22  0.90122770 0.1213615   0.9657673 0.01472862 #> 23  0.58409450 0.1213615   0.9657673 0.01472862 #> 24  0.64089192 0.1213615   0.9657673 0.01472862 #> 25  0.91021968 0.1213615   0.9657673 0.01472862 #> 26 -0.89660969 0.1213615   0.9657673 0.01472862 #> 27 -0.13195991 0.1213615   0.9657673 0.01472862 #> 28 -0.52968769 0.1213615   0.9657673 0.01472862 #> 29 -0.81799629 0.1213615   0.9657673 0.01472862 #> 30 -1.27199371 0.1213615   0.9657673 0.01472862 #> 31 -0.32096024 0.1213615   0.9657673 0.01472862 #> 32 -1.16780103 0.1213615   0.9657673 0.01472862 #> 33 -0.12295473 0.1213615   0.9657673 0.01472862 #> 34 -0.04285945 0.1213615   0.9657673 0.01472862 #> 35 -0.34323505 0.1213615   0.9657673 0.01472862 #> 36 -0.60541633 0.1213615   0.9657673 0.01472862 #> 37  0.17688718 0.1213615   0.9657673 0.01472862 #> 38 -0.55066055 0.1213615   0.9657673 0.01472862 #> 39 -1.05988219 0.1213615   0.9657673 0.01472862 #> 40 -0.04138802 0.1213615   0.9657673 0.01472862 #> 41 -0.12611837 0.1213615   0.9657673 0.01472862 #> 42 -0.60322892 0.1213615   0.9657673 0.01472862 #> 43 -0.11057176 0.1213615   0.9657673 0.01472862 #> 44 -1.06423085 0.1213615   0.9657673 0.01472862 #> 45 -1.08354999 0.1213615   0.9657673 0.01472862 #> 46 -0.84009484 0.1213615   0.9657673 0.01472862 #> 47 -1.14678213 0.1213615   0.9657673 0.01472862 #> 48 -0.57578976 0.1213615   0.9657673 0.01472862 #> 49  0.07186692 0.1213615   0.9657673 0.01472862 #> 50  0.14682421 0.1213615   0.9657673 0.01472862 #> 51  0.35871830 0.1213615   0.9657673 0.01472862 #> 52 -0.43403195 0.1213615   0.9657673 0.01472862 #> 53  0.44603111 0.1213615   0.9657673 0.01472862 #> 54  0.26352000 0.1213615   0.9657673 0.01472862 #> 55  0.55051165 0.1213615   0.9657673 0.01472862 #> 56  0.23453122 0.1213615   0.9657673 0.01472862 #> 57  0.27968138 0.1213615   0.9657673 0.01472862 #> 58  0.70960640 0.1213615   0.9657673 0.01472862 #> 59  0.25227978 0.1213615   0.9657673 0.01472862 #> 60  1.18849297 0.1213615   0.9657673 0.01472862 #> 61  0.21104946 0.1213615   0.9657673 0.01472862 #> 62 -1.16516281 0.1213615   0.9657673 0.01472862 #> 63 -0.85560065 0.1213615   0.9657673 0.01472862 #> 64  0.13398476 0.1213615   0.9657673 0.01472862 #> 65 -0.07912189 0.1213615   0.9657673 0.01472862 #> 66 -0.27146711 0.1213615   0.9657673 0.01472862 #> 67 -0.04417217 0.1213615   0.9657673 0.01472862 #> 68 -1.33425662 0.1213615   0.9657673 0.01472862 #> 69 -0.38720750 0.1213615   0.9657673 0.01472862 #> 70 -0.55355511 0.1213615   0.9657673 0.01472862 #> 71 -0.72242623 0.1213615   0.9657673 0.01472862 #> 72  0.30607449 0.1213615   0.9657673 0.01472862 #> 73  0.77707950 0.1213615   0.9657673 0.01472862 #> 74  0.06847481 0.1213615   0.9657673 0.01472862 #> 75 -0.11052927 0.1213615   0.9657673 0.01472862  # Multiple factors get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\", \"y1\", \"y2\", \"y3\", \"y4\")],        model = \" ind60 =~ x1 + x2 + x3                  dem60 =~ y1 + y2 + y3 + y4 \") #>       fs_ind60    fs_dem60 fs_ind60_se fs_dem60_se ind60_by_fs_ind60 #> 1  -0.54258816 -2.74640573   0.1245694   0.6307323         0.9553858 #> 2   0.12647664 -2.85646114   0.1245694   0.6307323         0.9553858 #> 3   0.73408891  2.74401728   0.1245694   0.6307323         0.9553858 #> 4   1.25253604  3.10856431   0.1245694   0.6307323         0.9553858 #> 5   0.83355267  1.92455641   0.1245694   0.6307323         0.9553858 #> 6   0.22426801  1.02292332   0.1245694   0.6307323         0.9553858 #> 7   0.12517739  1.00406461   0.1245694   0.6307323         0.9553858 #> 8   0.11783867 -0.37216403   0.1245694   0.6307323         0.9553858 #> 9   0.25175134 -1.24897911   0.1245694   0.6307323         0.9553858 #> 10  0.39938631  2.85267059   0.1245694   0.6307323         0.9553858 #> 11  0.67497777  1.41959595   0.1245694   0.6307323         0.9553858 #> 12  0.56462020  1.08769844   0.1245694   0.6307323         0.9553858 #> 13  1.31236592  1.54090232   0.1245694   0.6307323         0.9553858 #> 14  0.23246021  1.77370863   0.1245694   0.6307323         0.9553858 #> 15  0.58638481  2.45676871   0.1245694   0.6307323         0.9553858 #> 16  0.38404785  2.35887573   0.1245694   0.6307323         0.9553858 #> 17  0.05076465  0.04034088   0.1245694   0.6307323         0.9553858 #> 18 -0.01747337 -1.86718064   0.1245694   0.6307323         0.9553858 #> 19  0.90920762  3.61477756   0.1245694   0.6307323         0.9553858 #> 20  1.12553557  0.88355273   0.1245694   0.6307323         0.9553858 #> 21  0.97202590  3.62673300   0.1245694   0.6307323         0.9553858 #> 22  0.87820036 -3.02428925   0.1245694   0.6307323         0.9553858 #> 23  0.57540754 -1.51695438   0.1245694   0.6307323         0.9553858 #> 24  0.66221224  2.76341635   0.1245694   0.6307323         0.9553858 #> 25  0.92358281  2.00507336   0.1245694   0.6307323         0.9553858 #> 26 -0.89353051 -0.92008050   0.1245694   0.6307323         0.9553858 #> 27 -0.13984744 -1.19025576   0.1245694   0.6307323         0.9553858 #> 28 -0.53828496 -1.01247764   0.1245694   0.6307323         0.9553858 #> 29 -0.80834865  0.10456709   0.1245694   0.6307323         0.9553858 #> 30 -1.25324343 -0.71847055   0.1245694   0.6307323         0.9553858 #> 31 -0.33373641 -1.61401581   0.1245694   0.6307323         0.9553858 #> 32 -1.17441075 -3.27250363   0.1245694   0.6307323         0.9553858 #> 33 -0.12409974 -1.17530231   0.1245694   0.6307323         0.9553858 #> 34 -0.04239173 -0.53796274   0.1245694   0.6307323         0.9553858 #> 35 -0.34010528  0.74552889   0.1245694   0.6307323         0.9553858 #> 36 -0.58953870  1.61018662   0.1245694   0.6307323         0.9553858 #> 37  0.17453657 -0.28144814   0.1245694   0.6307323         0.9553858 #> 38 -0.54457243  0.37694690   0.1245694   0.6307323         0.9553858 #> 39 -1.05196602 -0.62919501   0.1245694   0.6307323         0.9553858 #> 40 -0.05504697 -0.03346842   0.1245694   0.6307323         0.9553858 #> 41 -0.12364358 -0.38394102   0.1245694   0.6307323         0.9553858 #> 42 -0.59058710  1.35347275   0.1245694   0.6307323         0.9553858 #> 43 -0.11968796  0.89227782   0.1245694   0.6307323         0.9553858 #> 44 -1.07176064 -2.08481096   0.1245694   0.6307323         0.9553858 #> 45 -1.09139097 -2.07944291   0.1245694   0.6307323         0.9553858 #> 46 -0.83287255  1.59590721   0.1245694   0.6307323         0.9553858 #> 47 -1.14519896 -1.53352201   0.1245694   0.6307323         0.9553858 #> 48 -0.56115378  2.08138051   0.1245694   0.6307323         0.9553858 #> 49  0.06493340 -1.04044137   0.1245694   0.6307323         0.9553858 #> 50  0.15671638  1.72618633   0.1245694   0.6307323         0.9553858 #> 51  0.34626130 -1.24967043   0.1245694   0.6307323         0.9553858 #> 52 -0.45158373 -2.31742576   0.1245694   0.6307323         0.9553858 #> 53  0.43233465 -1.07533341   0.1245694   0.6307323         0.9553858 #> 54  0.25779725 -0.02904676   0.1245694   0.6307323         0.9553858 #> 55  0.51730650 -2.78207923   0.1245694   0.6307323         0.9553858 #> 56  0.20104991 -2.49001474   0.1245694   0.6307323         0.9553858 #> 57  0.25318620 -2.52145147   0.1245694   0.6307323         0.9553858 #> 58  0.72354623  1.86717109   0.1245694   0.6307323         0.9553858 #> 59  0.24619740 -0.93321102   0.1245694   0.6307323         0.9553858 #> 60  1.21681210  3.19853937   0.1245694   0.6307323         0.9553858 #> 61  0.18167599 -3.15685030   0.1245694   0.6307323         0.9553858 #> 62 -1.16605067 -3.41334680   0.1245694   0.6307323         0.9553858 #> 63 -0.86491026 -3.11864398   0.1245694   0.6307323         0.9553858 #> 64  0.10990059 -0.47238885   0.1245694   0.6307323         0.9553858 #> 65 -0.07376176  2.95292007   0.1245694   0.6307323         0.9553858 #> 66 -0.28782931 -1.96509718   0.1245694   0.6307323         0.9553858 #> 67 -0.02508160  2.96218478   0.1245694   0.6307323         0.9553858 #> 68 -1.31843215 -1.59567027   0.1245694   0.6307323         0.9553858 #> 69 -0.40462357 -1.79146161   0.1245694   0.6307323         0.9553858 #> 70 -0.55568363 -1.01578892   0.1245694   0.6307323         0.9553858 #> 71 -0.71308015  0.08818212   0.1245694   0.6307323         0.9553858 #> 72  0.31014319  1.70765911   0.1245694   0.6307323         0.9553858 #> 73  0.79092897  1.86102556   0.1245694   0.6307323         0.9553858 #> 74  0.08770237  3.12885767   0.1245694   0.6307323         0.9553858 #> 75 -0.14138149 -2.41398025   0.1245694   0.6307323         0.9553858 #>    ind60_by_fs_dem60 dem60_by_fs_ind60 dem60_by_fs_dem60 ev_fs_ind60 #> 1           0.181827       0.005867694         0.8688887  0.01551752 #> 2           0.181827       0.005867694         0.8688887  0.01551752 #> 3           0.181827       0.005867694         0.8688887  0.01551752 #> 4           0.181827       0.005867694         0.8688887  0.01551752 #> 5           0.181827       0.005867694         0.8688887  0.01551752 #> 6           0.181827       0.005867694         0.8688887  0.01551752 #> 7           0.181827       0.005867694         0.8688887  0.01551752 #> 8           0.181827       0.005867694         0.8688887  0.01551752 #> 9           0.181827       0.005867694         0.8688887  0.01551752 #> 10          0.181827       0.005867694         0.8688887  0.01551752 #> 11          0.181827       0.005867694         0.8688887  0.01551752 #> 12          0.181827       0.005867694         0.8688887  0.01551752 #> 13          0.181827       0.005867694         0.8688887  0.01551752 #> 14          0.181827       0.005867694         0.8688887  0.01551752 #> 15          0.181827       0.005867694         0.8688887  0.01551752 #> 16          0.181827       0.005867694         0.8688887  0.01551752 #> 17          0.181827       0.005867694         0.8688887  0.01551752 #> 18          0.181827       0.005867694         0.8688887  0.01551752 #> 19          0.181827       0.005867694         0.8688887  0.01551752 #> 20          0.181827       0.005867694         0.8688887  0.01551752 #> 21          0.181827       0.005867694         0.8688887  0.01551752 #> 22          0.181827       0.005867694         0.8688887  0.01551752 #> 23          0.181827       0.005867694         0.8688887  0.01551752 #> 24          0.181827       0.005867694         0.8688887  0.01551752 #> 25          0.181827       0.005867694         0.8688887  0.01551752 #> 26          0.181827       0.005867694         0.8688887  0.01551752 #> 27          0.181827       0.005867694         0.8688887  0.01551752 #> 28          0.181827       0.005867694         0.8688887  0.01551752 #> 29          0.181827       0.005867694         0.8688887  0.01551752 #> 30          0.181827       0.005867694         0.8688887  0.01551752 #> 31          0.181827       0.005867694         0.8688887  0.01551752 #> 32          0.181827       0.005867694         0.8688887  0.01551752 #> 33          0.181827       0.005867694         0.8688887  0.01551752 #> 34          0.181827       0.005867694         0.8688887  0.01551752 #> 35          0.181827       0.005867694         0.8688887  0.01551752 #> 36          0.181827       0.005867694         0.8688887  0.01551752 #> 37          0.181827       0.005867694         0.8688887  0.01551752 #> 38          0.181827       0.005867694         0.8688887  0.01551752 #> 39          0.181827       0.005867694         0.8688887  0.01551752 #> 40          0.181827       0.005867694         0.8688887  0.01551752 #> 41          0.181827       0.005867694         0.8688887  0.01551752 #> 42          0.181827       0.005867694         0.8688887  0.01551752 #> 43          0.181827       0.005867694         0.8688887  0.01551752 #> 44          0.181827       0.005867694         0.8688887  0.01551752 #> 45          0.181827       0.005867694         0.8688887  0.01551752 #> 46          0.181827       0.005867694         0.8688887  0.01551752 #> 47          0.181827       0.005867694         0.8688887  0.01551752 #> 48          0.181827       0.005867694         0.8688887  0.01551752 #> 49          0.181827       0.005867694         0.8688887  0.01551752 #> 50          0.181827       0.005867694         0.8688887  0.01551752 #> 51          0.181827       0.005867694         0.8688887  0.01551752 #> 52          0.181827       0.005867694         0.8688887  0.01551752 #> 53          0.181827       0.005867694         0.8688887  0.01551752 #> 54          0.181827       0.005867694         0.8688887  0.01551752 #> 55          0.181827       0.005867694         0.8688887  0.01551752 #> 56          0.181827       0.005867694         0.8688887  0.01551752 #> 57          0.181827       0.005867694         0.8688887  0.01551752 #> 58          0.181827       0.005867694         0.8688887  0.01551752 #> 59          0.181827       0.005867694         0.8688887  0.01551752 #> 60          0.181827       0.005867694         0.8688887  0.01551752 #> 61          0.181827       0.005867694         0.8688887  0.01551752 #> 62          0.181827       0.005867694         0.8688887  0.01551752 #> 63          0.181827       0.005867694         0.8688887  0.01551752 #> 64          0.181827       0.005867694         0.8688887  0.01551752 #> 65          0.181827       0.005867694         0.8688887  0.01551752 #> 66          0.181827       0.005867694         0.8688887  0.01551752 #> 67          0.181827       0.005867694         0.8688887  0.01551752 #> 68          0.181827       0.005867694         0.8688887  0.01551752 #> 69          0.181827       0.005867694         0.8688887  0.01551752 #> 70          0.181827       0.005867694         0.8688887  0.01551752 #> 71          0.181827       0.005867694         0.8688887  0.01551752 #> 72          0.181827       0.005867694         0.8688887  0.01551752 #> 73          0.181827       0.005867694         0.8688887  0.01551752 #> 74          0.181827       0.005867694         0.8688887  0.01551752 #> 75          0.181827       0.005867694         0.8688887  0.01551752 #>    ecov_fs_dem60_fs_ind60 ev_fs_dem60 #> 1             0.005632564   0.3978232 #> 2             0.005632564   0.3978232 #> 3             0.005632564   0.3978232 #> 4             0.005632564   0.3978232 #> 5             0.005632564   0.3978232 #> 6             0.005632564   0.3978232 #> 7             0.005632564   0.3978232 #> 8             0.005632564   0.3978232 #> 9             0.005632564   0.3978232 #> 10            0.005632564   0.3978232 #> 11            0.005632564   0.3978232 #> 12            0.005632564   0.3978232 #> 13            0.005632564   0.3978232 #> 14            0.005632564   0.3978232 #> 15            0.005632564   0.3978232 #> 16            0.005632564   0.3978232 #> 17            0.005632564   0.3978232 #> 18            0.005632564   0.3978232 #> 19            0.005632564   0.3978232 #> 20            0.005632564   0.3978232 #> 21            0.005632564   0.3978232 #> 22            0.005632564   0.3978232 #> 23            0.005632564   0.3978232 #> 24            0.005632564   0.3978232 #> 25            0.005632564   0.3978232 #> 26            0.005632564   0.3978232 #> 27            0.005632564   0.3978232 #> 28            0.005632564   0.3978232 #> 29            0.005632564   0.3978232 #> 30            0.005632564   0.3978232 #> 31            0.005632564   0.3978232 #> 32            0.005632564   0.3978232 #> 33            0.005632564   0.3978232 #> 34            0.005632564   0.3978232 #> 35            0.005632564   0.3978232 #> 36            0.005632564   0.3978232 #> 37            0.005632564   0.3978232 #> 38            0.005632564   0.3978232 #> 39            0.005632564   0.3978232 #> 40            0.005632564   0.3978232 #> 41            0.005632564   0.3978232 #> 42            0.005632564   0.3978232 #> 43            0.005632564   0.3978232 #> 44            0.005632564   0.3978232 #> 45            0.005632564   0.3978232 #> 46            0.005632564   0.3978232 #> 47            0.005632564   0.3978232 #> 48            0.005632564   0.3978232 #> 49            0.005632564   0.3978232 #> 50            0.005632564   0.3978232 #> 51            0.005632564   0.3978232 #> 52            0.005632564   0.3978232 #> 53            0.005632564   0.3978232 #> 54            0.005632564   0.3978232 #> 55            0.005632564   0.3978232 #> 56            0.005632564   0.3978232 #> 57            0.005632564   0.3978232 #> 58            0.005632564   0.3978232 #> 59            0.005632564   0.3978232 #> 60            0.005632564   0.3978232 #> 61            0.005632564   0.3978232 #> 62            0.005632564   0.3978232 #> 63            0.005632564   0.3978232 #> 64            0.005632564   0.3978232 #> 65            0.005632564   0.3978232 #> 66            0.005632564   0.3978232 #> 67            0.005632564   0.3978232 #> 68            0.005632564   0.3978232 #> 69            0.005632564   0.3978232 #> 70            0.005632564   0.3978232 #> 71            0.005632564   0.3978232 #> 72            0.005632564   0.3978232 #> 73            0.005632564   0.3978232 #> 74            0.005632564   0.3978232 #> 75            0.005632564   0.3978232  # Multiple-group hs_model <- ' visual  =~ x1 + x2 + x3 ' fit <- cfa(hs_model,            data = HolzingerSwineford1939,            group = \"school\") get_fs(HolzingerSwineford1939, hs_model, group = \"school\") #> $Pasteur #>        fs_visual fs_visual_se visual_by_fs_visual ev_fs_visual  school #> 1   -0.821165191    0.3391326           0.6734826    0.1150109 Pasteur #> 2   -0.124009418    0.3391326           0.6734826    0.1150109 Pasteur #> 3   -0.370072089    0.3391326           0.6734826    0.1150109 Pasteur #> 4    0.440928618    0.3391326           0.6734826    0.1150109 Pasteur #> 5   -0.691389016    0.3391326           0.6734826    0.1150109 Pasteur #> 6   -0.110032619    0.3391326           0.6734826    0.1150109 Pasteur #> 7   -0.904127845    0.3391326           0.6734826    0.1150109 Pasteur #> 8   -0.031747573    0.3391326           0.6734826    0.1150109 Pasteur #> 9   -0.439478981    0.3391326           0.6734826    0.1150109 Pasteur #> 10  -0.938939050    0.3391326           0.6734826    0.1150109 Pasteur #> 11  -0.436821880    0.3391326           0.6734826    0.1150109 Pasteur #> 12   0.305033497    0.3391326           0.6734826    0.1150109 Pasteur #> 13   0.522076263    0.3391326           0.6734826    0.1150109 Pasteur #> 14  -0.090367931    0.3391326           0.6734826    0.1150109 Pasteur #> 15   0.526276771    0.3391326           0.6734826    0.1150109 Pasteur #> 16  -0.226580678    0.3391326           0.6734826    0.1150109 Pasteur #> 17  -0.582016192    0.3391326           0.6734826    0.1150109 Pasteur #> 18   0.017040431    0.3391326           0.6734826    0.1150109 Pasteur #> 19   0.563052459    0.3391326           0.6734826    0.1150109 Pasteur #> 20   0.746621910    0.3391326           0.6734826    0.1150109 Pasteur #> 21   0.234672405    0.3391326           0.6734826    0.1150109 Pasteur #> 22   1.157487518    0.3391326           0.6734826    0.1150109 Pasteur #> 23  -0.162272449    0.3391326           0.6734826    0.1150109 Pasteur #> 24  -0.556027059    0.3391326           0.6734826    0.1150109 Pasteur #> 25  -0.321443540    0.3391326           0.6734826    0.1150109 Pasteur #> 26   0.153141050    0.3391326           0.6734826    0.1150109 Pasteur #> 27   0.696234416    0.3391326           0.6734826    0.1150109 Pasteur #> 28  -0.020961039    0.3391326           0.6734826    0.1150109 Pasteur #> 29   0.532601236    0.3391326           0.6734826    0.1150109 Pasteur #> 30  -0.727687585    0.3391326           0.6734826    0.1150109 Pasteur #> 31  -0.676719580    0.3391326           0.6734826    0.1150109 Pasteur #> 32  -1.120216393    0.3391326           0.6734826    0.1150109 Pasteur #> 33  -0.313631732    0.3391326           0.6734826    0.1150109 Pasteur #> 34  -0.187091845    0.3391326           0.6734826    0.1150109 Pasteur #> 35  -0.887709484    0.3391326           0.6734826    0.1150109 Pasteur #> 36  -0.760795908    0.3391326           0.6734826    0.1150109 Pasteur #> 37   0.556943532    0.3391326           0.6734826    0.1150109 Pasteur #> 38  -0.458666570    0.3391326           0.6734826    0.1150109 Pasteur #> 39   0.514741536    0.3391326           0.6734826    0.1150109 Pasteur #> 40   0.373009089    0.3391326           0.6734826    0.1150109 Pasteur #> 41  -0.528550562    0.3391326           0.6734826    0.1150109 Pasteur #> 42  -0.865864795    0.3391326           0.6734826    0.1150109 Pasteur #> 43  -1.182344640    0.3391326           0.6734826    0.1150109 Pasteur #> 44  -0.435334517    0.3391326           0.6734826    0.1150109 Pasteur #> 45   0.306520860    0.3391326           0.6734826    0.1150109 Pasteur #> 46   0.821604565    0.3391326           0.6734826    0.1150109 Pasteur #> 47   1.213927875    0.3391326           0.6734826    0.1150109 Pasteur #> 48  -0.851887996    0.3391326           0.6734826    0.1150109 Pasteur #> 49  -0.085053749    0.3391326           0.6734826    0.1150109 Pasteur #> 50  -0.508885873    0.3391326           0.6734826    0.1150109 Pasteur #> 51   0.502467638    0.3391326           0.6734826    0.1150109 Pasteur #> 52   0.284732253    0.3391326           0.6734826    0.1150109 Pasteur #> 53   0.202677755    0.3391326           0.6734826    0.1150109 Pasteur #> 54  -0.335953502    0.3391326           0.6734826    0.1150109 Pasteur #> 55   0.556410369    0.3391326           0.6734826    0.1150109 Pasteur #> 56  -0.058746970    0.3391326           0.6734826    0.1150109 Pasteur #> 57  -0.066932487    0.3391326           0.6734826    0.1150109 Pasteur #> 58   0.554230368    0.3391326           0.6734826    0.1150109 Pasteur #> 59  -0.321761185    0.3391326           0.6734826    0.1150109 Pasteur #> 60  -0.421834819    0.3391326           0.6734826    0.1150109 Pasteur #> 61   0.345476529    0.3391326           0.6734826    0.1150109 Pasteur #> 62   0.194809883    0.3391326           0.6734826    0.1150109 Pasteur #> 63  -0.207870208    0.3391326           0.6734826    0.1150109 Pasteur #> 64  -0.441658981    0.3391326           0.6734826    0.1150109 Pasteur #> 65   0.102070958    0.3391326           0.6734826    0.1150109 Pasteur #> 66   0.311198487    0.3391326           0.6734826    0.1150109 Pasteur #> 67   0.676364229    0.3391326           0.6734826    0.1150109 Pasteur #> 68   0.297858262    0.3391326           0.6734826    0.1150109 Pasteur #> 69  -1.055487128    0.3391326           0.6734826    0.1150109 Pasteur #> 70  -0.737997019    0.3391326           0.6734826    0.1150109 Pasteur #> 71  -1.576099236    0.3391326           0.6734826    0.1150109 Pasteur #> 72   0.534360181    0.3391326           0.6734826    0.1150109 Pasteur #> 73  -0.105888156    0.3391326           0.6734826    0.1150109 Pasteur #> 74   0.266237302    0.3391326           0.6734826    0.1150109 Pasteur #> 75  -0.352427927    0.3391326           0.6734826    0.1150109 Pasteur #> 76  -0.334783784    0.3391326           0.6734826    0.1150109 Pasteur #> 77   0.133588508    0.3391326           0.6734826    0.1150109 Pasteur #> 78  -1.035662965    0.3391326           0.6734826    0.1150109 Pasteur #> 79   0.762507108    0.3391326           0.6734826    0.1150109 Pasteur #> 80  -0.260699265    0.3391326           0.6734826    0.1150109 Pasteur #> 81  -0.329095893    0.3391326           0.6734826    0.1150109 Pasteur #> 82   0.752413211    0.3391326           0.6734826    0.1150109 Pasteur #> 83   0.149268188    0.3391326           0.6734826    0.1150109 Pasteur #> 84  -0.208880471    0.3391326           0.6734826    0.1150109 Pasteur #> 85  -1.078285998    0.3391326           0.6734826    0.1150109 Pasteur #> 86   0.306043760    0.3391326           0.6734826    0.1150109 Pasteur #> 87   0.349677056    0.3391326           0.6734826    0.1150109 Pasteur #> 88   0.165686549    0.3391326           0.6734826    0.1150109 Pasteur #> 89   0.077307606    0.3391326           0.6734826    0.1150109 Pasteur #> 90  -0.077401396    0.3391326           0.6734826    0.1150109 Pasteur #> 91  -0.081863485    0.3391326           0.6734826    0.1150109 Pasteur #> 92   0.106748566    0.3391326           0.6734826    0.1150109 Pasteur #> 93  -0.211593616    0.3391326           0.6734826    0.1150109 Pasteur #> 94  -0.926665153    0.3391326           0.6734826    0.1150109 Pasteur #> 95  -0.739484382    0.3391326           0.6734826    0.1150109 Pasteur #> 96   0.570387167    0.3391326           0.6734826    0.1150109 Pasteur #> 97  -0.913642554    0.3391326           0.6734826    0.1150109 Pasteur #> 98   0.547484887    0.3391326           0.6734826    0.1150109 Pasteur #> 99  -0.602850599    0.3391326           0.6734826    0.1150109 Pasteur #> 100  0.225794270    0.3391326           0.6734826    0.1150109 Pasteur #> 101  0.620447015    0.3391326           0.6734826    0.1150109 Pasteur #> 102  0.158885005    0.3391326           0.6734826    0.1150109 Pasteur #> 103 -0.127938344    0.3391326           0.6734826    0.1150109 Pasteur #> 104 -0.420347455    0.3391326           0.6734826    0.1150109 Pasteur #> 105  1.327978307    0.3391326           0.6734826    0.1150109 Pasteur #> 106  0.181843348    0.3391326           0.6734826    0.1150109 Pasteur #> 107 -0.148932224    0.3391326           0.6734826    0.1150109 Pasteur #> 108  0.612373626    0.3391326           0.6734826    0.1150109 Pasteur #> 109 -0.066558798    0.3391326           0.6734826    0.1150109 Pasteur #> 110 -0.420880619    0.3391326           0.6734826    0.1150109 Pasteur #> 111  1.127036295    0.3391326           0.6734826    0.1150109 Pasteur #> 112  0.237591068    0.3391326           0.6734826    0.1150109 Pasteur #> 113  0.853758689    0.3391326           0.6734826    0.1150109 Pasteur #> 114 -0.143618023    0.3391326           0.6734826    0.1150109 Pasteur #> 115  0.475206679    0.3391326           0.6734826    0.1150109 Pasteur #> 116 -0.670554590    0.3391326           0.6734826    0.1150109 Pasteur #> 117  0.022672257    0.3391326           0.6734826    0.1150109 Pasteur #> 118  0.302002707    0.3391326           0.6734826    0.1150109 Pasteur #> 119  0.151392125    0.3391326           0.6734826    0.1150109 Pasteur #> 120 -0.475300449    0.3391326           0.6734826    0.1150109 Pasteur #> 121 -0.346740056    0.3391326           0.6734826    0.1150109 Pasteur #> 122 -0.078888759    0.3391326           0.6734826    0.1150109 Pasteur #> 123  1.197237913    0.3391326           0.6734826    0.1150109 Pasteur #> 124  0.539243306    0.3391326           0.6734826    0.1150109 Pasteur #> 125  0.867258388    0.3391326           0.6734826    0.1150109 Pasteur #> 126  0.592287901    0.3391326           0.6734826    0.1150109 Pasteur #> 127 -0.500540901    0.3391326           0.6734826    0.1150109 Pasteur #> 128 -0.361193954    0.3391326           0.6734826    0.1150109 Pasteur #> 129  0.626883588    0.3391326           0.6734826    0.1150109 Pasteur #> 130 -0.437514518    0.3391326           0.6734826    0.1150109 Pasteur #> 131  0.695972854    0.3391326           0.6734826    0.1150109 Pasteur #> 132  0.424715775    0.3391326           0.6734826    0.1150109 Pasteur #> 133 -0.203725744    0.3391326           0.6734826    0.1150109 Pasteur #> 134 -0.441499507    0.3391326           0.6734826    0.1150109 Pasteur #> 135  0.735619838    0.3391326           0.6734826    0.1150109 Pasteur #> 136  0.783874697    0.3391326           0.6734826    0.1150109 Pasteur #> 137  0.565709540    0.3391326           0.6734826    0.1150109 Pasteur #> 138  0.258425494    0.3391326           0.6734826    0.1150109 Pasteur #> 139  0.861093397    0.3391326           0.6734826    0.1150109 Pasteur #> 140 -0.059757233    0.3391326           0.6734826    0.1150109 Pasteur #> 141 -0.920340689    0.3391326           0.6734826    0.1150109 Pasteur #> 142  0.845629236    0.3391326           0.6734826    0.1150109 Pasteur #> 143  1.227427574    0.3391326           0.6734826    0.1150109 Pasteur #> 144  1.054223601    0.3391326           0.6734826    0.1150109 Pasteur #> 145 -1.246596805    0.3391326           0.6734826    0.1150109 Pasteur #> 146 -0.473120468    0.3391326           0.6734826    0.1150109 Pasteur #> 147 -0.560171503    0.3391326           0.6734826    0.1150109 Pasteur #> 148 -0.365394462    0.3391326           0.6734826    0.1150109 Pasteur #> 149  0.084744422    0.3391326           0.6734826    0.1150109 Pasteur #> 150  0.910676146    0.3391326           0.6734826    0.1150109 Pasteur #> 151  1.094189533    0.3391326           0.6734826    0.1150109 Pasteur #> 152 -0.013149231    0.3391326           0.6734826    0.1150109 Pasteur #> 153 -0.166472976    0.3391326           0.6734826    0.1150109 Pasteur #> 154  0.008695459    0.3391326           0.6734826    0.1150109 Pasteur #> 155 -0.094989494    0.3391326           0.6734826    0.1150109 Pasteur #> 156 -0.457123143    0.3391326           0.6734826    0.1150109 Pasteur #>  #> $`Grant-White` #>        fs_visual fs_visual_se visual_by_fs_visual ev_fs_visual      school #> 1   -0.915287109     0.311828           0.6990509   0.09723667 Grant-White #> 2    0.035963597     0.311828           0.6990509   0.09723667 Grant-White #> 3    0.355636604     0.311828           0.6990509   0.09723667 Grant-White #> 4   -0.387353871     0.311828           0.6990509   0.09723667 Grant-White #> 5   -0.622393942     0.311828           0.6990509   0.09723667 Grant-White #> 6    0.195944561     0.311828           0.6990509   0.09723667 Grant-White #> 7    1.353023831     0.311828           0.6990509   0.09723667 Grant-White #> 8   -0.341506254     0.311828           0.6990509   0.09723667 Grant-White #> 9   -0.199493575     0.311828           0.6990509   0.09723667 Grant-White #> 10  -0.689869149     0.311828           0.6990509   0.09723667 Grant-White #> 11  -0.463929554     0.311828           0.6990509   0.09723667 Grant-White #> 12  -0.423001505     0.311828           0.6990509   0.09723667 Grant-White #> 13   0.279743296     0.311828           0.6990509   0.09723667 Grant-White #> 14  -0.916908219     0.311828           0.6990509   0.09723667 Grant-White #> 15   0.589344501     0.311828           0.6990509   0.09723667 Grant-White #> 16   0.191474701     0.311828           0.6990509   0.09723667 Grant-White #> 17   0.935275715     0.311828           0.6990509   0.09723667 Grant-White #> 18   0.393715904     0.311828           0.6990509   0.09723667 Grant-White #> 19   0.086569994     0.311828           0.6990509   0.09723667 Grant-White #> 20   0.555606898     0.311828           0.6990509   0.09723667 Grant-White #> 21  -0.558217193     0.311828           0.6990509   0.09723667 Grant-White #> 22   0.766715894     0.311828           0.6990509   0.09723667 Grant-White #> 23   0.115548801     0.311828           0.6990509   0.09723667 Grant-White #> 24   0.901249191     0.311828           0.6990509   0.09723667 Grant-White #> 25   0.174316971     0.311828           0.6990509   0.09723667 Grant-White #> 26  -0.078980322     0.311828           0.6990509   0.09723667 Grant-White #> 27  -0.581882977     0.311828           0.6990509   0.09723667 Grant-White #> 28  -0.661179262     0.311828           0.6990509   0.09723667 Grant-White #> 29  -0.245341176     0.311828           0.6990509   0.09723667 Grant-White #> 30  -0.195801662     0.311828           0.6990509   0.09723667 Grant-White #> 31  -0.281221524     0.311828           0.6990509   0.09723667 Grant-White #> 32  -0.293909378     0.311828           0.6990509   0.09723667 Grant-White #> 33  -0.604192958     0.311828           0.6990509   0.09723667 Grant-White #> 34  -0.738437335     0.311828           0.6990509   0.09723667 Grant-White #> 35  -0.304109345     0.311828           0.6990509   0.09723667 Grant-White #> 36   0.104931733     0.311828           0.6990509   0.09723667 Grant-White #> 37  -0.025781487     0.311828           0.6990509   0.09723667 Grant-White #> 38  -0.897318824     0.311828           0.6990509   0.09723667 Grant-White #> 39  -0.892560027     0.311828           0.6990509   0.09723667 Grant-White #> 40  -0.078402465     0.311828           0.6990509   0.09723667 Grant-White #> 41  -0.379063934     0.311828           0.6990509   0.09723667 Grant-White #> 42  -0.324926380     0.311828           0.6990509   0.09723667 Grant-White #> 43  -0.684299797     0.311828           0.6990509   0.09723667 Grant-White #> 44  -0.304109345     0.311828           0.6990509   0.09723667 Grant-White #> 45   0.622793169     0.311828           0.6990509   0.09723667 Grant-White #> 46  -0.152835419     0.311828           0.6990509   0.09723667 Grant-White #> 47  -0.421902013     0.311828           0.6990509   0.09723667 Grant-White #> 48  -0.060883872     0.311828           0.6990509   0.09723667 Grant-White #> 49  -0.303298790     0.311828           0.6990509   0.09723667 Grant-White #> 50   0.425021826     0.311828           0.6990509   0.09723667 Grant-White #> 51   0.131478875     0.311828           0.6990509   0.09723667 Grant-White #> 52  -0.914998172     0.311828           0.6990509   0.09723667 Grant-White #> 53   0.324226132     0.311828           0.6990509   0.09723667 Grant-White #> 54  -0.086170767     0.311828           0.6990509   0.09723667 Grant-White #> 55   0.428424818     0.311828           0.6990509   0.09723667 Grant-White #> 56   0.188465179     0.311828           0.6990509   0.09723667 Grant-White #> 57  -0.306958111     0.311828           0.6990509   0.09723667 Grant-White #> 58   0.581736955     0.311828           0.6990509   0.09723667 Grant-White #> 59  -0.743485051     0.311828           0.6990509   0.09723667 Grant-White #> 60   0.263580524     0.311828           0.6990509   0.09723667 Grant-White #> 61   0.178786831     0.311828           0.6990509   0.09723667 Grant-White #> 62  -0.064832113     0.311828           0.6990509   0.09723667 Grant-White #> 63   0.499976414     0.311828           0.6990509   0.09723667 Grant-White #> 64  -0.092839593     0.311828           0.6990509   0.09723667 Grant-White #> 65  -0.263702931     0.311828           0.6990509   0.09723667 Grant-White #> 66  -0.983966325     0.311828           0.6990509   0.09723667 Grant-White #> 67   1.434912536     0.311828           0.6990509   0.09723667 Grant-White #> 68  -1.037582228     0.311828           0.6990509   0.09723667 Grant-White #> 69  -0.047569849     0.311828           0.6990509   0.09723667 Grant-White #> 70   1.084767775     0.311828           0.6990509   0.09723667 Grant-White #> 71   0.092011181     0.311828           0.6990509   0.09723667 Grant-White #> 72  -0.562687052     0.311828           0.6990509   0.09723667 Grant-White #> 73  -0.304919900     0.311828           0.6990509   0.09723667 Grant-White #> 74   1.038920175     0.311828           0.6990509   0.09723667 Grant-White #> 75  -0.789854287     0.311828           0.6990509   0.09723667 Grant-White #> 76  -0.602282928     0.311828           0.6990509   0.09723667 Grant-White #> 77  -0.894630830     0.311828           0.6990509   0.09723667 Grant-White #> 78   0.532614527     0.311828           0.6990509   0.09723667 Grant-White #> 79  -0.548955945     0.311828           0.6990509   0.09723667 Grant-White #> 80  -0.221514635     0.311828           0.6990509   0.09723667 Grant-White #> 81  -0.095849115     0.311828           0.6990509   0.09723667 Grant-White #> 82  -0.122235502     0.311828           0.6990509   0.09723667 Grant-White #> 83   0.892276864     0.311828           0.6990509   0.09723667 Grant-White #> 84   0.328439663     0.311828           0.6990509   0.09723667 Grant-White #> 85   1.217391042     0.311828           0.6990509   0.09723667 Grant-White #> 86   0.574513902     0.311828           0.6990509   0.09723667 Grant-White #> 87   0.160168762     0.311828           0.6990509   0.09723667 Grant-White #> 88   0.654909662     0.311828           0.6990509   0.09723667 Grant-White #> 89  -0.509777155     0.311828           0.6990509   0.09723667 Grant-White #> 90   1.201493560     0.311828           0.6990509   0.09723667 Grant-White #> 91   0.584874625     0.311828           0.6990509   0.09723667 Grant-White #> 92   0.075142371     0.311828           0.6990509   0.09723667 Grant-White #> 93   0.550976266     0.311828           0.6990509   0.09723667 Grant-White #> 94  -0.886308302     0.311828           0.6990509   0.09723667 Grant-White #> 95   0.552075757     0.311828           0.6990509   0.09723667 Grant-White #> 96   1.415972940     0.311828           0.6990509   0.09723667 Grant-White #> 97   0.298650301     0.311828           0.6990509   0.09723667 Grant-White #> 98  -0.143028906     0.311828           0.6990509   0.09723667 Grant-White #> 99   0.245195154     0.311828           0.6990509   0.09723667 Grant-White #> 100  0.247072593     0.311828           0.6990509   0.09723667 Grant-White #> 101  0.817322291     0.311828           0.6990509   0.09723667 Grant-White #> 102  0.651771976     0.311828           0.6990509   0.09723667 Grant-White #> 103  1.338875623     0.311828           0.6990509   0.09723667 Grant-White #> 104 -1.160005528     0.311828           0.6990509   0.09723667 Grant-White #> 105  0.163306449     0.311828           0.6990509   0.09723667 Grant-White #> 106 -0.387353871     0.311828           0.6990509   0.09723667 Grant-White #> 107 -0.517128372     0.311828           0.6990509   0.09723667 Grant-White #> 108  0.065103160     0.311828           0.6990509   0.09723667 Grant-White #> 109 -0.115438510     0.311828           0.6990509   0.09723667 Grant-White #> 110  0.094049376     0.311828           0.6990509   0.09723667 Grant-White #> 111  0.396725409     0.311828           0.6990509   0.09723667 Grant-White #> 112  0.672356312     0.311828           0.6990509   0.09723667 Grant-White #> 113  1.165974090     0.311828           0.6990509   0.09723667 Grant-White #> 114 -0.483518949     0.311828           0.6990509   0.09723667 Grant-White #> 115  0.035024877     0.311828           0.6990509   0.09723667 Grant-White #> 116  0.741974248     0.311828           0.6990509   0.09723667 Grant-White #> 117 -0.170386603     0.311828           0.6990509   0.09723667 Grant-White #> 118 -0.205873481     0.311828           0.6990509   0.09723667 Grant-White #> 119  0.714777307     0.311828           0.6990509   0.09723667 Grant-White #> 120 -0.620772831     0.311828           0.6990509   0.09723667 Grant-White #> 121 -0.313626938     0.311828           0.6990509   0.09723667 Grant-White #> 122 -0.157466035     0.311828           0.6990509   0.09723667 Grant-White #> 123  0.118269386     0.311828           0.6990509   0.09723667 Grant-White #> 124  0.101111673     0.311828           0.6990509   0.09723667 Grant-White #> 125 -0.625403463     0.311828           0.6990509   0.09723667 Grant-White #> 126  0.486638761     0.311828           0.6990509   0.09723667 Grant-White #> 127 -0.178676540     0.311828           0.6990509   0.09723667 Grant-White #> 128  0.274013189     0.311828           0.6990509   0.09723667 Grant-White #> 129 -0.316347523     0.311828           0.6990509   0.09723667 Grant-White #> 130 -0.026752814     0.311828           0.6990509   0.09723667 Grant-White #> 131  0.245323318     0.311828           0.6990509   0.09723667 Grant-White #> 132 -0.356336853     0.311828           0.6990509   0.09723667 Grant-White #> 133 -0.581594057     0.311828           0.6990509   0.09723667 Grant-White #> 134  0.263002667     0.311828           0.6990509   0.09723667 Grant-White #> 135 -0.864680712     0.311828           0.6990509   0.09723667 Grant-White #> 136 -0.377964443     0.311828           0.6990509   0.09723667 Grant-White #> 137 -0.112717909     0.311828           0.6990509   0.09723667 Grant-White #> 138  0.114449326     0.311828           0.6990509   0.09723667 Grant-White #> 139  0.001287274     0.311828           0.6990509   0.09723667 Grant-White #> 140  0.597634438     0.311828           0.6990509   0.09723667 Grant-White #> 141 -0.252531637     0.311828           0.6990509   0.09723667 Grant-White #> 142 -0.472901881     0.311828           0.6990509   0.09723667 Grant-White #> 143 -0.187255397     0.311828           0.6990509   0.09723667 Grant-White #> 144 -0.542415283     0.311828           0.6990509   0.09723667 Grant-White #> 145  0.358774274     0.311828           0.6990509   0.09723667 Grant-White #>  #> attr(,\"fsT\") #> attr(,\"fsT\")$Pasteur #>           fs_visual #> fs_visual 0.1150109 #>  #> attr(,\"fsT\")$`Grant-White` #>            fs_visual #> fs_visual 0.09723667 #>  #> attr(,\"fsL\") #> attr(,\"fsL\")$Pasteur #>              visual #> fs_visual 0.6734826 #>  #> attr(,\"fsL\")$`Grant-White` #>              visual #> fs_visual 0.6990509 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #> fs_visual  #>         0  #>  #> attr(,\"fsb\")$`Grant-White` #> fs_visual  #>         0  #>  #> attr(,\"scoring_matrix\") #> attr(,\"scoring_matrix\")$Pasteur #>             [,1]     [,2]      [,3] #> visual 0.1957873 0.109906 0.3316264 #>  #> attr(,\"scoring_matrix\")$`Grant-White` #>             [,1]      [,2]      [,3] #> visual 0.1624126 0.1458328 0.3515006 #>  # Or without the model get_fs(HolzingerSwineford1939[c(\"school\", \"x4\", \"x5\", \"x6\")],        group = \"school\") #> $Pasteur #>             fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1  school #> 1    0.3074500370 0.2999315   0.8833584 0.08995892 Pasteur #> 2   -0.7746062892 0.2999315   0.8833584 0.08995892 Pasteur #> 3   -1.5843019574 0.2999315   0.8833584 0.08995892 Pasteur #> 4    0.2739579120 0.2999315   0.8833584 0.08995892 Pasteur #> 5    0.1440153923 0.2999315   0.8833584 0.08995892 Pasteur #> 6   -1.0440895948 0.2999315   0.8833584 0.08995892 Pasteur #> 7    1.0507357396 0.2999315   0.8833584 0.08995892 Pasteur #> 8    0.1041882698 0.2999315   0.8833584 0.08995892 Pasteur #> 9    0.7750146375 0.2999315   0.8833584 0.08995892 Pasteur #> 10   0.4822117444 0.2999315   0.8833584 0.08995892 Pasteur #> 11  -0.4511886490 0.2999315   0.8833584 0.08995892 Pasteur #> 12   0.3522691973 0.2999315   0.8833584 0.08995892 Pasteur #> 13   0.0657041070 0.2999315   0.8833584 0.08995892 Pasteur #> 14   0.3259750264 0.2999315   0.8833584 0.08995892 Pasteur #> 15   1.3008341323 0.2999315   0.8833584 0.08995892 Pasteur #> 16  -0.2804588573 0.2999315   0.8833584 0.08995892 Pasteur #> 17  -0.3604017581 0.2999315   0.8833584 0.08995892 Pasteur #> 18  -0.7502293722 0.2999315   0.8833584 0.08995892 Pasteur #> 19   1.2600468631 0.2999315   0.8833584 0.08995892 Pasteur #> 20   0.2874908636 0.2999315   0.8833584 0.08995892 Pasteur #> 21  -1.1394826729 0.2999315   0.8833584 0.08995892 Pasteur #> 22  -0.3791151940 0.2999315   0.8833584 0.08995892 Pasteur #> 23   1.0444979094 0.2999315   0.8833584 0.08995892 Pasteur #> 24  -0.6248930150 0.2999315   0.8833584 0.08995892 Pasteur #> 25  -0.3689426673 0.2999315   0.8833584 0.08995892 Pasteur #> 26  -0.5663524842 0.2999315   0.8833584 0.08995892 Pasteur #> 27  -0.9568515617 0.2999315   0.8833584 0.08995892 Pasteur #> 28  -0.8137619455 0.2999315   0.8833584 0.08995892 Pasteur #> 29  -0.5028199109 0.2999315   0.8833584 0.08995892 Pasteur #> 30  -0.3054100713 0.2999315   0.8833584 0.08995892 Pasteur #> 31  -0.3728773637 0.2999315   0.8833584 0.08995892 Pasteur #> 32  -0.8529175744 0.2999315   0.8833584 0.08995892 Pasteur #> 33   0.4101382620 0.2999315   0.8833584 0.08995892 Pasteur #> 34   0.1848026386 0.2999315   0.8833584 0.08995892 Pasteur #> 35  -0.9680814159 0.2999315   0.8833584 0.08995892 Pasteur #> 36  -0.1436070439 0.2999315   0.8833584 0.08995892 Pasteur #> 37  -0.0126071782 0.2999315   0.8833584 0.08995892 Pasteur #> 38  -0.3531066368 0.2999315   0.8833584 0.08995892 Pasteur #> 39   1.0665717701 0.2999315   0.8833584 0.08995892 Pasteur #> 40   0.7993915544 0.2999315   0.8833584 0.08995892 Pasteur #> 41   0.1110975387 0.2999315   0.8833584 0.08995892 Pasteur #> 42  -0.2735495637 0.2999315   0.8833584 0.08995892 Pasteur #> 43  -0.7167372327 0.2999315   0.8833584 0.08995892 Pasteur #> 44  -0.6594424814 0.2999315   0.8833584 0.08995892 Pasteur #> 45   0.9507364688 0.2999315   0.8833584 0.08995892 Pasteur #> 46  -0.0478281107 0.2999315   0.8833584 0.08995892 Pasteur #> 47  -1.7573348450 0.2999315   0.8833584 0.08995892 Pasteur #> 48  -0.4620326417 0.2999315   0.8833584 0.08995892 Pasteur #> 49  -1.0305566597 0.2999315   0.8833584 0.08995892 Pasteur #> 50   0.7618675383 0.2999315   0.8833584 0.08995892 Pasteur #> 51  -1.3414986833 0.2999315   0.8833584 0.08995892 Pasteur #> 52  -0.0761397743 0.2999315   0.8833584 0.08995892 Pasteur #> 53  -1.8231705136 0.2999315   0.8833584 0.08995892 Pasteur #> 54   0.0094666323 0.2999315   0.8833584 0.08995892 Pasteur #> 55   0.0436302463 0.2999315   0.8833584 0.08995892 Pasteur #> 56  -0.0001315726 0.2999315   0.8833584 0.08995892 Pasteur #> 57  -0.6571393750 0.2999315   0.8833584 0.08995892 Pasteur #> 58   0.6121542642 0.2999315   0.8833584 0.08995892 Pasteur #> 59  -1.0957208458 0.2999315   0.8833584 0.08995892 Pasteur #> 60  -0.9289257761 0.2999315   0.8833584 0.08995892 Pasteur #> 61   0.9553426588 0.2999315   0.8833584 0.08995892 Pasteur #> 62   0.3647448029 0.2999315   0.8833584 0.08995892 Pasteur #> 63  -1.1003270330 0.2999315   0.8833584 0.08995892 Pasteur #> 64   0.4259742697 0.2999315   0.8833584 0.08995892 Pasteur #> 65   0.1689666035 0.2999315   0.8833584 0.08995892 Pasteur #> 66   0.6586050419 0.2999315   0.8833584 0.08995892 Pasteur #> 67  -0.2952375720 0.2999315   0.8833584 0.08995892 Pasteur #> 68  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 69  -0.5613604418 0.2999315   0.8833584 0.08995892 Pasteur #> 70   0.5632119383 0.2999315   0.8833584 0.08995892 Pasteur #> 71  -0.7769093955 0.2999315   0.8833584 0.08995892 Pasteur #> 72  -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 73  -0.7808440920 0.2999315   0.8833584 0.08995892 Pasteur #> 74   0.9270309952 0.2999315   0.8833584 0.08995892 Pasteur #> 75  -0.5426470306 0.2999315   0.8833584 0.08995892 Pasteur #> 76  -1.1065648385 0.2999315   0.8833584 0.08995892 Pasteur #> 77   0.6233841094 0.2999315   0.8833584 0.08995892 Pasteur #> 78  -1.7010974136 0.2999315   0.8833584 0.08995892 Pasteur #> 79  -0.0013773330 0.2999315   0.8833584 0.08995892 Pasteur #> 80  -1.2772946275 0.2999315   0.8833584 0.08995892 Pasteur #> 81  -1.0344913561 0.2999315   0.8833584 0.08995892 Pasteur #> 82   0.4345151789 0.2999315   0.8833584 0.08995892 Pasteur #> 83  -1.5280645151 0.2999315   0.8833584 0.08995892 Pasteur #> 84  -0.6101143231 0.2999315   0.8833584 0.08995892 Pasteur #> 85  -1.5122284832 0.2999315   0.8833584 0.08995892 Pasteur #> 86   1.2011204798 0.2999315   0.8833584 0.08995892 Pasteur #> 87  -0.3258523027 0.2999315   0.8833584 0.08995892 Pasteur #> 88   0.6687775411 0.2999315   0.8833584 0.08995892 Pasteur #> 89  -1.6382363147 0.2999315   0.8833584 0.08995892 Pasteur #> 90   0.3062042720 0.2999315   0.8833584 0.08995892 Pasteur #> 91  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 92  -0.6798846937 0.2999315   0.8833584 0.08995892 Pasteur #> 93  -1.1865077366 0.2999315   0.8833584 0.08995892 Pasteur #> 94  -0.5011882981 0.2999315   0.8833584 0.08995892 Pasteur #> 95   0.7362448336 0.2999315   0.8833584 0.08995892 Pasteur #> 96   0.4934415622 0.2999315   0.8833584 0.08995892 Pasteur #> 97   0.9661866515 0.2999315   0.8833584 0.08995892 Pasteur #> 98   1.7212764661 0.2999315   0.8833584 0.08995892 Pasteur #> 99  -0.8199997483 0.2999315   0.8833584 0.08995892 Pasteur #> 100  0.7369163271 0.2999315   0.8833584 0.08995892 Pasteur #> 101 -0.5403439270 0.2999315   0.8833584 0.08995892 Pasteur #> 102  0.0525570079 0.2999315   0.8833584 0.08995892 Pasteur #> 103  0.4973762860 0.2999315   0.8833584 0.08995892 Pasteur #> 104  0.5434412113 0.2999315   0.8833584 0.08995892 Pasteur #> 105  1.4015048920 0.2999315   0.8833584 0.08995892 Pasteur #> 106  0.5338429790 0.2999315   0.8833584 0.08995892 Pasteur #> 107  1.5005470281 0.2999315   0.8833584 0.08995892 Pasteur #> 108 -0.4353526184 0.2999315   0.8833584 0.08995892 Pasteur #> 109  1.7269399974 0.2999315   0.8833584 0.08995892 Pasteur #> 110 -0.1863115671 0.2999315   0.8833584 0.08995892 Pasteur #> 111  0.7431541299 0.2999315   0.8833584 0.08995892 Pasteur #> 112  0.3345159128 0.2999315   0.8833584 0.08995892 Pasteur #> 113  0.3111963144 0.2999315   0.8833584 0.08995892 Pasteur #> 114  0.6750153713 0.2999315   0.8833584 0.08995892 Pasteur #> 115 -1.3822859470 0.2999315   0.8833584 0.08995892 Pasteur #> 116  0.4299090164 0.2999315   0.8833584 0.08995892 Pasteur #> 117  0.4368182853 0.2999315   0.8833584 0.08995892 Pasteur #> 118 -0.6334339242 0.2999315   0.8833584 0.08995892 Pasteur #> 119 -0.9153928519 0.2999315   0.8833584 0.08995892 Pasteur #> 120 -0.2662544424 0.2999315   0.8833584 0.08995892 Pasteur #> 121 -0.1238362896 0.2999315   0.8833584 0.08995892 Pasteur #> 122 -0.1987871727 0.2999315   0.8833584 0.08995892 Pasteur #> 123  1.5676284956 0.2999315   0.8833584 0.08995892 Pasteur #> 124 -0.2906313821 0.2999315   0.8833584 0.08995892 Pasteur #> 125  0.7125393874 0.2999315   0.8833584 0.08995892 Pasteur #> 126  0.1324998831 0.2999315   0.8833584 0.08995892 Pasteur #> 127  1.1488177518 0.2999315   0.8833584 0.08995892 Pasteur #> 128  0.5559168169 0.2999315   0.8833584 0.08995892 Pasteur #> 129  0.8572606191 0.2999315   0.8833584 0.08995892 Pasteur #> 130 -0.9789254060 0.2999315   0.8833584 0.08995892 Pasteur #> 131  1.4416206448 0.2999315   0.8833584 0.08995892 Pasteur #> 132  0.4542859333 0.2999315   0.8833584 0.08995892 Pasteur #> 133 -1.3845890506 0.2999315   0.8833584 0.08995892 Pasteur #> 134 -0.2883282757 0.2999315   0.8833584 0.08995892 Pasteur #> 135  0.4430560881 0.2999315   0.8833584 0.08995892 Pasteur #> 136  1.2089899229 0.2999315   0.8833584 0.08995892 Pasteur #> 137  1.1942112109 0.2999315   0.8833584 0.08995892 Pasteur #> 138  0.6013102486 0.2999315   0.8833584 0.08995892 Pasteur #> 139  0.2371053667 0.2999315   0.8833584 0.08995892 Pasteur #> 140  1.0053422804 0.2999315   0.8833584 0.08995892 Pasteur #> 141  0.8095640810 0.2999315   0.8833584 0.08995892 Pasteur #> 142 -1.4408264861 0.2999315   0.8833584 0.08995892 Pasteur #> 143  1.3821199900 0.2999315   0.8833584 0.08995892 Pasteur #> 144  2.7284791267 0.2999315   0.8833584 0.08995892 Pasteur #> 145  0.0123440494 0.2999315   0.8833584 0.08995892 Pasteur #> 146  0.8277032180 0.2999315   0.8833584 0.08995892 Pasteur #> 147  0.7862444827 0.2999315   0.8833584 0.08995892 Pasteur #> 148 -1.1325734026 0.2999315   0.8833584 0.08995892 Pasteur #> 149  1.7660956264 0.2999315   0.8833584 0.08995892 Pasteur #> 150 -0.3712457509 0.2999315   0.8833584 0.08995892 Pasteur #> 151  1.8944065607 0.2999315   0.8833584 0.08995892 Pasteur #> 152  0.6098511578 0.2999315   0.8833584 0.08995892 Pasteur #> 153  0.2654170028 0.2999315   0.8833584 0.08995892 Pasteur #> 154 -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 155 -0.6005160981 0.2999315   0.8833584 0.08995892 Pasteur #> 156  0.3562038937 0.2999315   0.8833584 0.08995892 Pasteur #>  #> $`Grant-White` #>           fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1      school #> 1   -0.39525603 0.3152173   0.8801489 0.09936192 Grant-White #> 2   -0.63397248 0.3152173   0.8801489 0.09936192 Grant-White #> 3    0.20062403 0.3152173   0.8801489 0.09936192 Grant-White #> 4   -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 5    0.43519197 0.3152173   0.8801489 0.09936192 Grant-White #> 6    0.31151246 0.3152173   0.8801489 0.09936192 Grant-White #> 7    2.15612913 0.3152173   0.8801489 0.09936192 Grant-White #> 8   -0.29014192 0.3152173   0.8801489 0.09936192 Grant-White #> 9   -0.08369303 0.3152173   0.8801489 0.09936192 Grant-White #> 10  -0.01807396 0.3152173   0.8801489 0.09936192 Grant-White #> 11  -0.34820234 0.3152173   0.8801489 0.09936192 Grant-White #> 12  -2.10215974 0.3152173   0.8801489 0.09936192 Grant-White #> 13  -0.64552119 0.3152173   0.8801489 0.09936192 Grant-White #> 14  -1.46155228 0.3152173   0.8801489 0.09936192 Grant-White #> 15   1.02620880 0.3152173   0.8801489 0.09936192 Grant-White #> 16  -1.05064956 0.3152173   0.8801489 0.09936192 Grant-White #> 17   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 18   0.95822548 0.3152173   0.8801489 0.09936192 Grant-White #> 19  -0.25355303 0.3152173   0.8801489 0.09936192 Grant-White #> 20   1.42141427 0.3152173   0.8801489 0.09936192 Grant-White #> 21  -0.95400523 0.3152173   0.8801489 0.09936192 Grant-White #> 22   1.00295292 0.3152173   0.8801489 0.09936192 Grant-White #> 23   1.21841354 0.3152173   0.8801489 0.09936192 Grant-White #> 24  -1.24987101 0.3152173   0.8801489 0.09936192 Grant-White #> 25  -0.51984663 0.3152173   0.8801489 0.09936192 Grant-White #> 26  -0.04710419 0.3152173   0.8801489 0.09936192 Grant-White #> 27   0.43934048 0.3152173   0.8801489 0.09936192 Grant-White #> 28  -1.03117301 0.3152173   0.8801489 0.09936192 Grant-White #> 29  -0.95022593 0.3152173   0.8801489 0.09936192 Grant-White #> 30  -0.11417634 0.3152173   0.8801489 0.09936192 Grant-White #> 31  -0.40048841 0.3152173   0.8801489 0.09936192 Grant-White #> 32   0.10506359 0.3152173   0.8801489 0.09936192 Grant-White #> 33  -0.33541128 0.3152173   0.8801489 0.09936192 Grant-White #> 34  -1.55565961 0.3152173   0.8801489 0.09936192 Grant-White #> 35  -0.84420068 0.3152173   0.8801489 0.09936192 Grant-White #> 36   0.07802839 0.3152173   0.8801489 0.09936192 Grant-White #> 37   0.20116597 0.3152173   0.8801489 0.09936192 Grant-White #> 38  -2.52639546 0.3152173   0.8801489 0.09936192 Grant-White #> 39  -0.69149096 0.3152173   0.8801489 0.09936192 Grant-White #> 40   2.02343787 0.3152173   0.8801489 0.09936192 Grant-White #> 41   0.97338078 0.3152173   0.8801489 0.09936192 Grant-White #> 42   0.42040588 0.3152173   0.8801489 0.09936192 Grant-White #> 43  -0.92605892 0.3152173   0.8801489 0.09936192 Grant-White #> 44  -0.56690032 0.3152173   0.8801489 0.09936192 Grant-White #> 45   0.12021889 0.3152173   0.8801489 0.09936192 Grant-White #> 46   0.62108043 0.3152173   0.8801489 0.09936192 Grant-White #> 47  -1.34219405 0.3152173   0.8801489 0.09936192 Grant-White #> 48   0.16258210 0.3152173   0.8801489 0.09936192 Grant-White #> 49  -0.03231810 0.3152173   0.8801489 0.09936192 Grant-White #> 50   0.24444031 0.3152173   0.8801489 0.09936192 Grant-White #> 51  -0.74431900 0.3152173   0.8801489 0.09936192 Grant-White #> 52  -0.43798842 0.3152173   0.8801489 0.09936192 Grant-White #> 53  -1.85297845 0.3152173   0.8801489 0.09936192 Grant-White #> 54  -0.82563527 0.3152173   0.8801489 0.09936192 Grant-White #> 55   1.20039010 0.3152173   0.8801489 0.09936192 Grant-White #> 56  -0.33287433 0.3152173   0.8801489 0.09936192 Grant-White #> 57   0.01996797 0.3152173   0.8801489 0.09936192 Grant-White #> 58   1.67582800 0.3152173   0.8801489 0.09936192 Grant-White #> 59  -0.71906808 0.3152173   0.8801489 0.09936192 Grant-White #> 60  -0.20504632 0.3152173   0.8801489 0.09936192 Grant-White #> 61   1.96214007 0.3152173   0.8801489 0.09936192 Grant-White #> 62  -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 63  -0.35343474 0.3152173   0.8801489 0.09936192 Grant-White #> 64  -1.95809255 0.3152173   0.8801489 0.09936192 Grant-White #> 65  -1.36021750 0.3152173   0.8801489 0.09936192 Grant-White #> 66   0.08595623 0.3152173   0.8801489 0.09936192 Grant-White #> 67  -0.23407652 0.3152173   0.8801489 0.09936192 Grant-White #> 68   0.67805696 0.3152173   0.8801489 0.09936192 Grant-White #> 69  -0.42951863 0.3152173   0.8801489 0.09936192 Grant-White #> 70  -0.69203290 0.3152173   0.8801489 0.09936192 Grant-White #> 71  -0.71583072 0.3152173   0.8801489 0.09936192 Grant-White #> 72  -0.19603459 0.3152173   0.8801489 0.09936192 Grant-White #> 73  -0.36767888 0.3152173   0.8801489 0.09936192 Grant-White #> 74   1.77425660 0.3152173   0.8801489 0.09936192 Grant-White #> 75  -0.67924187 0.3152173   0.8801489 0.09936192 Grant-White #> 76   0.07603337 0.3152173   0.8801489 0.09936192 Grant-White #> 77   1.49895127 0.3152173   0.8801489 0.09936192 Grant-White #> 78  -0.78813525 0.3152173   0.8801489 0.09936192 Grant-White #> 79  -1.35643816 0.3152173   0.8801489 0.09936192 Grant-White #> 80  -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 81   1.10806701 0.3152173   0.8801489 0.09936192 Grant-White #> 82   1.04768034 0.3152173   0.8801489 0.09936192 Grant-White #> 83   1.02242947 0.3152173   0.8801489 0.09936192 Grant-White #> 84   0.38236395 0.3152173   0.8801489 0.09936192 Grant-White #> 85   0.56447306 0.3152173   0.8801489 0.09936192 Grant-White #> 86   0.78803426 0.3152173   0.8801489 0.09936192 Grant-White #> 87   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 88   0.40383552 0.3152173   0.8801489 0.09936192 Grant-White #> 89  -0.44376277 0.3152173   0.8801489 0.09936192 Grant-White #> 90   0.08126577 0.3152173   0.8801489 0.09936192 Grant-White #> 91   0.14833793 0.3152173   0.8801489 0.09936192 Grant-White #> 92   0.53922219 0.3152173   0.8801489 0.09936192 Grant-White #> 93   0.53598481 0.3152173   0.8801489 0.09936192 Grant-White #> 94  -0.17024174 0.3152173   0.8801489 0.09936192 Grant-White #> 95   0.43610310 0.3152173   0.8801489 0.09936192 Grant-White #> 96   2.22843366 0.3152173   0.8801489 0.09936192 Grant-White #> 97   1.36480696 0.3152173   0.8801489 0.09936192 Grant-White #> 98   0.65135298 0.3152173   0.8801489 0.09936192 Grant-White #> 99   1.69439338 0.3152173   0.8801489 0.09936192 Grant-White #> 100 -0.15745068 0.3152173   0.8801489 0.09936192 Grant-White #> 101 -0.27680894 0.3152173   0.8801489 0.09936192 Grant-White #> 102  1.80473991 0.3152173   0.8801489 0.09936192 Grant-White #> 103 -0.03286005 0.3152173   0.8801489 0.09936192 Grant-White #> 104  0.21541008 0.3152173   0.8801489 0.09936192 Grant-White #> 105  0.63586648 0.3152173   0.8801489 0.09936192 Grant-White #> 106 -0.44122580 0.3152173   0.8801489 0.09936192 Grant-White #> 107  0.24389836 0.3152173   0.8801489 0.09936192 Grant-White #> 108  0.97392270 0.3152173   0.8801489 0.09936192 Grant-White #> 109  1.00619031 0.3152173   0.8801489 0.09936192 Grant-White #> 110  0.95967859 0.3152173   0.8801489 0.09936192 Grant-White #> 111  2.10529611 0.3152173   0.8801489 0.09936192 Grant-White #> 112  0.95012491 0.3152173   0.8801489 0.09936192 Grant-White #> 113  1.14033462 0.3152173   0.8801489 0.09936192 Grant-White #> 114 -0.41527450 0.3152173   0.8801489 0.09936192 Grant-White #> 115  0.50263334 0.3152173   0.8801489 0.09936192 Grant-White #> 116  0.00518188 0.3152173   0.8801489 0.09936192 Grant-White #> 117 -0.30961846 0.3152173   0.8801489 0.09936192 Grant-White #> 118 -0.38570232 0.3152173   0.8801489 0.09936192 Grant-White #> 119 -0.88747505 0.3152173   0.8801489 0.09936192 Grant-White #> 120 -1.11340047 0.3152173   0.8801489 0.09936192 Grant-White #> 121 -0.22830216 0.3152173   0.8801489 0.09936192 Grant-White #> 122  0.16781447 0.3152173   0.8801489 0.09936192 Grant-White #> 123 -1.16622845 0.3152173   0.8801489 0.09936192 Grant-White #> 124 -0.45277447 0.3152173   0.8801489 0.09936192 Grant-White #> 125 -0.69527031 0.3152173   0.8801489 0.09936192 Grant-White #> 126  1.16558552 0.3152173   0.8801489 0.09936192 Grant-White #> 127 -0.49081643 0.3152173   0.8801489 0.09936192 Grant-White #> 128  0.45412656 0.3152173   0.8801489 0.09936192 Grant-White #> 129 -0.75910506 0.3152173   0.8801489 0.09936192 Grant-White #> 130 -0.46232819 0.3152173   0.8801489 0.09936192 Grant-White #> 131  1.33631868 0.3152173   0.8801489 0.09936192 Grant-White #> 132 -0.78236094 0.3152173   0.8801489 0.09936192 Grant-White #> 133  0.11407532 0.3152173   0.8801489 0.09936192 Grant-White #> 134 -0.26111172 0.3152173   0.8801489 0.09936192 Grant-White #> 135 -0.58492377 0.3152173   0.8801489 0.09936192 Grant-White #> 136 -1.40872427 0.3152173   0.8801489 0.09936192 Grant-White #> 137 -0.24308825 0.3152173   0.8801489 0.09936192 Grant-White #> 138 -0.17601610 0.3152173   0.8801489 0.09936192 Grant-White #> 139 -0.74486092 0.3152173   0.8801489 0.09936192 Grant-White #> 140 -0.13419481 0.3152173   0.8801489 0.09936192 Grant-White #> 141 -0.74809830 0.3152173   0.8801489 0.09936192 Grant-White #> 142 -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 143  0.88737403 0.3152173   0.8801489 0.09936192 Grant-White #> 144 -0.05665784 0.3152173   0.8801489 0.09936192 Grant-White #> 145  0.58303847 0.3152173   0.8801489 0.09936192 Grant-White #>  #> attr(,\"fsT\") #> attr(,\"fsT\")$Pasteur #>            fs_f1 #> fs_f1 0.08995892 #>  #> attr(,\"fsT\")$`Grant-White` #>            fs_f1 #> fs_f1 0.09936192 #>  #> attr(,\"fsL\") #> attr(,\"fsL\")$Pasteur #>              f1 #> fs_f1 0.8833584 #>  #> attr(,\"fsL\")$`Grant-White` #>              f1 #> fs_f1 0.8801489 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #> fs_f1  #>     0  #>  #> attr(,\"fsb\")$`Grant-White` #> fs_f1  #>     0  #>  #> attr(,\"scoring_matrix\") #> attr(,\"scoring_matrix\")$Pasteur #>         [,1]      [,2]      [,3] #> f1 0.2280246 0.3381964 0.2740895 #>  #> attr(,\"scoring_matrix\")$`Grant-White` #>         [,1]      [,2]      [,3] #> f1 0.3580747 0.2682886 0.2662936 #>"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_int.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute interaction indicators for tspa() function — get_fs_int","title":"Compute interaction indicators for tspa() function — get_fs_int","text":"function computes product indicators corresponding standard errors according equation (3) Hsiao et al. (2021). double-mean-centering (DMC) strategy applied function, first-order indicators mean-centered first product indicators latent interaction term(s) mean-centered (Lin et al., 2010)","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_int.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute interaction indicators for tspa() function — get_fs_int","text":"","code":"get_fs_int(   dat,   fs_name,   se_fs,   loading_fs,   lat_var = rep_len(1, length.out = length(fs_name)),   model = NULL )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_int.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute interaction indicators for tspa() function — get_fs_int","text":"dat data frame containing first-order factor score indiactors standard error. fs_name vector indicating names factor scores se_fs vector indicating standard error factor scores loading_fs vector indicating model-implied loadings factor scores lat_var vector indicating latent variances model optional string specifying measurement model lavaan syntax.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_int.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute interaction indicators for tspa() function — get_fs_int","text":"data frame product indicators interaction terms, loadings standard errors.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_int.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute interaction indicators for tspa() function — get_fs_int","text":"Hsiao, Y.-Y., Kwok, O.-M., & Lai, M. H. C. (2021). Modeling measurement errors exogenous composites congeneric measures interaction models. Structural Equation Modeling: Multidisciplinary Journal, 28(2), 250–260. https://doi.org/10.1080/10705511.2020.1782206","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_int.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute interaction indicators for tspa() function — get_fs_int","text":"","code":"library(lavaan) fs1 <- get_fs(HolzingerSwineford1939,               model = \"visual  =~ x1 + x2 + x3\",               std.lv = TRUE) fs2 <- get_fs(HolzingerSwineford1939,               model = \"textual =~ x4 + x5 + x6\",               std.lv = TRUE) fs_dat <- cbind(fs1, fs2) fs_dat2 <- get_fs_int(fs_dat,   fs_name = c(\"fs_visual\", \"fs_textual\"),   se_fs = c(\"fs_visual_se\", \"fs_textual_se\"),   loading_fs = c(\"visual_by_fs_visual\",                  \"textual_by_fs_textual\") ) head(fs_dat2[c(\"fs_visual\", \"fs_textual\", \"fs_visual:fs_textual\",                \"fs_visual:fs_textual_se\", \"fs_visual:fs_textual_ld\")]) #>     fs_visual   fs_textual fs_visual:fs_textual fs_visual:fs_textual_se #> 1 -0.98165784 -0.036420142           -0.1913384               0.4927885 #> 2 -0.08745190 -1.072572360           -0.1332920               0.4927885 #> 3 -0.44092861 -1.891551979            0.6069489               0.4927885 #> 4  0.73898620  0.001151637           -0.2262394               0.4927885 #> 5 -0.86328354 -0.111420121           -0.1309033               0.4927885 #> 6 -0.07855806 -1.389127610           -0.1179633               0.4927885 #>   fs_visual:fs_textual_ld #> 1                0.584614 #> 2                0.584614 #> 3                0.584614 #> 4                0.584614 #> 5                0.584614 #> 6                0.584614"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"Get Factor Scores Corresponding Standard Error Measurement","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"","code":"get_fs_lavaan(   lavobj,   method = c(\"regression\", \"Bartlett\"),   corrected_fsT = FALSE,   vfsLT = FALSE,   reliability = FALSE )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"lavobj lavaan model object using get_fs_lavaan(). method Character. Method computing factor scores (options \"regression\" \"Bartlett\"). Currently, default \"regression\" consistent lavPredict, Bartlett scores desirable properties may preferred 2S-PA. corrected_fsT Logical. Whether correct sampling error factor score weights computing error variance estimates factor scores. vfsLT Logical. Whether return covariance matrix fsT fsL, can used input vcov_corrected() obtain corrected covariances standard errors tspa() results. currently ignored. reliability Logical. Whether return reliability factor scores.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"data frame containing factor scores (prefix \"fs_\"), standard errors (suffix \"_se\"), implied loadings factor \"_by_\" factor scores, error variance-covariance factor scores (prefix \"evfs_\"). following also returned attributes: * fsT: error covariance factor scores * fsL: loading matrix factor scores * fsb: intercepts factor scores * scoring_matrix: weights computing factor scores items","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/get_fs_lavaan.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Factor Scores and the Corresponding Standard Error of Measurement — get_fs_lavaan","text":"","code":"library(lavaan) get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\")]) #>          fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1 #> 1  -0.52616832 0.1213615   0.9657673 0.01472862 #> 2   0.14365274 0.1213615   0.9657673 0.01472862 #> 3   0.71435592 0.1213615   0.9657673 0.01472862 #> 4   1.23992565 0.1213615   0.9657673 0.01472862 #> 5   0.83190803 0.1213615   0.9657673 0.01472862 #> 6   0.21238453 0.1213615   0.9657673 0.01472862 #> 7   0.11880855 0.1213615   0.9657673 0.01472862 #> 8   0.11322703 0.1213615   0.9657673 0.01472862 #> 9   0.25617279 0.1213615   0.9657673 0.01472862 #> 10  0.37112496 0.1213615   0.9657673 0.01472862 #> 11  0.67281395 0.1213615   0.9657673 0.01472862 #> 12  0.56885577 0.1213615   0.9657673 0.01472862 #> 13  1.31369791 0.1213615   0.9657673 0.01472862 #> 14  0.22042629 0.1213615   0.9657673 0.01472862 #> 15  0.57849228 0.1213615   0.9657673 0.01472862 #> 16  0.37805983 0.1213615   0.9657673 0.01472862 #> 17  0.05734046 0.1213615   0.9657673 0.01472862 #> 18 -0.01609202 0.1213615   0.9657673 0.01472862 #> 19  0.88923616 0.1213615   0.9657673 0.01472862 #> 20  1.11445897 0.1213615   0.9657673 0.01472862 #> 21  0.94657339 0.1213615   0.9657673 0.01472862 #> 22  0.90122770 0.1213615   0.9657673 0.01472862 #> 23  0.58409450 0.1213615   0.9657673 0.01472862 #> 24  0.64089192 0.1213615   0.9657673 0.01472862 #> 25  0.91021968 0.1213615   0.9657673 0.01472862 #> 26 -0.89660969 0.1213615   0.9657673 0.01472862 #> 27 -0.13195991 0.1213615   0.9657673 0.01472862 #> 28 -0.52968769 0.1213615   0.9657673 0.01472862 #> 29 -0.81799629 0.1213615   0.9657673 0.01472862 #> 30 -1.27199371 0.1213615   0.9657673 0.01472862 #> 31 -0.32096024 0.1213615   0.9657673 0.01472862 #> 32 -1.16780103 0.1213615   0.9657673 0.01472862 #> 33 -0.12295473 0.1213615   0.9657673 0.01472862 #> 34 -0.04285945 0.1213615   0.9657673 0.01472862 #> 35 -0.34323505 0.1213615   0.9657673 0.01472862 #> 36 -0.60541633 0.1213615   0.9657673 0.01472862 #> 37  0.17688718 0.1213615   0.9657673 0.01472862 #> 38 -0.55066055 0.1213615   0.9657673 0.01472862 #> 39 -1.05988219 0.1213615   0.9657673 0.01472862 #> 40 -0.04138802 0.1213615   0.9657673 0.01472862 #> 41 -0.12611837 0.1213615   0.9657673 0.01472862 #> 42 -0.60322892 0.1213615   0.9657673 0.01472862 #> 43 -0.11057176 0.1213615   0.9657673 0.01472862 #> 44 -1.06423085 0.1213615   0.9657673 0.01472862 #> 45 -1.08354999 0.1213615   0.9657673 0.01472862 #> 46 -0.84009484 0.1213615   0.9657673 0.01472862 #> 47 -1.14678213 0.1213615   0.9657673 0.01472862 #> 48 -0.57578976 0.1213615   0.9657673 0.01472862 #> 49  0.07186692 0.1213615   0.9657673 0.01472862 #> 50  0.14682421 0.1213615   0.9657673 0.01472862 #> 51  0.35871830 0.1213615   0.9657673 0.01472862 #> 52 -0.43403195 0.1213615   0.9657673 0.01472862 #> 53  0.44603111 0.1213615   0.9657673 0.01472862 #> 54  0.26352000 0.1213615   0.9657673 0.01472862 #> 55  0.55051165 0.1213615   0.9657673 0.01472862 #> 56  0.23453122 0.1213615   0.9657673 0.01472862 #> 57  0.27968138 0.1213615   0.9657673 0.01472862 #> 58  0.70960640 0.1213615   0.9657673 0.01472862 #> 59  0.25227978 0.1213615   0.9657673 0.01472862 #> 60  1.18849297 0.1213615   0.9657673 0.01472862 #> 61  0.21104946 0.1213615   0.9657673 0.01472862 #> 62 -1.16516281 0.1213615   0.9657673 0.01472862 #> 63 -0.85560065 0.1213615   0.9657673 0.01472862 #> 64  0.13398476 0.1213615   0.9657673 0.01472862 #> 65 -0.07912189 0.1213615   0.9657673 0.01472862 #> 66 -0.27146711 0.1213615   0.9657673 0.01472862 #> 67 -0.04417217 0.1213615   0.9657673 0.01472862 #> 68 -1.33425662 0.1213615   0.9657673 0.01472862 #> 69 -0.38720750 0.1213615   0.9657673 0.01472862 #> 70 -0.55355511 0.1213615   0.9657673 0.01472862 #> 71 -0.72242623 0.1213615   0.9657673 0.01472862 #> 72  0.30607449 0.1213615   0.9657673 0.01472862 #> 73  0.77707950 0.1213615   0.9657673 0.01472862 #> 74  0.06847481 0.1213615   0.9657673 0.01472862 #> 75 -0.11052927 0.1213615   0.9657673 0.01472862  # Multiple factors get_fs(PoliticalDemocracy[c(\"x1\", \"x2\", \"x3\", \"y1\", \"y2\", \"y3\", \"y4\")],        model = \" ind60 =~ x1 + x2 + x3                  dem60 =~ y1 + y2 + y3 + y4 \") #>       fs_ind60    fs_dem60 fs_ind60_se fs_dem60_se ind60_by_fs_ind60 #> 1  -0.54258816 -2.74640573   0.1245694   0.6307323         0.9553858 #> 2   0.12647664 -2.85646114   0.1245694   0.6307323         0.9553858 #> 3   0.73408891  2.74401728   0.1245694   0.6307323         0.9553858 #> 4   1.25253604  3.10856431   0.1245694   0.6307323         0.9553858 #> 5   0.83355267  1.92455641   0.1245694   0.6307323         0.9553858 #> 6   0.22426801  1.02292332   0.1245694   0.6307323         0.9553858 #> 7   0.12517739  1.00406461   0.1245694   0.6307323         0.9553858 #> 8   0.11783867 -0.37216403   0.1245694   0.6307323         0.9553858 #> 9   0.25175134 -1.24897911   0.1245694   0.6307323         0.9553858 #> 10  0.39938631  2.85267059   0.1245694   0.6307323         0.9553858 #> 11  0.67497777  1.41959595   0.1245694   0.6307323         0.9553858 #> 12  0.56462020  1.08769844   0.1245694   0.6307323         0.9553858 #> 13  1.31236592  1.54090232   0.1245694   0.6307323         0.9553858 #> 14  0.23246021  1.77370863   0.1245694   0.6307323         0.9553858 #> 15  0.58638481  2.45676871   0.1245694   0.6307323         0.9553858 #> 16  0.38404785  2.35887573   0.1245694   0.6307323         0.9553858 #> 17  0.05076465  0.04034088   0.1245694   0.6307323         0.9553858 #> 18 -0.01747337 -1.86718064   0.1245694   0.6307323         0.9553858 #> 19  0.90920762  3.61477756   0.1245694   0.6307323         0.9553858 #> 20  1.12553557  0.88355273   0.1245694   0.6307323         0.9553858 #> 21  0.97202590  3.62673300   0.1245694   0.6307323         0.9553858 #> 22  0.87820036 -3.02428925   0.1245694   0.6307323         0.9553858 #> 23  0.57540754 -1.51695438   0.1245694   0.6307323         0.9553858 #> 24  0.66221224  2.76341635   0.1245694   0.6307323         0.9553858 #> 25  0.92358281  2.00507336   0.1245694   0.6307323         0.9553858 #> 26 -0.89353051 -0.92008050   0.1245694   0.6307323         0.9553858 #> 27 -0.13984744 -1.19025576   0.1245694   0.6307323         0.9553858 #> 28 -0.53828496 -1.01247764   0.1245694   0.6307323         0.9553858 #> 29 -0.80834865  0.10456709   0.1245694   0.6307323         0.9553858 #> 30 -1.25324343 -0.71847055   0.1245694   0.6307323         0.9553858 #> 31 -0.33373641 -1.61401581   0.1245694   0.6307323         0.9553858 #> 32 -1.17441075 -3.27250363   0.1245694   0.6307323         0.9553858 #> 33 -0.12409974 -1.17530231   0.1245694   0.6307323         0.9553858 #> 34 -0.04239173 -0.53796274   0.1245694   0.6307323         0.9553858 #> 35 -0.34010528  0.74552889   0.1245694   0.6307323         0.9553858 #> 36 -0.58953870  1.61018662   0.1245694   0.6307323         0.9553858 #> 37  0.17453657 -0.28144814   0.1245694   0.6307323         0.9553858 #> 38 -0.54457243  0.37694690   0.1245694   0.6307323         0.9553858 #> 39 -1.05196602 -0.62919501   0.1245694   0.6307323         0.9553858 #> 40 -0.05504697 -0.03346842   0.1245694   0.6307323         0.9553858 #> 41 -0.12364358 -0.38394102   0.1245694   0.6307323         0.9553858 #> 42 -0.59058710  1.35347275   0.1245694   0.6307323         0.9553858 #> 43 -0.11968796  0.89227782   0.1245694   0.6307323         0.9553858 #> 44 -1.07176064 -2.08481096   0.1245694   0.6307323         0.9553858 #> 45 -1.09139097 -2.07944291   0.1245694   0.6307323         0.9553858 #> 46 -0.83287255  1.59590721   0.1245694   0.6307323         0.9553858 #> 47 -1.14519896 -1.53352201   0.1245694   0.6307323         0.9553858 #> 48 -0.56115378  2.08138051   0.1245694   0.6307323         0.9553858 #> 49  0.06493340 -1.04044137   0.1245694   0.6307323         0.9553858 #> 50  0.15671638  1.72618633   0.1245694   0.6307323         0.9553858 #> 51  0.34626130 -1.24967043   0.1245694   0.6307323         0.9553858 #> 52 -0.45158373 -2.31742576   0.1245694   0.6307323         0.9553858 #> 53  0.43233465 -1.07533341   0.1245694   0.6307323         0.9553858 #> 54  0.25779725 -0.02904676   0.1245694   0.6307323         0.9553858 #> 55  0.51730650 -2.78207923   0.1245694   0.6307323         0.9553858 #> 56  0.20104991 -2.49001474   0.1245694   0.6307323         0.9553858 #> 57  0.25318620 -2.52145147   0.1245694   0.6307323         0.9553858 #> 58  0.72354623  1.86717109   0.1245694   0.6307323         0.9553858 #> 59  0.24619740 -0.93321102   0.1245694   0.6307323         0.9553858 #> 60  1.21681210  3.19853937   0.1245694   0.6307323         0.9553858 #> 61  0.18167599 -3.15685030   0.1245694   0.6307323         0.9553858 #> 62 -1.16605067 -3.41334680   0.1245694   0.6307323         0.9553858 #> 63 -0.86491026 -3.11864398   0.1245694   0.6307323         0.9553858 #> 64  0.10990059 -0.47238885   0.1245694   0.6307323         0.9553858 #> 65 -0.07376176  2.95292007   0.1245694   0.6307323         0.9553858 #> 66 -0.28782931 -1.96509718   0.1245694   0.6307323         0.9553858 #> 67 -0.02508160  2.96218478   0.1245694   0.6307323         0.9553858 #> 68 -1.31843215 -1.59567027   0.1245694   0.6307323         0.9553858 #> 69 -0.40462357 -1.79146161   0.1245694   0.6307323         0.9553858 #> 70 -0.55568363 -1.01578892   0.1245694   0.6307323         0.9553858 #> 71 -0.71308015  0.08818212   0.1245694   0.6307323         0.9553858 #> 72  0.31014319  1.70765911   0.1245694   0.6307323         0.9553858 #> 73  0.79092897  1.86102556   0.1245694   0.6307323         0.9553858 #> 74  0.08770237  3.12885767   0.1245694   0.6307323         0.9553858 #> 75 -0.14138149 -2.41398025   0.1245694   0.6307323         0.9553858 #>    ind60_by_fs_dem60 dem60_by_fs_ind60 dem60_by_fs_dem60 ev_fs_ind60 #> 1           0.181827       0.005867694         0.8688887  0.01551752 #> 2           0.181827       0.005867694         0.8688887  0.01551752 #> 3           0.181827       0.005867694         0.8688887  0.01551752 #> 4           0.181827       0.005867694         0.8688887  0.01551752 #> 5           0.181827       0.005867694         0.8688887  0.01551752 #> 6           0.181827       0.005867694         0.8688887  0.01551752 #> 7           0.181827       0.005867694         0.8688887  0.01551752 #> 8           0.181827       0.005867694         0.8688887  0.01551752 #> 9           0.181827       0.005867694         0.8688887  0.01551752 #> 10          0.181827       0.005867694         0.8688887  0.01551752 #> 11          0.181827       0.005867694         0.8688887  0.01551752 #> 12          0.181827       0.005867694         0.8688887  0.01551752 #> 13          0.181827       0.005867694         0.8688887  0.01551752 #> 14          0.181827       0.005867694         0.8688887  0.01551752 #> 15          0.181827       0.005867694         0.8688887  0.01551752 #> 16          0.181827       0.005867694         0.8688887  0.01551752 #> 17          0.181827       0.005867694         0.8688887  0.01551752 #> 18          0.181827       0.005867694         0.8688887  0.01551752 #> 19          0.181827       0.005867694         0.8688887  0.01551752 #> 20          0.181827       0.005867694         0.8688887  0.01551752 #> 21          0.181827       0.005867694         0.8688887  0.01551752 #> 22          0.181827       0.005867694         0.8688887  0.01551752 #> 23          0.181827       0.005867694         0.8688887  0.01551752 #> 24          0.181827       0.005867694         0.8688887  0.01551752 #> 25          0.181827       0.005867694         0.8688887  0.01551752 #> 26          0.181827       0.005867694         0.8688887  0.01551752 #> 27          0.181827       0.005867694         0.8688887  0.01551752 #> 28          0.181827       0.005867694         0.8688887  0.01551752 #> 29          0.181827       0.005867694         0.8688887  0.01551752 #> 30          0.181827       0.005867694         0.8688887  0.01551752 #> 31          0.181827       0.005867694         0.8688887  0.01551752 #> 32          0.181827       0.005867694         0.8688887  0.01551752 #> 33          0.181827       0.005867694         0.8688887  0.01551752 #> 34          0.181827       0.005867694         0.8688887  0.01551752 #> 35          0.181827       0.005867694         0.8688887  0.01551752 #> 36          0.181827       0.005867694         0.8688887  0.01551752 #> 37          0.181827       0.005867694         0.8688887  0.01551752 #> 38          0.181827       0.005867694         0.8688887  0.01551752 #> 39          0.181827       0.005867694         0.8688887  0.01551752 #> 40          0.181827       0.005867694         0.8688887  0.01551752 #> 41          0.181827       0.005867694         0.8688887  0.01551752 #> 42          0.181827       0.005867694         0.8688887  0.01551752 #> 43          0.181827       0.005867694         0.8688887  0.01551752 #> 44          0.181827       0.005867694         0.8688887  0.01551752 #> 45          0.181827       0.005867694         0.8688887  0.01551752 #> 46          0.181827       0.005867694         0.8688887  0.01551752 #> 47          0.181827       0.005867694         0.8688887  0.01551752 #> 48          0.181827       0.005867694         0.8688887  0.01551752 #> 49          0.181827       0.005867694         0.8688887  0.01551752 #> 50          0.181827       0.005867694         0.8688887  0.01551752 #> 51          0.181827       0.005867694         0.8688887  0.01551752 #> 52          0.181827       0.005867694         0.8688887  0.01551752 #> 53          0.181827       0.005867694         0.8688887  0.01551752 #> 54          0.181827       0.005867694         0.8688887  0.01551752 #> 55          0.181827       0.005867694         0.8688887  0.01551752 #> 56          0.181827       0.005867694         0.8688887  0.01551752 #> 57          0.181827       0.005867694         0.8688887  0.01551752 #> 58          0.181827       0.005867694         0.8688887  0.01551752 #> 59          0.181827       0.005867694         0.8688887  0.01551752 #> 60          0.181827       0.005867694         0.8688887  0.01551752 #> 61          0.181827       0.005867694         0.8688887  0.01551752 #> 62          0.181827       0.005867694         0.8688887  0.01551752 #> 63          0.181827       0.005867694         0.8688887  0.01551752 #> 64          0.181827       0.005867694         0.8688887  0.01551752 #> 65          0.181827       0.005867694         0.8688887  0.01551752 #> 66          0.181827       0.005867694         0.8688887  0.01551752 #> 67          0.181827       0.005867694         0.8688887  0.01551752 #> 68          0.181827       0.005867694         0.8688887  0.01551752 #> 69          0.181827       0.005867694         0.8688887  0.01551752 #> 70          0.181827       0.005867694         0.8688887  0.01551752 #> 71          0.181827       0.005867694         0.8688887  0.01551752 #> 72          0.181827       0.005867694         0.8688887  0.01551752 #> 73          0.181827       0.005867694         0.8688887  0.01551752 #> 74          0.181827       0.005867694         0.8688887  0.01551752 #> 75          0.181827       0.005867694         0.8688887  0.01551752 #>    ecov_fs_dem60_fs_ind60 ev_fs_dem60 #> 1             0.005632564   0.3978232 #> 2             0.005632564   0.3978232 #> 3             0.005632564   0.3978232 #> 4             0.005632564   0.3978232 #> 5             0.005632564   0.3978232 #> 6             0.005632564   0.3978232 #> 7             0.005632564   0.3978232 #> 8             0.005632564   0.3978232 #> 9             0.005632564   0.3978232 #> 10            0.005632564   0.3978232 #> 11            0.005632564   0.3978232 #> 12            0.005632564   0.3978232 #> 13            0.005632564   0.3978232 #> 14            0.005632564   0.3978232 #> 15            0.005632564   0.3978232 #> 16            0.005632564   0.3978232 #> 17            0.005632564   0.3978232 #> 18            0.005632564   0.3978232 #> 19            0.005632564   0.3978232 #> 20            0.005632564   0.3978232 #> 21            0.005632564   0.3978232 #> 22            0.005632564   0.3978232 #> 23            0.005632564   0.3978232 #> 24            0.005632564   0.3978232 #> 25            0.005632564   0.3978232 #> 26            0.005632564   0.3978232 #> 27            0.005632564   0.3978232 #> 28            0.005632564   0.3978232 #> 29            0.005632564   0.3978232 #> 30            0.005632564   0.3978232 #> 31            0.005632564   0.3978232 #> 32            0.005632564   0.3978232 #> 33            0.005632564   0.3978232 #> 34            0.005632564   0.3978232 #> 35            0.005632564   0.3978232 #> 36            0.005632564   0.3978232 #> 37            0.005632564   0.3978232 #> 38            0.005632564   0.3978232 #> 39            0.005632564   0.3978232 #> 40            0.005632564   0.3978232 #> 41            0.005632564   0.3978232 #> 42            0.005632564   0.3978232 #> 43            0.005632564   0.3978232 #> 44            0.005632564   0.3978232 #> 45            0.005632564   0.3978232 #> 46            0.005632564   0.3978232 #> 47            0.005632564   0.3978232 #> 48            0.005632564   0.3978232 #> 49            0.005632564   0.3978232 #> 50            0.005632564   0.3978232 #> 51            0.005632564   0.3978232 #> 52            0.005632564   0.3978232 #> 53            0.005632564   0.3978232 #> 54            0.005632564   0.3978232 #> 55            0.005632564   0.3978232 #> 56            0.005632564   0.3978232 #> 57            0.005632564   0.3978232 #> 58            0.005632564   0.3978232 #> 59            0.005632564   0.3978232 #> 60            0.005632564   0.3978232 #> 61            0.005632564   0.3978232 #> 62            0.005632564   0.3978232 #> 63            0.005632564   0.3978232 #> 64            0.005632564   0.3978232 #> 65            0.005632564   0.3978232 #> 66            0.005632564   0.3978232 #> 67            0.005632564   0.3978232 #> 68            0.005632564   0.3978232 #> 69            0.005632564   0.3978232 #> 70            0.005632564   0.3978232 #> 71            0.005632564   0.3978232 #> 72            0.005632564   0.3978232 #> 73            0.005632564   0.3978232 #> 74            0.005632564   0.3978232 #> 75            0.005632564   0.3978232  # Multiple-group hs_model <- ' visual  =~ x1 + x2 + x3 ' fit <- cfa(hs_model,            data = HolzingerSwineford1939,            group = \"school\") get_fs(HolzingerSwineford1939, hs_model, group = \"school\") #> $Pasteur #>        fs_visual fs_visual_se visual_by_fs_visual ev_fs_visual  school #> 1   -0.821165191    0.3391326           0.6734826    0.1150109 Pasteur #> 2   -0.124009418    0.3391326           0.6734826    0.1150109 Pasteur #> 3   -0.370072089    0.3391326           0.6734826    0.1150109 Pasteur #> 4    0.440928618    0.3391326           0.6734826    0.1150109 Pasteur #> 5   -0.691389016    0.3391326           0.6734826    0.1150109 Pasteur #> 6   -0.110032619    0.3391326           0.6734826    0.1150109 Pasteur #> 7   -0.904127845    0.3391326           0.6734826    0.1150109 Pasteur #> 8   -0.031747573    0.3391326           0.6734826    0.1150109 Pasteur #> 9   -0.439478981    0.3391326           0.6734826    0.1150109 Pasteur #> 10  -0.938939050    0.3391326           0.6734826    0.1150109 Pasteur #> 11  -0.436821880    0.3391326           0.6734826    0.1150109 Pasteur #> 12   0.305033497    0.3391326           0.6734826    0.1150109 Pasteur #> 13   0.522076263    0.3391326           0.6734826    0.1150109 Pasteur #> 14  -0.090367931    0.3391326           0.6734826    0.1150109 Pasteur #> 15   0.526276771    0.3391326           0.6734826    0.1150109 Pasteur #> 16  -0.226580678    0.3391326           0.6734826    0.1150109 Pasteur #> 17  -0.582016192    0.3391326           0.6734826    0.1150109 Pasteur #> 18   0.017040431    0.3391326           0.6734826    0.1150109 Pasteur #> 19   0.563052459    0.3391326           0.6734826    0.1150109 Pasteur #> 20   0.746621910    0.3391326           0.6734826    0.1150109 Pasteur #> 21   0.234672405    0.3391326           0.6734826    0.1150109 Pasteur #> 22   1.157487518    0.3391326           0.6734826    0.1150109 Pasteur #> 23  -0.162272449    0.3391326           0.6734826    0.1150109 Pasteur #> 24  -0.556027059    0.3391326           0.6734826    0.1150109 Pasteur #> 25  -0.321443540    0.3391326           0.6734826    0.1150109 Pasteur #> 26   0.153141050    0.3391326           0.6734826    0.1150109 Pasteur #> 27   0.696234416    0.3391326           0.6734826    0.1150109 Pasteur #> 28  -0.020961039    0.3391326           0.6734826    0.1150109 Pasteur #> 29   0.532601236    0.3391326           0.6734826    0.1150109 Pasteur #> 30  -0.727687585    0.3391326           0.6734826    0.1150109 Pasteur #> 31  -0.676719580    0.3391326           0.6734826    0.1150109 Pasteur #> 32  -1.120216393    0.3391326           0.6734826    0.1150109 Pasteur #> 33  -0.313631732    0.3391326           0.6734826    0.1150109 Pasteur #> 34  -0.187091845    0.3391326           0.6734826    0.1150109 Pasteur #> 35  -0.887709484    0.3391326           0.6734826    0.1150109 Pasteur #> 36  -0.760795908    0.3391326           0.6734826    0.1150109 Pasteur #> 37   0.556943532    0.3391326           0.6734826    0.1150109 Pasteur #> 38  -0.458666570    0.3391326           0.6734826    0.1150109 Pasteur #> 39   0.514741536    0.3391326           0.6734826    0.1150109 Pasteur #> 40   0.373009089    0.3391326           0.6734826    0.1150109 Pasteur #> 41  -0.528550562    0.3391326           0.6734826    0.1150109 Pasteur #> 42  -0.865864795    0.3391326           0.6734826    0.1150109 Pasteur #> 43  -1.182344640    0.3391326           0.6734826    0.1150109 Pasteur #> 44  -0.435334517    0.3391326           0.6734826    0.1150109 Pasteur #> 45   0.306520860    0.3391326           0.6734826    0.1150109 Pasteur #> 46   0.821604565    0.3391326           0.6734826    0.1150109 Pasteur #> 47   1.213927875    0.3391326           0.6734826    0.1150109 Pasteur #> 48  -0.851887996    0.3391326           0.6734826    0.1150109 Pasteur #> 49  -0.085053749    0.3391326           0.6734826    0.1150109 Pasteur #> 50  -0.508885873    0.3391326           0.6734826    0.1150109 Pasteur #> 51   0.502467638    0.3391326           0.6734826    0.1150109 Pasteur #> 52   0.284732253    0.3391326           0.6734826    0.1150109 Pasteur #> 53   0.202677755    0.3391326           0.6734826    0.1150109 Pasteur #> 54  -0.335953502    0.3391326           0.6734826    0.1150109 Pasteur #> 55   0.556410369    0.3391326           0.6734826    0.1150109 Pasteur #> 56  -0.058746970    0.3391326           0.6734826    0.1150109 Pasteur #> 57  -0.066932487    0.3391326           0.6734826    0.1150109 Pasteur #> 58   0.554230368    0.3391326           0.6734826    0.1150109 Pasteur #> 59  -0.321761185    0.3391326           0.6734826    0.1150109 Pasteur #> 60  -0.421834819    0.3391326           0.6734826    0.1150109 Pasteur #> 61   0.345476529    0.3391326           0.6734826    0.1150109 Pasteur #> 62   0.194809883    0.3391326           0.6734826    0.1150109 Pasteur #> 63  -0.207870208    0.3391326           0.6734826    0.1150109 Pasteur #> 64  -0.441658981    0.3391326           0.6734826    0.1150109 Pasteur #> 65   0.102070958    0.3391326           0.6734826    0.1150109 Pasteur #> 66   0.311198487    0.3391326           0.6734826    0.1150109 Pasteur #> 67   0.676364229    0.3391326           0.6734826    0.1150109 Pasteur #> 68   0.297858262    0.3391326           0.6734826    0.1150109 Pasteur #> 69  -1.055487128    0.3391326           0.6734826    0.1150109 Pasteur #> 70  -0.737997019    0.3391326           0.6734826    0.1150109 Pasteur #> 71  -1.576099236    0.3391326           0.6734826    0.1150109 Pasteur #> 72   0.534360181    0.3391326           0.6734826    0.1150109 Pasteur #> 73  -0.105888156    0.3391326           0.6734826    0.1150109 Pasteur #> 74   0.266237302    0.3391326           0.6734826    0.1150109 Pasteur #> 75  -0.352427927    0.3391326           0.6734826    0.1150109 Pasteur #> 76  -0.334783784    0.3391326           0.6734826    0.1150109 Pasteur #> 77   0.133588508    0.3391326           0.6734826    0.1150109 Pasteur #> 78  -1.035662965    0.3391326           0.6734826    0.1150109 Pasteur #> 79   0.762507108    0.3391326           0.6734826    0.1150109 Pasteur #> 80  -0.260699265    0.3391326           0.6734826    0.1150109 Pasteur #> 81  -0.329095893    0.3391326           0.6734826    0.1150109 Pasteur #> 82   0.752413211    0.3391326           0.6734826    0.1150109 Pasteur #> 83   0.149268188    0.3391326           0.6734826    0.1150109 Pasteur #> 84  -0.208880471    0.3391326           0.6734826    0.1150109 Pasteur #> 85  -1.078285998    0.3391326           0.6734826    0.1150109 Pasteur #> 86   0.306043760    0.3391326           0.6734826    0.1150109 Pasteur #> 87   0.349677056    0.3391326           0.6734826    0.1150109 Pasteur #> 88   0.165686549    0.3391326           0.6734826    0.1150109 Pasteur #> 89   0.077307606    0.3391326           0.6734826    0.1150109 Pasteur #> 90  -0.077401396    0.3391326           0.6734826    0.1150109 Pasteur #> 91  -0.081863485    0.3391326           0.6734826    0.1150109 Pasteur #> 92   0.106748566    0.3391326           0.6734826    0.1150109 Pasteur #> 93  -0.211593616    0.3391326           0.6734826    0.1150109 Pasteur #> 94  -0.926665153    0.3391326           0.6734826    0.1150109 Pasteur #> 95  -0.739484382    0.3391326           0.6734826    0.1150109 Pasteur #> 96   0.570387167    0.3391326           0.6734826    0.1150109 Pasteur #> 97  -0.913642554    0.3391326           0.6734826    0.1150109 Pasteur #> 98   0.547484887    0.3391326           0.6734826    0.1150109 Pasteur #> 99  -0.602850599    0.3391326           0.6734826    0.1150109 Pasteur #> 100  0.225794270    0.3391326           0.6734826    0.1150109 Pasteur #> 101  0.620447015    0.3391326           0.6734826    0.1150109 Pasteur #> 102  0.158885005    0.3391326           0.6734826    0.1150109 Pasteur #> 103 -0.127938344    0.3391326           0.6734826    0.1150109 Pasteur #> 104 -0.420347455    0.3391326           0.6734826    0.1150109 Pasteur #> 105  1.327978307    0.3391326           0.6734826    0.1150109 Pasteur #> 106  0.181843348    0.3391326           0.6734826    0.1150109 Pasteur #> 107 -0.148932224    0.3391326           0.6734826    0.1150109 Pasteur #> 108  0.612373626    0.3391326           0.6734826    0.1150109 Pasteur #> 109 -0.066558798    0.3391326           0.6734826    0.1150109 Pasteur #> 110 -0.420880619    0.3391326           0.6734826    0.1150109 Pasteur #> 111  1.127036295    0.3391326           0.6734826    0.1150109 Pasteur #> 112  0.237591068    0.3391326           0.6734826    0.1150109 Pasteur #> 113  0.853758689    0.3391326           0.6734826    0.1150109 Pasteur #> 114 -0.143618023    0.3391326           0.6734826    0.1150109 Pasteur #> 115  0.475206679    0.3391326           0.6734826    0.1150109 Pasteur #> 116 -0.670554590    0.3391326           0.6734826    0.1150109 Pasteur #> 117  0.022672257    0.3391326           0.6734826    0.1150109 Pasteur #> 118  0.302002707    0.3391326           0.6734826    0.1150109 Pasteur #> 119  0.151392125    0.3391326           0.6734826    0.1150109 Pasteur #> 120 -0.475300449    0.3391326           0.6734826    0.1150109 Pasteur #> 121 -0.346740056    0.3391326           0.6734826    0.1150109 Pasteur #> 122 -0.078888759    0.3391326           0.6734826    0.1150109 Pasteur #> 123  1.197237913    0.3391326           0.6734826    0.1150109 Pasteur #> 124  0.539243306    0.3391326           0.6734826    0.1150109 Pasteur #> 125  0.867258388    0.3391326           0.6734826    0.1150109 Pasteur #> 126  0.592287901    0.3391326           0.6734826    0.1150109 Pasteur #> 127 -0.500540901    0.3391326           0.6734826    0.1150109 Pasteur #> 128 -0.361193954    0.3391326           0.6734826    0.1150109 Pasteur #> 129  0.626883588    0.3391326           0.6734826    0.1150109 Pasteur #> 130 -0.437514518    0.3391326           0.6734826    0.1150109 Pasteur #> 131  0.695972854    0.3391326           0.6734826    0.1150109 Pasteur #> 132  0.424715775    0.3391326           0.6734826    0.1150109 Pasteur #> 133 -0.203725744    0.3391326           0.6734826    0.1150109 Pasteur #> 134 -0.441499507    0.3391326           0.6734826    0.1150109 Pasteur #> 135  0.735619838    0.3391326           0.6734826    0.1150109 Pasteur #> 136  0.783874697    0.3391326           0.6734826    0.1150109 Pasteur #> 137  0.565709540    0.3391326           0.6734826    0.1150109 Pasteur #> 138  0.258425494    0.3391326           0.6734826    0.1150109 Pasteur #> 139  0.861093397    0.3391326           0.6734826    0.1150109 Pasteur #> 140 -0.059757233    0.3391326           0.6734826    0.1150109 Pasteur #> 141 -0.920340689    0.3391326           0.6734826    0.1150109 Pasteur #> 142  0.845629236    0.3391326           0.6734826    0.1150109 Pasteur #> 143  1.227427574    0.3391326           0.6734826    0.1150109 Pasteur #> 144  1.054223601    0.3391326           0.6734826    0.1150109 Pasteur #> 145 -1.246596805    0.3391326           0.6734826    0.1150109 Pasteur #> 146 -0.473120468    0.3391326           0.6734826    0.1150109 Pasteur #> 147 -0.560171503    0.3391326           0.6734826    0.1150109 Pasteur #> 148 -0.365394462    0.3391326           0.6734826    0.1150109 Pasteur #> 149  0.084744422    0.3391326           0.6734826    0.1150109 Pasteur #> 150  0.910676146    0.3391326           0.6734826    0.1150109 Pasteur #> 151  1.094189533    0.3391326           0.6734826    0.1150109 Pasteur #> 152 -0.013149231    0.3391326           0.6734826    0.1150109 Pasteur #> 153 -0.166472976    0.3391326           0.6734826    0.1150109 Pasteur #> 154  0.008695459    0.3391326           0.6734826    0.1150109 Pasteur #> 155 -0.094989494    0.3391326           0.6734826    0.1150109 Pasteur #> 156 -0.457123143    0.3391326           0.6734826    0.1150109 Pasteur #>  #> $`Grant-White` #>        fs_visual fs_visual_se visual_by_fs_visual ev_fs_visual      school #> 1   -0.915287109     0.311828           0.6990509   0.09723667 Grant-White #> 2    0.035963597     0.311828           0.6990509   0.09723667 Grant-White #> 3    0.355636604     0.311828           0.6990509   0.09723667 Grant-White #> 4   -0.387353871     0.311828           0.6990509   0.09723667 Grant-White #> 5   -0.622393942     0.311828           0.6990509   0.09723667 Grant-White #> 6    0.195944561     0.311828           0.6990509   0.09723667 Grant-White #> 7    1.353023831     0.311828           0.6990509   0.09723667 Grant-White #> 8   -0.341506254     0.311828           0.6990509   0.09723667 Grant-White #> 9   -0.199493575     0.311828           0.6990509   0.09723667 Grant-White #> 10  -0.689869149     0.311828           0.6990509   0.09723667 Grant-White #> 11  -0.463929554     0.311828           0.6990509   0.09723667 Grant-White #> 12  -0.423001505     0.311828           0.6990509   0.09723667 Grant-White #> 13   0.279743296     0.311828           0.6990509   0.09723667 Grant-White #> 14  -0.916908219     0.311828           0.6990509   0.09723667 Grant-White #> 15   0.589344501     0.311828           0.6990509   0.09723667 Grant-White #> 16   0.191474701     0.311828           0.6990509   0.09723667 Grant-White #> 17   0.935275715     0.311828           0.6990509   0.09723667 Grant-White #> 18   0.393715904     0.311828           0.6990509   0.09723667 Grant-White #> 19   0.086569994     0.311828           0.6990509   0.09723667 Grant-White #> 20   0.555606898     0.311828           0.6990509   0.09723667 Grant-White #> 21  -0.558217193     0.311828           0.6990509   0.09723667 Grant-White #> 22   0.766715894     0.311828           0.6990509   0.09723667 Grant-White #> 23   0.115548801     0.311828           0.6990509   0.09723667 Grant-White #> 24   0.901249191     0.311828           0.6990509   0.09723667 Grant-White #> 25   0.174316971     0.311828           0.6990509   0.09723667 Grant-White #> 26  -0.078980322     0.311828           0.6990509   0.09723667 Grant-White #> 27  -0.581882977     0.311828           0.6990509   0.09723667 Grant-White #> 28  -0.661179262     0.311828           0.6990509   0.09723667 Grant-White #> 29  -0.245341176     0.311828           0.6990509   0.09723667 Grant-White #> 30  -0.195801662     0.311828           0.6990509   0.09723667 Grant-White #> 31  -0.281221524     0.311828           0.6990509   0.09723667 Grant-White #> 32  -0.293909378     0.311828           0.6990509   0.09723667 Grant-White #> 33  -0.604192958     0.311828           0.6990509   0.09723667 Grant-White #> 34  -0.738437335     0.311828           0.6990509   0.09723667 Grant-White #> 35  -0.304109345     0.311828           0.6990509   0.09723667 Grant-White #> 36   0.104931733     0.311828           0.6990509   0.09723667 Grant-White #> 37  -0.025781487     0.311828           0.6990509   0.09723667 Grant-White #> 38  -0.897318824     0.311828           0.6990509   0.09723667 Grant-White #> 39  -0.892560027     0.311828           0.6990509   0.09723667 Grant-White #> 40  -0.078402465     0.311828           0.6990509   0.09723667 Grant-White #> 41  -0.379063934     0.311828           0.6990509   0.09723667 Grant-White #> 42  -0.324926380     0.311828           0.6990509   0.09723667 Grant-White #> 43  -0.684299797     0.311828           0.6990509   0.09723667 Grant-White #> 44  -0.304109345     0.311828           0.6990509   0.09723667 Grant-White #> 45   0.622793169     0.311828           0.6990509   0.09723667 Grant-White #> 46  -0.152835419     0.311828           0.6990509   0.09723667 Grant-White #> 47  -0.421902013     0.311828           0.6990509   0.09723667 Grant-White #> 48  -0.060883872     0.311828           0.6990509   0.09723667 Grant-White #> 49  -0.303298790     0.311828           0.6990509   0.09723667 Grant-White #> 50   0.425021826     0.311828           0.6990509   0.09723667 Grant-White #> 51   0.131478875     0.311828           0.6990509   0.09723667 Grant-White #> 52  -0.914998172     0.311828           0.6990509   0.09723667 Grant-White #> 53   0.324226132     0.311828           0.6990509   0.09723667 Grant-White #> 54  -0.086170767     0.311828           0.6990509   0.09723667 Grant-White #> 55   0.428424818     0.311828           0.6990509   0.09723667 Grant-White #> 56   0.188465179     0.311828           0.6990509   0.09723667 Grant-White #> 57  -0.306958111     0.311828           0.6990509   0.09723667 Grant-White #> 58   0.581736955     0.311828           0.6990509   0.09723667 Grant-White #> 59  -0.743485051     0.311828           0.6990509   0.09723667 Grant-White #> 60   0.263580524     0.311828           0.6990509   0.09723667 Grant-White #> 61   0.178786831     0.311828           0.6990509   0.09723667 Grant-White #> 62  -0.064832113     0.311828           0.6990509   0.09723667 Grant-White #> 63   0.499976414     0.311828           0.6990509   0.09723667 Grant-White #> 64  -0.092839593     0.311828           0.6990509   0.09723667 Grant-White #> 65  -0.263702931     0.311828           0.6990509   0.09723667 Grant-White #> 66  -0.983966325     0.311828           0.6990509   0.09723667 Grant-White #> 67   1.434912536     0.311828           0.6990509   0.09723667 Grant-White #> 68  -1.037582228     0.311828           0.6990509   0.09723667 Grant-White #> 69  -0.047569849     0.311828           0.6990509   0.09723667 Grant-White #> 70   1.084767775     0.311828           0.6990509   0.09723667 Grant-White #> 71   0.092011181     0.311828           0.6990509   0.09723667 Grant-White #> 72  -0.562687052     0.311828           0.6990509   0.09723667 Grant-White #> 73  -0.304919900     0.311828           0.6990509   0.09723667 Grant-White #> 74   1.038920175     0.311828           0.6990509   0.09723667 Grant-White #> 75  -0.789854287     0.311828           0.6990509   0.09723667 Grant-White #> 76  -0.602282928     0.311828           0.6990509   0.09723667 Grant-White #> 77  -0.894630830     0.311828           0.6990509   0.09723667 Grant-White #> 78   0.532614527     0.311828           0.6990509   0.09723667 Grant-White #> 79  -0.548955945     0.311828           0.6990509   0.09723667 Grant-White #> 80  -0.221514635     0.311828           0.6990509   0.09723667 Grant-White #> 81  -0.095849115     0.311828           0.6990509   0.09723667 Grant-White #> 82  -0.122235502     0.311828           0.6990509   0.09723667 Grant-White #> 83   0.892276864     0.311828           0.6990509   0.09723667 Grant-White #> 84   0.328439663     0.311828           0.6990509   0.09723667 Grant-White #> 85   1.217391042     0.311828           0.6990509   0.09723667 Grant-White #> 86   0.574513902     0.311828           0.6990509   0.09723667 Grant-White #> 87   0.160168762     0.311828           0.6990509   0.09723667 Grant-White #> 88   0.654909662     0.311828           0.6990509   0.09723667 Grant-White #> 89  -0.509777155     0.311828           0.6990509   0.09723667 Grant-White #> 90   1.201493560     0.311828           0.6990509   0.09723667 Grant-White #> 91   0.584874625     0.311828           0.6990509   0.09723667 Grant-White #> 92   0.075142371     0.311828           0.6990509   0.09723667 Grant-White #> 93   0.550976266     0.311828           0.6990509   0.09723667 Grant-White #> 94  -0.886308302     0.311828           0.6990509   0.09723667 Grant-White #> 95   0.552075757     0.311828           0.6990509   0.09723667 Grant-White #> 96   1.415972940     0.311828           0.6990509   0.09723667 Grant-White #> 97   0.298650301     0.311828           0.6990509   0.09723667 Grant-White #> 98  -0.143028906     0.311828           0.6990509   0.09723667 Grant-White #> 99   0.245195154     0.311828           0.6990509   0.09723667 Grant-White #> 100  0.247072593     0.311828           0.6990509   0.09723667 Grant-White #> 101  0.817322291     0.311828           0.6990509   0.09723667 Grant-White #> 102  0.651771976     0.311828           0.6990509   0.09723667 Grant-White #> 103  1.338875623     0.311828           0.6990509   0.09723667 Grant-White #> 104 -1.160005528     0.311828           0.6990509   0.09723667 Grant-White #> 105  0.163306449     0.311828           0.6990509   0.09723667 Grant-White #> 106 -0.387353871     0.311828           0.6990509   0.09723667 Grant-White #> 107 -0.517128372     0.311828           0.6990509   0.09723667 Grant-White #> 108  0.065103160     0.311828           0.6990509   0.09723667 Grant-White #> 109 -0.115438510     0.311828           0.6990509   0.09723667 Grant-White #> 110  0.094049376     0.311828           0.6990509   0.09723667 Grant-White #> 111  0.396725409     0.311828           0.6990509   0.09723667 Grant-White #> 112  0.672356312     0.311828           0.6990509   0.09723667 Grant-White #> 113  1.165974090     0.311828           0.6990509   0.09723667 Grant-White #> 114 -0.483518949     0.311828           0.6990509   0.09723667 Grant-White #> 115  0.035024877     0.311828           0.6990509   0.09723667 Grant-White #> 116  0.741974248     0.311828           0.6990509   0.09723667 Grant-White #> 117 -0.170386603     0.311828           0.6990509   0.09723667 Grant-White #> 118 -0.205873481     0.311828           0.6990509   0.09723667 Grant-White #> 119  0.714777307     0.311828           0.6990509   0.09723667 Grant-White #> 120 -0.620772831     0.311828           0.6990509   0.09723667 Grant-White #> 121 -0.313626938     0.311828           0.6990509   0.09723667 Grant-White #> 122 -0.157466035     0.311828           0.6990509   0.09723667 Grant-White #> 123  0.118269386     0.311828           0.6990509   0.09723667 Grant-White #> 124  0.101111673     0.311828           0.6990509   0.09723667 Grant-White #> 125 -0.625403463     0.311828           0.6990509   0.09723667 Grant-White #> 126  0.486638761     0.311828           0.6990509   0.09723667 Grant-White #> 127 -0.178676540     0.311828           0.6990509   0.09723667 Grant-White #> 128  0.274013189     0.311828           0.6990509   0.09723667 Grant-White #> 129 -0.316347523     0.311828           0.6990509   0.09723667 Grant-White #> 130 -0.026752814     0.311828           0.6990509   0.09723667 Grant-White #> 131  0.245323318     0.311828           0.6990509   0.09723667 Grant-White #> 132 -0.356336853     0.311828           0.6990509   0.09723667 Grant-White #> 133 -0.581594057     0.311828           0.6990509   0.09723667 Grant-White #> 134  0.263002667     0.311828           0.6990509   0.09723667 Grant-White #> 135 -0.864680712     0.311828           0.6990509   0.09723667 Grant-White #> 136 -0.377964443     0.311828           0.6990509   0.09723667 Grant-White #> 137 -0.112717909     0.311828           0.6990509   0.09723667 Grant-White #> 138  0.114449326     0.311828           0.6990509   0.09723667 Grant-White #> 139  0.001287274     0.311828           0.6990509   0.09723667 Grant-White #> 140  0.597634438     0.311828           0.6990509   0.09723667 Grant-White #> 141 -0.252531637     0.311828           0.6990509   0.09723667 Grant-White #> 142 -0.472901881     0.311828           0.6990509   0.09723667 Grant-White #> 143 -0.187255397     0.311828           0.6990509   0.09723667 Grant-White #> 144 -0.542415283     0.311828           0.6990509   0.09723667 Grant-White #> 145  0.358774274     0.311828           0.6990509   0.09723667 Grant-White #>  #> attr(,\"fsT\") #> attr(,\"fsT\")$Pasteur #>           fs_visual #> fs_visual 0.1150109 #>  #> attr(,\"fsT\")$`Grant-White` #>            fs_visual #> fs_visual 0.09723667 #>  #> attr(,\"fsL\") #> attr(,\"fsL\")$Pasteur #>              visual #> fs_visual 0.6734826 #>  #> attr(,\"fsL\")$`Grant-White` #>              visual #> fs_visual 0.6990509 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #> fs_visual  #>         0  #>  #> attr(,\"fsb\")$`Grant-White` #> fs_visual  #>         0  #>  #> attr(,\"scoring_matrix\") #> attr(,\"scoring_matrix\")$Pasteur #>             [,1]     [,2]      [,3] #> visual 0.1957873 0.109906 0.3316264 #>  #> attr(,\"scoring_matrix\")$`Grant-White` #>             [,1]      [,2]      [,3] #> visual 0.1624126 0.1458328 0.3515006 #>  # Or without the model get_fs(HolzingerSwineford1939[c(\"school\", \"x4\", \"x5\", \"x6\")],        group = \"school\") #> $Pasteur #>             fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1  school #> 1    0.3074500370 0.2999315   0.8833584 0.08995892 Pasteur #> 2   -0.7746062892 0.2999315   0.8833584 0.08995892 Pasteur #> 3   -1.5843019574 0.2999315   0.8833584 0.08995892 Pasteur #> 4    0.2739579120 0.2999315   0.8833584 0.08995892 Pasteur #> 5    0.1440153923 0.2999315   0.8833584 0.08995892 Pasteur #> 6   -1.0440895948 0.2999315   0.8833584 0.08995892 Pasteur #> 7    1.0507357396 0.2999315   0.8833584 0.08995892 Pasteur #> 8    0.1041882698 0.2999315   0.8833584 0.08995892 Pasteur #> 9    0.7750146375 0.2999315   0.8833584 0.08995892 Pasteur #> 10   0.4822117444 0.2999315   0.8833584 0.08995892 Pasteur #> 11  -0.4511886490 0.2999315   0.8833584 0.08995892 Pasteur #> 12   0.3522691973 0.2999315   0.8833584 0.08995892 Pasteur #> 13   0.0657041070 0.2999315   0.8833584 0.08995892 Pasteur #> 14   0.3259750264 0.2999315   0.8833584 0.08995892 Pasteur #> 15   1.3008341323 0.2999315   0.8833584 0.08995892 Pasteur #> 16  -0.2804588573 0.2999315   0.8833584 0.08995892 Pasteur #> 17  -0.3604017581 0.2999315   0.8833584 0.08995892 Pasteur #> 18  -0.7502293722 0.2999315   0.8833584 0.08995892 Pasteur #> 19   1.2600468631 0.2999315   0.8833584 0.08995892 Pasteur #> 20   0.2874908636 0.2999315   0.8833584 0.08995892 Pasteur #> 21  -1.1394826729 0.2999315   0.8833584 0.08995892 Pasteur #> 22  -0.3791151940 0.2999315   0.8833584 0.08995892 Pasteur #> 23   1.0444979094 0.2999315   0.8833584 0.08995892 Pasteur #> 24  -0.6248930150 0.2999315   0.8833584 0.08995892 Pasteur #> 25  -0.3689426673 0.2999315   0.8833584 0.08995892 Pasteur #> 26  -0.5663524842 0.2999315   0.8833584 0.08995892 Pasteur #> 27  -0.9568515617 0.2999315   0.8833584 0.08995892 Pasteur #> 28  -0.8137619455 0.2999315   0.8833584 0.08995892 Pasteur #> 29  -0.5028199109 0.2999315   0.8833584 0.08995892 Pasteur #> 30  -0.3054100713 0.2999315   0.8833584 0.08995892 Pasteur #> 31  -0.3728773637 0.2999315   0.8833584 0.08995892 Pasteur #> 32  -0.8529175744 0.2999315   0.8833584 0.08995892 Pasteur #> 33   0.4101382620 0.2999315   0.8833584 0.08995892 Pasteur #> 34   0.1848026386 0.2999315   0.8833584 0.08995892 Pasteur #> 35  -0.9680814159 0.2999315   0.8833584 0.08995892 Pasteur #> 36  -0.1436070439 0.2999315   0.8833584 0.08995892 Pasteur #> 37  -0.0126071782 0.2999315   0.8833584 0.08995892 Pasteur #> 38  -0.3531066368 0.2999315   0.8833584 0.08995892 Pasteur #> 39   1.0665717701 0.2999315   0.8833584 0.08995892 Pasteur #> 40   0.7993915544 0.2999315   0.8833584 0.08995892 Pasteur #> 41   0.1110975387 0.2999315   0.8833584 0.08995892 Pasteur #> 42  -0.2735495637 0.2999315   0.8833584 0.08995892 Pasteur #> 43  -0.7167372327 0.2999315   0.8833584 0.08995892 Pasteur #> 44  -0.6594424814 0.2999315   0.8833584 0.08995892 Pasteur #> 45   0.9507364688 0.2999315   0.8833584 0.08995892 Pasteur #> 46  -0.0478281107 0.2999315   0.8833584 0.08995892 Pasteur #> 47  -1.7573348450 0.2999315   0.8833584 0.08995892 Pasteur #> 48  -0.4620326417 0.2999315   0.8833584 0.08995892 Pasteur #> 49  -1.0305566597 0.2999315   0.8833584 0.08995892 Pasteur #> 50   0.7618675383 0.2999315   0.8833584 0.08995892 Pasteur #> 51  -1.3414986833 0.2999315   0.8833584 0.08995892 Pasteur #> 52  -0.0761397743 0.2999315   0.8833584 0.08995892 Pasteur #> 53  -1.8231705136 0.2999315   0.8833584 0.08995892 Pasteur #> 54   0.0094666323 0.2999315   0.8833584 0.08995892 Pasteur #> 55   0.0436302463 0.2999315   0.8833584 0.08995892 Pasteur #> 56  -0.0001315726 0.2999315   0.8833584 0.08995892 Pasteur #> 57  -0.6571393750 0.2999315   0.8833584 0.08995892 Pasteur #> 58   0.6121542642 0.2999315   0.8833584 0.08995892 Pasteur #> 59  -1.0957208458 0.2999315   0.8833584 0.08995892 Pasteur #> 60  -0.9289257761 0.2999315   0.8833584 0.08995892 Pasteur #> 61   0.9553426588 0.2999315   0.8833584 0.08995892 Pasteur #> 62   0.3647448029 0.2999315   0.8833584 0.08995892 Pasteur #> 63  -1.1003270330 0.2999315   0.8833584 0.08995892 Pasteur #> 64   0.4259742697 0.2999315   0.8833584 0.08995892 Pasteur #> 65   0.1689666035 0.2999315   0.8833584 0.08995892 Pasteur #> 66   0.6586050419 0.2999315   0.8833584 0.08995892 Pasteur #> 67  -0.2952375720 0.2999315   0.8833584 0.08995892 Pasteur #> 68  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 69  -0.5613604418 0.2999315   0.8833584 0.08995892 Pasteur #> 70   0.5632119383 0.2999315   0.8833584 0.08995892 Pasteur #> 71  -0.7769093955 0.2999315   0.8833584 0.08995892 Pasteur #> 72  -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 73  -0.7808440920 0.2999315   0.8833584 0.08995892 Pasteur #> 74   0.9270309952 0.2999315   0.8833584 0.08995892 Pasteur #> 75  -0.5426470306 0.2999315   0.8833584 0.08995892 Pasteur #> 76  -1.1065648385 0.2999315   0.8833584 0.08995892 Pasteur #> 77   0.6233841094 0.2999315   0.8833584 0.08995892 Pasteur #> 78  -1.7010974136 0.2999315   0.8833584 0.08995892 Pasteur #> 79  -0.0013773330 0.2999315   0.8833584 0.08995892 Pasteur #> 80  -1.2772946275 0.2999315   0.8833584 0.08995892 Pasteur #> 81  -1.0344913561 0.2999315   0.8833584 0.08995892 Pasteur #> 82   0.4345151789 0.2999315   0.8833584 0.08995892 Pasteur #> 83  -1.5280645151 0.2999315   0.8833584 0.08995892 Pasteur #> 84  -0.6101143231 0.2999315   0.8833584 0.08995892 Pasteur #> 85  -1.5122284832 0.2999315   0.8833584 0.08995892 Pasteur #> 86   1.2011204798 0.2999315   0.8833584 0.08995892 Pasteur #> 87  -0.3258523027 0.2999315   0.8833584 0.08995892 Pasteur #> 88   0.6687775411 0.2999315   0.8833584 0.08995892 Pasteur #> 89  -1.6382363147 0.2999315   0.8833584 0.08995892 Pasteur #> 90   0.3062042720 0.2999315   0.8833584 0.08995892 Pasteur #> 91  -0.4745082474 0.2999315   0.8833584 0.08995892 Pasteur #> 92  -0.6798846937 0.2999315   0.8833584 0.08995892 Pasteur #> 93  -1.1865077366 0.2999315   0.8833584 0.08995892 Pasteur #> 94  -0.5011882981 0.2999315   0.8833584 0.08995892 Pasteur #> 95   0.7362448336 0.2999315   0.8833584 0.08995892 Pasteur #> 96   0.4934415622 0.2999315   0.8833584 0.08995892 Pasteur #> 97   0.9661866515 0.2999315   0.8833584 0.08995892 Pasteur #> 98   1.7212764661 0.2999315   0.8833584 0.08995892 Pasteur #> 99  -0.8199997483 0.2999315   0.8833584 0.08995892 Pasteur #> 100  0.7369163271 0.2999315   0.8833584 0.08995892 Pasteur #> 101 -0.5403439270 0.2999315   0.8833584 0.08995892 Pasteur #> 102  0.0525570079 0.2999315   0.8833584 0.08995892 Pasteur #> 103  0.4973762860 0.2999315   0.8833584 0.08995892 Pasteur #> 104  0.5434412113 0.2999315   0.8833584 0.08995892 Pasteur #> 105  1.4015048920 0.2999315   0.8833584 0.08995892 Pasteur #> 106  0.5338429790 0.2999315   0.8833584 0.08995892 Pasteur #> 107  1.5005470281 0.2999315   0.8833584 0.08995892 Pasteur #> 108 -0.4353526184 0.2999315   0.8833584 0.08995892 Pasteur #> 109  1.7269399974 0.2999315   0.8833584 0.08995892 Pasteur #> 110 -0.1863115671 0.2999315   0.8833584 0.08995892 Pasteur #> 111  0.7431541299 0.2999315   0.8833584 0.08995892 Pasteur #> 112  0.3345159128 0.2999315   0.8833584 0.08995892 Pasteur #> 113  0.3111963144 0.2999315   0.8833584 0.08995892 Pasteur #> 114  0.6750153713 0.2999315   0.8833584 0.08995892 Pasteur #> 115 -1.3822859470 0.2999315   0.8833584 0.08995892 Pasteur #> 116  0.4299090164 0.2999315   0.8833584 0.08995892 Pasteur #> 117  0.4368182853 0.2999315   0.8833584 0.08995892 Pasteur #> 118 -0.6334339242 0.2999315   0.8833584 0.08995892 Pasteur #> 119 -0.9153928519 0.2999315   0.8833584 0.08995892 Pasteur #> 120 -0.2662544424 0.2999315   0.8833584 0.08995892 Pasteur #> 121 -0.1238362896 0.2999315   0.8833584 0.08995892 Pasteur #> 122 -0.1987871727 0.2999315   0.8833584 0.08995892 Pasteur #> 123  1.5676284956 0.2999315   0.8833584 0.08995892 Pasteur #> 124 -0.2906313821 0.2999315   0.8833584 0.08995892 Pasteur #> 125  0.7125393874 0.2999315   0.8833584 0.08995892 Pasteur #> 126  0.1324998831 0.2999315   0.8833584 0.08995892 Pasteur #> 127  1.1488177518 0.2999315   0.8833584 0.08995892 Pasteur #> 128  0.5559168169 0.2999315   0.8833584 0.08995892 Pasteur #> 129  0.8572606191 0.2999315   0.8833584 0.08995892 Pasteur #> 130 -0.9789254060 0.2999315   0.8833584 0.08995892 Pasteur #> 131  1.4416206448 0.2999315   0.8833584 0.08995892 Pasteur #> 132  0.4542859333 0.2999315   0.8833584 0.08995892 Pasteur #> 133 -1.3845890506 0.2999315   0.8833584 0.08995892 Pasteur #> 134 -0.2883282757 0.2999315   0.8833584 0.08995892 Pasteur #> 135  0.4430560881 0.2999315   0.8833584 0.08995892 Pasteur #> 136  1.2089899229 0.2999315   0.8833584 0.08995892 Pasteur #> 137  1.1942112109 0.2999315   0.8833584 0.08995892 Pasteur #> 138  0.6013102486 0.2999315   0.8833584 0.08995892 Pasteur #> 139  0.2371053667 0.2999315   0.8833584 0.08995892 Pasteur #> 140  1.0053422804 0.2999315   0.8833584 0.08995892 Pasteur #> 141  0.8095640810 0.2999315   0.8833584 0.08995892 Pasteur #> 142 -1.4408264861 0.2999315   0.8833584 0.08995892 Pasteur #> 143  1.3821199900 0.2999315   0.8833584 0.08995892 Pasteur #> 144  2.7284791267 0.2999315   0.8833584 0.08995892 Pasteur #> 145  0.0123440494 0.2999315   0.8833584 0.08995892 Pasteur #> 146  0.8277032180 0.2999315   0.8833584 0.08995892 Pasteur #> 147  0.7862444827 0.2999315   0.8833584 0.08995892 Pasteur #> 148 -1.1325734026 0.2999315   0.8833584 0.08995892 Pasteur #> 149  1.7660956264 0.2999315   0.8833584 0.08995892 Pasteur #> 150 -0.3712457509 0.2999315   0.8833584 0.08995892 Pasteur #> 151  1.8944065607 0.2999315   0.8833584 0.08995892 Pasteur #> 152  0.6098511578 0.2999315   0.8833584 0.08995892 Pasteur #> 153  0.2654170028 0.2999315   0.8833584 0.08995892 Pasteur #> 154 -1.1434174113 0.2999315   0.8833584 0.08995892 Pasteur #> 155 -0.6005160981 0.2999315   0.8833584 0.08995892 Pasteur #> 156  0.3562038937 0.2999315   0.8833584 0.08995892 Pasteur #>  #> $`Grant-White` #>           fs_f1  fs_f1_se f1_by_fs_f1   ev_fs_f1      school #> 1   -0.39525603 0.3152173   0.8801489 0.09936192 Grant-White #> 2   -0.63397248 0.3152173   0.8801489 0.09936192 Grant-White #> 3    0.20062403 0.3152173   0.8801489 0.09936192 Grant-White #> 4   -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 5    0.43519197 0.3152173   0.8801489 0.09936192 Grant-White #> 6    0.31151246 0.3152173   0.8801489 0.09936192 Grant-White #> 7    2.15612913 0.3152173   0.8801489 0.09936192 Grant-White #> 8   -0.29014192 0.3152173   0.8801489 0.09936192 Grant-White #> 9   -0.08369303 0.3152173   0.8801489 0.09936192 Grant-White #> 10  -0.01807396 0.3152173   0.8801489 0.09936192 Grant-White #> 11  -0.34820234 0.3152173   0.8801489 0.09936192 Grant-White #> 12  -2.10215974 0.3152173   0.8801489 0.09936192 Grant-White #> 13  -0.64552119 0.3152173   0.8801489 0.09936192 Grant-White #> 14  -1.46155228 0.3152173   0.8801489 0.09936192 Grant-White #> 15   1.02620880 0.3152173   0.8801489 0.09936192 Grant-White #> 16  -1.05064956 0.3152173   0.8801489 0.09936192 Grant-White #> 17   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 18   0.95822548 0.3152173   0.8801489 0.09936192 Grant-White #> 19  -0.25355303 0.3152173   0.8801489 0.09936192 Grant-White #> 20   1.42141427 0.3152173   0.8801489 0.09936192 Grant-White #> 21  -0.95400523 0.3152173   0.8801489 0.09936192 Grant-White #> 22   1.00295292 0.3152173   0.8801489 0.09936192 Grant-White #> 23   1.21841354 0.3152173   0.8801489 0.09936192 Grant-White #> 24  -1.24987101 0.3152173   0.8801489 0.09936192 Grant-White #> 25  -0.51984663 0.3152173   0.8801489 0.09936192 Grant-White #> 26  -0.04710419 0.3152173   0.8801489 0.09936192 Grant-White #> 27   0.43934048 0.3152173   0.8801489 0.09936192 Grant-White #> 28  -1.03117301 0.3152173   0.8801489 0.09936192 Grant-White #> 29  -0.95022593 0.3152173   0.8801489 0.09936192 Grant-White #> 30  -0.11417634 0.3152173   0.8801489 0.09936192 Grant-White #> 31  -0.40048841 0.3152173   0.8801489 0.09936192 Grant-White #> 32   0.10506359 0.3152173   0.8801489 0.09936192 Grant-White #> 33  -0.33541128 0.3152173   0.8801489 0.09936192 Grant-White #> 34  -1.55565961 0.3152173   0.8801489 0.09936192 Grant-White #> 35  -0.84420068 0.3152173   0.8801489 0.09936192 Grant-White #> 36   0.07802839 0.3152173   0.8801489 0.09936192 Grant-White #> 37   0.20116597 0.3152173   0.8801489 0.09936192 Grant-White #> 38  -2.52639546 0.3152173   0.8801489 0.09936192 Grant-White #> 39  -0.69149096 0.3152173   0.8801489 0.09936192 Grant-White #> 40   2.02343787 0.3152173   0.8801489 0.09936192 Grant-White #> 41   0.97338078 0.3152173   0.8801489 0.09936192 Grant-White #> 42   0.42040588 0.3152173   0.8801489 0.09936192 Grant-White #> 43  -0.92605892 0.3152173   0.8801489 0.09936192 Grant-White #> 44  -0.56690032 0.3152173   0.8801489 0.09936192 Grant-White #> 45   0.12021889 0.3152173   0.8801489 0.09936192 Grant-White #> 46   0.62108043 0.3152173   0.8801489 0.09936192 Grant-White #> 47  -1.34219405 0.3152173   0.8801489 0.09936192 Grant-White #> 48   0.16258210 0.3152173   0.8801489 0.09936192 Grant-White #> 49  -0.03231810 0.3152173   0.8801489 0.09936192 Grant-White #> 50   0.24444031 0.3152173   0.8801489 0.09936192 Grant-White #> 51  -0.74431900 0.3152173   0.8801489 0.09936192 Grant-White #> 52  -0.43798842 0.3152173   0.8801489 0.09936192 Grant-White #> 53  -1.85297845 0.3152173   0.8801489 0.09936192 Grant-White #> 54  -0.82563527 0.3152173   0.8801489 0.09936192 Grant-White #> 55   1.20039010 0.3152173   0.8801489 0.09936192 Grant-White #> 56  -0.33287433 0.3152173   0.8801489 0.09936192 Grant-White #> 57   0.01996797 0.3152173   0.8801489 0.09936192 Grant-White #> 58   1.67582800 0.3152173   0.8801489 0.09936192 Grant-White #> 59  -0.71906808 0.3152173   0.8801489 0.09936192 Grant-White #> 60  -0.20504632 0.3152173   0.8801489 0.09936192 Grant-White #> 61   1.96214007 0.3152173   0.8801489 0.09936192 Grant-White #> 62  -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 63  -0.35343474 0.3152173   0.8801489 0.09936192 Grant-White #> 64  -1.95809255 0.3152173   0.8801489 0.09936192 Grant-White #> 65  -1.36021750 0.3152173   0.8801489 0.09936192 Grant-White #> 66   0.08595623 0.3152173   0.8801489 0.09936192 Grant-White #> 67  -0.23407652 0.3152173   0.8801489 0.09936192 Grant-White #> 68   0.67805696 0.3152173   0.8801489 0.09936192 Grant-White #> 69  -0.42951863 0.3152173   0.8801489 0.09936192 Grant-White #> 70  -0.69203290 0.3152173   0.8801489 0.09936192 Grant-White #> 71  -0.71583072 0.3152173   0.8801489 0.09936192 Grant-White #> 72  -0.19603459 0.3152173   0.8801489 0.09936192 Grant-White #> 73  -0.36767888 0.3152173   0.8801489 0.09936192 Grant-White #> 74   1.77425660 0.3152173   0.8801489 0.09936192 Grant-White #> 75  -0.67924187 0.3152173   0.8801489 0.09936192 Grant-White #> 76   0.07603337 0.3152173   0.8801489 0.09936192 Grant-White #> 77   1.49895127 0.3152173   0.8801489 0.09936192 Grant-White #> 78  -0.78813525 0.3152173   0.8801489 0.09936192 Grant-White #> 79  -1.35643816 0.3152173   0.8801489 0.09936192 Grant-White #> 80  -0.34242798 0.3152173   0.8801489 0.09936192 Grant-White #> 81   1.10806701 0.3152173   0.8801489 0.09936192 Grant-White #> 82   1.04768034 0.3152173   0.8801489 0.09936192 Grant-White #> 83   1.02242947 0.3152173   0.8801489 0.09936192 Grant-White #> 84   0.38236395 0.3152173   0.8801489 0.09936192 Grant-White #> 85   0.56447306 0.3152173   0.8801489 0.09936192 Grant-White #> 86   0.78803426 0.3152173   0.8801489 0.09936192 Grant-White #> 87   0.43087072 0.3152173   0.8801489 0.09936192 Grant-White #> 88   0.40383552 0.3152173   0.8801489 0.09936192 Grant-White #> 89  -0.44376277 0.3152173   0.8801489 0.09936192 Grant-White #> 90   0.08126577 0.3152173   0.8801489 0.09936192 Grant-White #> 91   0.14833793 0.3152173   0.8801489 0.09936192 Grant-White #> 92   0.53922219 0.3152173   0.8801489 0.09936192 Grant-White #> 93   0.53598481 0.3152173   0.8801489 0.09936192 Grant-White #> 94  -0.17024174 0.3152173   0.8801489 0.09936192 Grant-White #> 95   0.43610310 0.3152173   0.8801489 0.09936192 Grant-White #> 96   2.22843366 0.3152173   0.8801489 0.09936192 Grant-White #> 97   1.36480696 0.3152173   0.8801489 0.09936192 Grant-White #> 98   0.65135298 0.3152173   0.8801489 0.09936192 Grant-White #> 99   1.69439338 0.3152173   0.8801489 0.09936192 Grant-White #> 100 -0.15745068 0.3152173   0.8801489 0.09936192 Grant-White #> 101 -0.27680894 0.3152173   0.8801489 0.09936192 Grant-White #> 102  1.80473991 0.3152173   0.8801489 0.09936192 Grant-White #> 103 -0.03286005 0.3152173   0.8801489 0.09936192 Grant-White #> 104  0.21541008 0.3152173   0.8801489 0.09936192 Grant-White #> 105  0.63586648 0.3152173   0.8801489 0.09936192 Grant-White #> 106 -0.44122580 0.3152173   0.8801489 0.09936192 Grant-White #> 107  0.24389836 0.3152173   0.8801489 0.09936192 Grant-White #> 108  0.97392270 0.3152173   0.8801489 0.09936192 Grant-White #> 109  1.00619031 0.3152173   0.8801489 0.09936192 Grant-White #> 110  0.95967859 0.3152173   0.8801489 0.09936192 Grant-White #> 111  2.10529611 0.3152173   0.8801489 0.09936192 Grant-White #> 112  0.95012491 0.3152173   0.8801489 0.09936192 Grant-White #> 113  1.14033462 0.3152173   0.8801489 0.09936192 Grant-White #> 114 -0.41527450 0.3152173   0.8801489 0.09936192 Grant-White #> 115  0.50263334 0.3152173   0.8801489 0.09936192 Grant-White #> 116  0.00518188 0.3152173   0.8801489 0.09936192 Grant-White #> 117 -0.30961846 0.3152173   0.8801489 0.09936192 Grant-White #> 118 -0.38570232 0.3152173   0.8801489 0.09936192 Grant-White #> 119 -0.88747505 0.3152173   0.8801489 0.09936192 Grant-White #> 120 -1.11340047 0.3152173   0.8801489 0.09936192 Grant-White #> 121 -0.22830216 0.3152173   0.8801489 0.09936192 Grant-White #> 122  0.16781447 0.3152173   0.8801489 0.09936192 Grant-White #> 123 -1.16622845 0.3152173   0.8801489 0.09936192 Grant-White #> 124 -0.45277447 0.3152173   0.8801489 0.09936192 Grant-White #> 125 -0.69527031 0.3152173   0.8801489 0.09936192 Grant-White #> 126  1.16558552 0.3152173   0.8801489 0.09936192 Grant-White #> 127 -0.49081643 0.3152173   0.8801489 0.09936192 Grant-White #> 128  0.45412656 0.3152173   0.8801489 0.09936192 Grant-White #> 129 -0.75910506 0.3152173   0.8801489 0.09936192 Grant-White #> 130 -0.46232819 0.3152173   0.8801489 0.09936192 Grant-White #> 131  1.33631868 0.3152173   0.8801489 0.09936192 Grant-White #> 132 -0.78236094 0.3152173   0.8801489 0.09936192 Grant-White #> 133  0.11407532 0.3152173   0.8801489 0.09936192 Grant-White #> 134 -0.26111172 0.3152173   0.8801489 0.09936192 Grant-White #> 135 -0.58492377 0.3152173   0.8801489 0.09936192 Grant-White #> 136 -1.40872427 0.3152173   0.8801489 0.09936192 Grant-White #> 137 -0.24308825 0.3152173   0.8801489 0.09936192 Grant-White #> 138 -0.17601610 0.3152173   0.8801489 0.09936192 Grant-White #> 139 -0.74486092 0.3152173   0.8801489 0.09936192 Grant-White #> 140 -0.13419481 0.3152173   0.8801489 0.09936192 Grant-White #> 141 -0.74809830 0.3152173   0.8801489 0.09936192 Grant-White #> 142 -0.93452871 0.3152173   0.8801489 0.09936192 Grant-White #> 143  0.88737403 0.3152173   0.8801489 0.09936192 Grant-White #> 144 -0.05665784 0.3152173   0.8801489 0.09936192 Grant-White #> 145  0.58303847 0.3152173   0.8801489 0.09936192 Grant-White #>  #> attr(,\"fsT\") #> attr(,\"fsT\")$Pasteur #>            fs_f1 #> fs_f1 0.08995892 #>  #> attr(,\"fsT\")$`Grant-White` #>            fs_f1 #> fs_f1 0.09936192 #>  #> attr(,\"fsL\") #> attr(,\"fsL\")$Pasteur #>              f1 #> fs_f1 0.8833584 #>  #> attr(,\"fsL\")$`Grant-White` #>              f1 #> fs_f1 0.8801489 #>  #> attr(,\"fsb\") #> attr(,\"fsb\")$Pasteur #> fs_f1  #>     0  #>  #> attr(,\"fsb\")$`Grant-White` #> fs_f1  #>     0  #>  #> attr(,\"scoring_matrix\") #> attr(,\"scoring_matrix\")$Pasteur #>         [,1]      [,2]      [,3] #> f1 0.2280246 0.3381964 0.2740895 #>  #> attr(,\"scoring_matrix\")$`Grant-White` #>         [,1]      [,2]      [,3] #> f1 0.3580747 0.2682886 0.2662936 #>"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":null,"dir":"Reference","previous_headings":"","what":"Grand Standardized Solution — grand_standardized_solution","title":"Grand Standardized Solution — grand_standardized_solution","text":"Grand standardized solution two-stage path analysis model.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grand Standardized Solution — grand_standardized_solution","text":"","code":"grand_standardized_solution(   object,   model_list = NULL,   se = TRUE,   acov_par = NULL,   free_list = NULL,   level = 0.95 )  grandStandardizedSolution(   object,   model_list = NULL,   se = TRUE,   acov_par = NULL,   free_list = NULL,   level = 0.95 )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grand Standardized Solution — grand_standardized_solution","text":"object object class lavaan. model_list list string variable describing structural path model, lavaan syntax. se Boolean variable. TRUE, standard errors grand standardized parameters computed. acov_par asymptotic variance-covariance matrix fitted model object. free_list list model matrices indicate position free parameters parameter vector. level confidence level required.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Grand Standardized Solution — grand_standardized_solution","text":"matrix standardized model parameters standard errors.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/grand_standardized_solution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Grand Standardized Solution — grand_standardized_solution","text":"","code":"library(lavaan)  ## A single-group, two-factor example mod1 <- '    # latent variables      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4    # regressions      dem60 ~ ind60 ' fit1 <- sem(model = mod1,           data  = PoliticalDemocracy) grand_standardized_solution(fit1) #> The grand standardized solution is equivalent to lavaan::standardizedSolution() for a model with a single group. #>     lhs op   rhs exo group block label est.std  se     z pvalue ci.lower #> 8 dem60  ~ ind60   0     1     1          0.46 0.1 4.593      0    0.264 #>   ci.upper #> 8    0.657  ## A single-group, three-factor example mod2 <- '     # latent variables       ind60 =~ x1 + x2 + x3       dem60 =~ y1 + y2 + y3 + y4       dem65 =~ y5 + y6 + y7 + y8     # regressions       dem60 ~ ind60       dem65 ~ ind60 + dem60 ' fit2 <- sem(model = mod2,             data  = PoliticalDemocracy) grand_standardized_solution(fit2) #> The grand standardized solution is equivalent to lavaan::standardizedSolution() for a model with a single group. #>      lhs op   rhs exo group block label est.std    se      z pvalue ci.lower #> 12 dem60  ~ ind60   0     1     1         0.448 0.102  4.393  0.000    0.248 #> 13 dem65  ~ ind60   0     1     1         0.146 0.070  2.071  0.038    0.008 #> 14 dem65  ~ dem60   0     1     1         0.913 0.048 19.120  0.000    0.819 #>    ci.upper #> 12    0.648 #> 13    0.283 #> 14    1.006  ## A multigroup, two-factor example mod3 <- '   # latent variable definitions     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9   # regressions     visual ~ c(b1, b1) * speed ' fit3 <- sem(mod3, data = HolzingerSwineford1939,             group = \"school\",             group.equal = c(\"loadings\", \"intercepts\")) grand_standardized_solution(fit3) #>       lhs op   rhs exo group block label est.std    se     z pvalue ci.lower #> 7  visual  ~ speed   0     1     1    b1   0.431 0.073 5.867      0    0.287 #> 30 visual  ~ speed   0     2     2    b1   0.431 0.073 5.867      0    0.287 #>    ci.upper #> 7     0.575 #> 30    0.575  ## A multigroup, three-factor example mod4 <- '   # latent variable definitions     visual =~ x1 + x2 + x3     textual =~ x4 + x5 + x6     speed =~ x7 + x8 + x9    # regressions     visual ~ c(b1, b1) * textual + c(b2, b2) * speed ' fit4 <- sem(mod4, data = HolzingerSwineford1939,             group = \"school\",             group.equal = c(\"loadings\", \"intercepts\")) grand_standardized_solution(fit4) #>       lhs op     rhs exo group block label est.std    se     z pvalue ci.lower #> 10 visual  ~ textual   0     1     1    b1   0.419 0.073 5.704      0    0.275 #> 11 visual  ~   speed   0     1     1    b2   0.324 0.078 4.145      0    0.171 #> 46 visual  ~ textual   0     2     2    b1   0.419 0.073 5.704      0    0.275 #> 47 visual  ~   speed   0     2     2    b2   0.324 0.078 4.145      0    0.171 #>    ci.upper #> 10    0.563 #> 11    0.477 #> 46    0.563 #> 47    0.477"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":null,"dir":"Reference","previous_headings":"","what":"Two-Stage Path Analysis — tspa","title":"Two-Stage Path Analysis — tspa","text":"Fit two-stage path analysis (2S-PA) model.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Two-Stage Path Analysis — tspa","text":"","code":"tspa(   model,   data,   reliability = NULL,   se = \"standard\",   se_fs = NULL,   fsT = NULL,   fsL = NULL,   fsb = NULL,   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Two-Stage Path Analysis — tspa","text":"model string variable describing structural path model, lavaan syntax. data data frame containing factor scores. reliability numeric vector representing reliability indexes latent factor. Currently tspa() support reliability argument. Please use se. se Deprecated avoid conflict argument name lavaan::lavaan(). se_fs numeric vector representing standard errors factor score variable single-group 2S-PA. list data frame storing standard errors group latent factor multigroup 2S-PA. fsT error variance-covariance matrix factor scores, can obtained output get_fs() using attr() argument = \"fsT\". fsL matrix loadings cross-loadings latent variables factor scores fs, can obtained output get_fs() using attr() argument = \"fsL\". details see multiple-factors vignette: vignette(\"multiple-factors\", package = \"R2spa\"). fsb vector intercepts factor scores fs, can obtained output get_fs() using attr() argument = \"fsb\". ... Additional arguments passed sem. See lavOptions complete list.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Two-Stage Path Analysis — tspa","text":"object class lavaan, attribute tspaModel contains model syntax.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Two-Stage Path Analysis — tspa","text":"","code":"library(lavaan)  # single-group, two-factor example, factor scores obtained separately # get factor scores fs_dat_ind60 <- get_fs(data = PoliticalDemocracy,                        model = \"ind60 =~ x1 + x2 + x3\") fs_dat_dem60 <- get_fs(data = PoliticalDemocracy,                        model = \"dem60 =~ y1 + y2 + y3 + y4\") fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60) # tspa model tspa(model = \"dem60 ~ ind60\", data = fs_dat,      se_fs = c(ind60 = fs_dat_ind60[1, \"fs_ind60_se\"],                dem60 = fs_dat_dem60[1, \"fs_dem60_se\"])) #> lavaan 0.6-18 ended normally after 17 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         3 #>  #>   Number of observations                            75 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0  # single-group, three-factor example mod2 <- \"   # latent variables     ind60 =~ x1 + x2 + x3     dem60 =~ y1 + y2 + y3 + y4     dem65 =~ y5 + y6 + y7 + y8 \" fs_dat2 <- get_fs(PoliticalDemocracy, model = mod2, std.lv = TRUE) tspa(model = \"dem60 ~ ind60               dem65 ~ ind60 + dem60\",      data = fs_dat2,      fsT = attr(fs_dat2, \"fsT\"),      fsL = attr(fs_dat2, \"fsL\")) #> lavaan 0.6-18 ended normally after 21 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         6 #>  #>   Number of observations                            75 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0  # multigroup, two-factor example mod3 <- \"   # latent variables     visual =~ x1 + x2 + x3     speed =~ x7 + x8 + x9 \" fs_dat3 <- get_fs(HolzingerSwineford1939, model = mod3, std.lv = TRUE,                   group = \"school\") tspa(model = \"visual ~ speed\",      data = fs_dat3,      fsT = attr(fs_dat3, \"fsT\"),      fsL = attr(fs_dat3, \"fsL\"),      group = \"school\") #> lavaan 0.6-18 ended normally after 28 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0 #>   Test statistic for each group: #>     Pasteur                                      0.000 #>     Grant-White                                  0.000  # multigroup, three-factor example mod4 <- \"   # latent variables     visual =~ x1 + x2 + x3     textual =~ x4 + x5 + x6     speed =~ x7 + x8 + x9 \" fs_dat4 <- get_fs(HolzingerSwineford1939, model = mod4, std.lv = TRUE,                   group = \"school\") tspa(model = \"visual ~ speed               textual ~ visual + speed\",      data = fs_dat4,      fsT = attr(fs_dat4, \"fsT\"),      fsL = attr(fs_dat4, \"fsL\"),      group = \"school\") #> lavaan 0.6-18 ended normally after 39 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        18 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0 #>   Test statistic for each group: #>     Pasteur                                      0.000 #>     Grant-White                                  0.000  # get factor scores fs_dat_visual <- get_fs(data = HolzingerSwineford1939,                         model = \"visual =~ x1 + x2 + x3\",                         group = \"school\") fs_dat_speed <- get_fs(data = HolzingerSwineford1939,                        model = \"speed =~ x7 + x8 + x9\",                        group = \"school\") fs_hs <- cbind(do.call(rbind, fs_dat_visual),                do.call(rbind, fs_dat_speed))  # tspa model tspa(model = \"visual ~ speed\",      data = fs_hs,      se_fs = data.frame(visual = c(0.3391326, 0.311828),                         speed = c(0.2786875, 0.2740507)),      group = \"school\",      group.equal = \"regressions\") #> lavaan 0.6-18 ended normally after 19 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>   Number of equality constraints                     1 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.014 #>   Degrees of freedom                                 1 #>   P-value (Chi-square)                           0.907 #>   Test statistic for each group: #>     Pasteur                                      0.010 #>     Grant-White                                  0.003  # manually adding equality constraints on the regression coefficients tspa(model = \"visual ~ c(b1, b1) * speed\",      data = fs_hs,      se_fs = list(visual = c(0.3391326, 0.311828),                   speed = c(0.2786875, 0.2740507)),      group = \"school\") #> lavaan 0.6-18 ended normally after 19 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>   Number of equality constraints                     1 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.014 #>   Degrees of freedom                                 1 #>   P-value (Chi-square)                           0.907 #>   Test statistic for each group: #>     Pasteur                                      0.010 #>     Grant-White                                  0.003"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"Fit two-stage path analysis (2S-PA) model.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"","code":"tspa_mx_model(   mx_model,   data,   mat_ld,   mat_ev,   mat_int = NULL,   fs_lv_names = NULL,   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"mx_model model object class OpenMx::MxRAMModel, created OpenMx::mxModel() umx package. structural model part. data data frame containing factor scores. mat_ld \\(p \\times p\\) matrix indicating loadings factor scores latent variables. ith row indicate loadings ith factor score variable latent variables. one following: matrix created OpenMx::mxMatrix() loading values. named numeric matrix, rownames column names matching factor score latent variables. named character matrix, rownames column names matching factor score latent variables, character values indicating variable names data corresponding loadings. mat_ev Similar mat_ld error variance-covariance matrix factor scores. mat_int Similar mat_ld measurement intercept matrix factor scores. fs_lv_names named character vector element name factor score variable, names vector corresponding name latent variables. ... Additional arguments passed OpenMx::mxModel().","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"object class OpenMx::MxModel. Note model run.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_mx_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Two-Stage Path Analysis with Definition Variables Using OpenMx — tspa_mx_model","text":"","code":"library(mirt) #> Loading required package: stats4 #> Loading required package: lattice library(umx) #> Loading required package: OpenMx #> To take full advantage of multiple cores, use: #>   mxOption(key='Number of Threads', value=parallel::detectCores()) #now #>   Sys.setenv(OMP_NUM_THREADS=parallel::detectCores()) #before library(OpenMx) #> For an overview type '?umx' #>  #> Attaching package: ‘umx’ #> The following object is masked from ‘package:stats’: #>  #>     loadings library(OpenMx) # Simulate data with mirt set.seed(1324) num_obs <- 100 # Simulate theta eta <- MASS::mvrnorm(num_obs, mu = c(0, 0), Sigma = diag(c(1, 1 - 0.5^2)),                      empirical = TRUE) th1 <- eta[, 1] th2 <- -1 + 0.5 * th1 + eta[, 2] # items and response data a1 <- matrix(1, 10) d1 <- matrix(rnorm(10)) a2 <- matrix(runif(10, min = 0.5, max = 1.5)) d2 <- matrix(rnorm(10)) dat1 <- mirt::simdata(a = a1, d = d1,                       N = num_obs, itemtype = \"2PL\", Theta = th1) dat2 <- mirt::simdata(a = a2, d = d2, N = num_obs,                       itemtype = \"2PL\", Theta = th2) # Factor scores mod1 <- mirt(dat1, model = 1, itemtype = \"Rasch\", verbose = FALSE) mod2 <- mirt(dat2, model = 1, itemtype = \"2PL\", verbose = FALSE) fs1 <- fscores(mod1, full.scores.SE = TRUE) fs2 <- fscores(mod2, full.scores.SE = TRUE) # Combine factor scores and standard errors into data set fs_dat <- as.data.frame(cbind(fs1, fs2)) names(fs_dat) <- c(\"fs1\", \"se_fs1\", \"fs2\", \"se_fs2\") # Compute reliability and error variances fs_dat <- within(fs_dat, expr = {   rel_fs1 <- 1 - se_fs1^2   rel_fs2 <- 1 - se_fs2^2   ev_fs1 <- se_fs1^2 * (1 - se_fs1^2)   ev_fs2 <- se_fs2^2 * (1 - se_fs2^2) }) # OpenMx model (from umx so that lavaan syntax can be used) fsreg_umx <- umxLav2RAM(   \"     fs2 ~ fs1     fs2 + fs1 ~ 1   \",   printTab = FALSE) #>  #> ?plot.MxModel options: std, means, digits, strip_zero, file, splines=T/F/ortho,..., min=, max =, same = , fixed, resid= 'circle|line|none' # Prepare loading and error covariance matrices cross_load <- matrix(c(\"rel_fs2\", NA, NA, \"rel_fs1\"), nrow = 2) |>   `dimnames<-`(rep(list(c(\"fs2\", \"fs1\")), 2)) err_cov <- matrix(c(\"ev_fs2\", NA, NA, \"ev_fs1\"), nrow = 2) |>   `dimnames<-`(rep(list(c(\"fs2\", \"fs1\")), 2)) # Create 2S-PA model (with definition variables) tspa_mx <-   tspa_mx_model(fsreg_umx,     data = fs_dat,     mat_ld = cross_load,     mat_ev = err_cov   ) # Run OpenMx tspa_mx_fit <- mxRun(tspa_mx) #> Running 2SPAD with 5 parameters # Summarize the results summary(tspa_mx_fit) #> Summary of 2SPAD  #>   #> free parameters: #>           name matrix row col     Estimate Std.Error A #> 1   fs1_to_fs2   m1.A fs2 fs1  0.526025404 0.2351855   #> 2 fs2_with_fs2   m1.S fs2 fs2  0.857316415 0.2253235   #> 3 fs1_with_fs1   m1.S fs1 fs1  0.541308628 0.1472492   #> 4   one_to_fs2   m1.M   1 fs2 -0.003189929 0.1234779   #> 5   one_to_fs1   m1.M   1 fs1  0.003287301 0.1002638   #>  #> Model Statistics:  #>                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units) #>        Model:              5                    395              453.4113 #>    Saturated:             NA                     NA                    NA #> Independence:             NA                     NA                    NA #> Number of observations/statistics: 100/400 #>  #> Information Criteria:  #>       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted #> AIC:      -336.5887               463.4113                 464.0496 #> BIC:     -1365.6310               476.4371                 460.6459 #> CFI: NA  #> TLI: 1   (also known as NNFI)  #> RMSEA:  0  [95% CI (NA, NA)] #> Prob(RMSEA <= 0.05): NA #> To get additional fit indices, see help(mxRefModels) #> timestamp: 2024-09-06 04:15:37  #> Wall clock time: 0.152458 secs  #> optimizer:  SLSQP  #> OpenMx version number: 2.21.12  #> Need help?  See help(mxSummary)  #>"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Diagnostic plots of fitted 2S-PA model — tspa_plot","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"Diagnostic plots fitted 2S-PA model","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"","code":"tspa_plot(   tspa_fit,   title = NULL,   label_x = NULL,   label_y = NULL,   abbreviation = TRUE,   fscores_type = c(\"original\", \"lavaan\"),   ask = FALSE,   ... )"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"tspa_fit object class lavaan, representing output generated tspa() function. title Character. Set name scatter plot. default value \"Scatterplot\". label_x Character. Set name x-axis. default value \"fs_\" followed variable names. label_y Character. Set name y-axis. default value \"fs_\" followed variable names. abbreviation Logic input. FALSE indicated, group name shown full. default setting TRUE. fscores_type Character. Set type factor score input. default setting using factor score observed data (.e., output get_fs()). fscore_type = \"est\", use output lavaan::lavPredict(). ask Logic input. TRUE indicated, user asked plot generated. default setting 'False'. ... Additional arguments passed plot. See plot list.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"scatterplot factor scores, residual plot.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/tspa_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Diagnostic plots of fitted 2S-PA model — tspa_plot","text":"","code":"library(lavaan) model <- \" # latent variable definitions ind60 =~ x1 + x2 + x3 dem60 =~ y1 + a*y2 + b*y3 + c*y4 # regressions dem60 ~ ind60 \" fs_dat_ind60 <- get_fs(   data = PoliticalDemocracy,   model = \"ind60 =~ x1 + x2 + x3\" ) fs_dat_dem60 <- get_fs(   data = PoliticalDemocracy,   model = \"dem60 =~ y1 + y2 + y3 + y4\" ) fs_dat <- cbind(fs_dat_ind60, fs_dat_dem60)  tspa_fit <- tspa(   model = \"dem60 ~ ind60\",   data = fs_dat,   se_fs = list(ind60 = 0.1213615, dem60 = 0.6756472) ) tspa_plot(tspa_fit)"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/vcov_corrected.html","id":null,"dir":"Reference","previous_headings":"","what":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","title":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","text":"First-order correction sampling covariance 2S-PA estimates","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/vcov_corrected.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","text":"","code":"vcov_corrected(tspa_fit, vfsLT, which_free = NULL, ...)"},{"path":"https://gengrui-zhang.github.io/R2spa/reference/vcov_corrected.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","text":"tspa_fit fitted model tspa(). vfsLT sampling covariance matrix fsL fsT, can obtained get_fs() argument vfsLT = TRUE. which_free optional numeric vector indicating parameters fsL fsT free. parameters ordered fsL matrix lower-triangular part fsT, columns. example, two-factor model, fsL fsT 2 x 2 matrices, error covariance two factor scores (.e., [2, 1] element fsT) index 6. ... Currently used.","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/reference/vcov_corrected.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"First-order correction of sampling covariance for 2S-PA estimates — vcov_corrected","text":"corrected covariance matrix dimension vcov(tspa_fit).","code":""},{"path":[]},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"new-features-0-0-4","dir":"Changelog","previous_headings":"","what":"New Features","title":"R2spa 0.0.4","text":"Add function get_fs_int() estimating interaction effects 2S-PA (#82). Add computation reliability function get_fscore() (#81) Add functions obtaining tidy-ed factor scores data get_fscore() (#79)","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"improvements-0-0-4","dir":"Changelog","previous_headings":"","what":"Improvements","title":"R2spa 0.0.4","text":"Add examples get_fs_int() (#82). Rename vc ev (error variance-covariance) better consistency","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"bug-fixes-0-0-4","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"R2spa 0.0.4","text":"Fix bug se_fs argument tspa() (#90).","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"documentation-0-0-4","dir":"Changelog","previous_headings":"","what":"Documentation","title":"R2spa 0.0.4","text":"tspa-growth-vignette (#50) get_fs_int-vignette (#82) reliability (#81) missing-data (#79)","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"other-0-0-4","dir":"Changelog","previous_headings":"","what":"Other","title":"R2spa 0.0.4","text":"General code clean-(#82).","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"r2spa-003","dir":"Changelog","previous_headings":"","what":"R2spa 0.0.3","title":"R2spa 0.0.3","text":"Add function tspa_plot() bivariate residual plots (#23) get_fs() gains argument corrected_fsT computing corrected error estimates (#50) New function vcov_corrected() computing corrected SEs (#39) New function get_fs_lavaan() computing factor scores relevant matrices directly lavaan output (#61) Initial support 2S-PA OpenMx tspa_mx() Update naming relevant matrices computing factor scores: fsT: error covariance factor scores fsL: loading matrix factor scores fsb: intercepts factor scores scoring_matrix: weights computing factor scores items New vignettes : Corrected error variance factor scores (#50) Corrected standard errors incorporating uncertainty measurement parameters factor scores (#39) Using 2S-PA EFA scores Using 2S-PA OpenMx definition variables (PR #57) Latent interaction categorical indicators (#27) Growth modeling Better error messages tspa() (#53) Support mean structure growth model (#36, #19) Clean code lintr (#33)","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"r2spa-002","dir":"Changelog","previous_headings":"","what":"R2spa 0.0.2","title":"R2spa 0.0.2","text":"Use pkgdown create website, GitHub action (#22) get_fs() now returns list multi-group models (#29). New function grandStandardizedSolution() computes standardized solution based grand mean grand SD (#13). tspa() gains argument vc cross_loadings, useful factor scores obtained multi-factor models (#7). See vignette(\"multiple-factors\").","code":""},{"path":"https://gengrui-zhang.github.io/R2spa/news/index.html","id":"r2spa-001","dir":"Changelog","previous_headings":"","what":"R2spa 0.0.1","title":"R2spa 0.0.1","text":"Work--Progress! 0.0.1 version","code":""}]
