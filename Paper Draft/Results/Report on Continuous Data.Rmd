---
title             : "Report on Continuous Data"

author: 
  - name          : "Jimmy"

bibliography      : "r-references.bib"

floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
header-includes   :
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(papaja)

sim_0827 <- readRDS("/Users/jimmy_z/R Projects/R2spa/simulation_result_0827.rds")
sim_results <- sim_0827 %>% 
  dplyr::select(-c(beta1, beta2, c(SIM_TIME:WARNINGS), REPLICATIONS)) %>%
  mutate(across(where(is.numeric), round, 3))
```

## Standardized Bias of $\beta_{XM}$

In Table 1, the standardized biases of the path regarding the interaction effect (i.e., $\beta_{XM}$) were all within an absolute value (0.40) regarded as a commonly acceptable threshold (Collins et al., 2001), with a range of [-.37, .30]. The unconstrained product indicator (UPI) method yielded the most negative standardized bias (-.37) whereas the reliability-adjusted product indicator (RAPI) method yielded the most positiv one (.30). 

The effect of the population reliability levels was obvious on the bias estimates generated by the RAPI model, such that larger population reliability resulted in smaller magnitude of standardized bias. For example, when the sample size (N) was 250 and the correlation between two latent indicators (i.e., X and M) was 0, the bias estimates went down for RAPI (from 0.30 to 0.20 to 0.15). Such a decreasing trend was found in the 2SPA (i.e., two-stage path analysis) model as well, but slighter compared to the RAPI model (from 0.10 to -0.01 to -0.05). The impact of correlation between X and M did not demonstrate a clear pattern but showed an unignorable effect on the bias estimates. The absolute difference on the bias estimates across three correlation levels (i.e., 0.7-0.9) ranged from 0 to 0.13 across overall records. Specifically, a few decreasing trends appeared in the UPI model estimates (e.g., -0.22 to -0.17 to -0.16 under N = 250 and $\rho$ = 0.8) with higher reliability levels. When the sample size was large and reliability was relatively low, there was a decrease in the interaction estimates for 2SPA (e.g,, 0.09 to 0.08 to 0.04) and RAPI (e.g, 0.26 to 0.25 to 0.20) as well. To sum up, results indicate that measurement errors in the latent predictors play an important role in estimating the interaction effects such that a higher level of errors leads to more underestimated coefficients. 

It was found that the standardized bias estimates of the RAPI method were positive while those of the UPI method were negative across every simulation condition. As for the 2SPA method, the bias estimates distributed closer to the center (i.e., 0) with positive and negative values. For example, when the correlation was 0.8 and sample size was 250, the standardized bias was 0.16 for RAPI, -0.22 for UPI, and -0.01 for 2SPA. The above findings suggest that the RAPI method and the UPI method were more likely to yield biased estimates of the interaction effect than the 2SPA method, in which the UPI was more prone to conservative interaction estimates whereas the RAPI was more prone to overestimating the effects.

```{r standardized bias, message=FALSE, warning=FALSE}
std_bias <- sim_results %>% 
  dplyr::select(N:std_bias.tspa_yint_est) %>%
  filter(beta3 == 0.75) %>%
  dplyr::select(-beta3) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(N = ifelse(cor_xm == 0.3 | cor_xm == 0.6, " ", N),
         rel = ifelse(cor_xm == 0.3 | cor_xm == 0.6, " ", rel),
         cor_xm = ifelse(cor_xm == 0, "0", cor_xm))
names(std_bias) <- c("N", "$\\rho$", "Correlation", "RAPI", "UPI", "2SPA")

std_table <- apa_table(std_bias, 
                       escape = F,
                       caption = "Standardized Bias of $\\rho$ from 2000 Replications",
                       align = c(rep("c", ncol(std_bias))),
                       note = "The results were based on $\\beta$ = (1, 0.9, 0.75).")

std_table
```

## Relative SE Bias of $\beta_{XM}$

The relative SE bias results of three methods were presented in Table 2. It was found that the magnitudes of relative SE bias were all negative and out of the acceptable range (-10$\%$, 10$\%$) under the condition of low sample size and low reliability level. The SE bias estimates of the RAPI and 2SPA methods all fell within the 10$\%$ range while the UPI had some estimates slightly out of range (e.g., -11$\%$) when the sample size was 500 and reliability was 0.9. The 2SPA method had the lowest relative SE range and (-2$\%$ to -14$\%$) across three methods, in which the bias estimates under two-thirds of simulation conditions were within the 10$\%$ range. The RAPI method had slightly better performance than the UPI method, such that its bias estimates range was lower (RAPI: -3$\%$ to 23$\%$; UPI: -10$\%$ to -70$\%$).


```{r}
rse_bias <- sim_results %>% 
  dplyr::select(N:rel, rse_bias.rapi_yint_se:rse_bias.tspa_yint_se) %>%
  filter(beta3 == 0.75) %>%
  dplyr::select(-beta3) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(N = ifelse(cor_xm == 0.3 | cor_xm == 0.6, " ", N),
         rel = ifelse(cor_xm == 0.3 | cor_xm == 0.6, " ", rel),
         cor_xm = ifelse(cor_xm == 0, "0", cor_xm),
         rse_bias.rapi_yint_se = rse_bias.rapi_yint_se*100,
         rse_bias.upi_yint_se = rse_bias.upi_yint_se*100,
         rse_bias.tspa_yint_se = rse_bias.tspa_yint_se*100)
names(rse_bias) <- c("N", "$\\rho$", "Correlation", "RAPI", "UPI", "2SPA")

rse_table <- apa_table(rse_bias, 
                       escape = F,
                       caption = "Relative SE Bias ($\\%$) of $\\rho$ from 2000 Replications",
                       align = c(rep("c", ncol(rse_bias))),
                       note = "The results were based on $\\beta$ = (1, 0.9, 0.75).")

rse_table
```

## Coverage Rate of 95% CI of $\beta_{XM}$

```{r}
coverage <- sim_results %>% 
  dplyr::select(N:rel, coverage.rapi_yint_est:coverage.tspa_yint_est) %>%
  filter(beta3 == 0.75) %>%
  dplyr::select(-beta3) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(N = ifelse(cor_xm == 0.3 | cor_xm == 0.6, " ", N),
         rel = ifelse(cor_xm == 0.3 | cor_xm == 0.6, " ", rel),
         cor_xm = ifelse(cor_xm == 0, "0", cor_xm),
         coverage.rapi_yint_est = coverage.rapi_yint_est*100,
         coverage.upi_yint_est = coverage.upi_yint_est*100,
         coverage.tspa_yint_est = coverage.tspa_yint_est*100)
names(coverage) <- c("N", "$\\rho$", "Correlation", "RAPI", "UPI", "2SPA")

coverage_table <- apa_table(coverage, 
                       escape = F,
                       caption = "Coverage Rate of 95$\\%$ CI of $\\rho$ from 2000 Replications",
                       align = c(rep("c", ncol(coverage))),
                       note = "The results were based on $\\beta$ = (1, 0.9, 0.75).")

coverage_table
```

As shown in Table 3, only the coverage rate of 95$\%$ CI for the 2SPA model was adequate (91.20-94.60$\%$) regarding to the recommended criteria (i.e., 91-98$\%$). The RAPI method demonstrated barely satisfied coverage rates in which 2 rates out of 18 conditions were slightly below 91$\%$. The UPI method generally did not meet the criteria with obvious gaps such that it yielded 70-89$\%$ or so coverage rates under most of the conditions. The result indicated that 2SPA had a higher probability of capturing the true latent interaction effect than the RAPI and UPI methods.

## RMSE of $\beta_{XM}$

```{r}
rmse <- sim_results %>% 
  dplyr::select(N:rel, rmse.rapi_yint_est:rmse.tspa_yint_est) %>%
  filter(beta3 == 0.75) %>%
  dplyr::select(-beta3) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(N = ifelse(cor_xm == 0.3 | cor_xm == 0.6, " ", N),
         rel = ifelse(cor_xm == 0.3 | cor_xm == 0.6, " ", rel),
         cor_xm = ifelse(cor_xm == 0, "0", cor_xm))
names(rmse) <- c("N", "$\\rho$", "Correlation", "RAPI", "UPI", "2SPA")

rmse_table <- apa_table(rmse, 
                       escape = F,
                       caption = "RMSE of $\\rho$ from 2000 Replications",
                       align = c(rep("c", ncol(rmse))),
                       note = "The results were based on $\\beta$ = (1, 0.9, 0.75).")

rmse_table
```

The RMSE values were presented in Table 4. It was obviously found that the RMSE values generally decreased as the sample and reliability level increased across three methods. The 2SPA method demonstrated the best performance in terms of the range of RMSE values (0.04-0.11) than the RAPI (0.04-0.14) and UPI (0.04-0.33) methods, in which the maximum RMSE value generated by the UPI was ostensibly larger than the other two methods. It indicated that the 2SPA method was the most effective and accurate in estimating the latent interaction effects, whereas the RAPI method was more acceptable than the UPI method. Nevertheless, such advantage of the 2SPA method was more prominent under the small sample size and reliability condition. For example, when the sample size was 250 and the reliability was 0.7, the difference on RMSE values among three methods could be as large as 0.24. As the sample size and reliability increased, such difference was minimized to 0.01. 

## Discussion (on the different results)

Compared to the performance of the RAPI method in Hsiao et al. (2021), the simulation results of four model evaluation indices for the RAPI in the current study demonstrated obvious discrepancies on standardized bias, relative SE bias, and the 95$\%$CI coverage rage of $\beta_{XM}$. Although all the standardized biases of the interaction effect in this study were all below the absolute .40 threshold values, the range of biases (i.e., range = [0.09, 0.30]) was larger than the one (i.e., range = [0.08, 0.18]) in Hsiao et al. (2021). The largest magnitude was 0.12 larger while the lowest magnitude was about the same. There was a common decreasing trend on the standardized bias estimates as the reliability level went up in both studies. Regarding the relative SE bias, the difference was obvious. In Hsiao el al. (2021)'s study, all the relative SE bias estimates were within the 10$\%$ acceptable range, from -5.54$\%$ to -1.61$\%$. However, the relative SE biases in the current study had a few values falling out from the 10$\%$ range, especially under small sample size and small reliability levels. For example, when N = 250 and $\rho$ = 0.70, the relative SE biases were -18.40$\%$, -22.70$\%$, and -21.80$\%$, which were largely beyond the acceptable range. Even though with high sample size and high reliability, the relative SE bias estimates in the current study were obviously larger and outside the range in Hsiao et al. (2021). As for the 95$\%$CI coverage rate, there were five records in this study below the acceptable 91$\%$ threshold value, while all the records in Hsiao et al. (2021) passed the limit. The unacceptable records were under low reliability levels in both sample size conditions.
The pattern of change coverage rates was overall opposite in two studies. In the study of Hsiao et al. (2021), the coverage rates were generally higher under low sample size and low reliability level. For example, when N = 250, the highest coverage rate was 96.95$\%$ with $\rho$ = 0.7; when N = 500, the highest one was 95.85$\%$ with $\rho$ = 0.7 as well. This situation was reversed in the current study, such that the highest coverage rate in two sample size groups were both under $\rho$ = 0.9 (i.e., 94.50$\%$ and 94.00$\%$). Besides, the magnitude of coverage rate was increasing as the reliability level was enhanced, while Hsiao et al. showed an opposite trend. The magnitude and pattern of RMSE values between two studies mostly matched such that all the values were within the acceptable range. 




